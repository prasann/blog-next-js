{
    "version": "https://jsonfeed.org/version/1",
    "title": "Random Presence",
    "home_page_url": "https://prasanna.dev",
    "feed_url": "https://prasanna.dev/feeds/feed.json",
    "description": "Random presence of my thoughts and learning...",
    "icon": "https://prasanna.dev/assets/logo.png",
    "author": {
        "name": "Prasanna Venkatesan",
        "url": "https://prasanna.dev"
    },
    "items": [
        {
            "id": "https://prasanna.dev/posts/experience-with-aws-saa",
            "content_html": "\nI recently passed the AWS SAA certification 🎉, penning my experience here.\n\nI have been working with AWS for the past ~3 years on and off based on the projects and client needs. One of the challenges for me is that I was always reactive. Usually the needs are EC2, ECS, S3, Cloudfront, Cloudwatch and RDS and that's it. I haven't tried lots of their services, and I wasn't even aware of the optimisations that can be done on both cost as well as in the architecture front.\n\nDecided to pick up this certification AWS SAA to proactively design and build secured, resilient and cost optimised architectures upfront and not to wait for the issues to pop up and then optimise. This is my first certification and certainly re-lived my college exam days while preparing for this exam.\n\n## Preparation\n\nStarted this around ~3 months back, but was very much overwhelmed with the amount of materials (courses/ notes) that's on the internet. Eventually settled on the following things,\n\n1. [Udemy course](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c02/) by **[Stephane Maarek](https://www.udemy.com/user/stephane-maarek/) 🎥** This is the longest course (~27hours) I have ever signed up for. This is an excellent start if you are looking to get this certification. This gives an intro and decent depth on all (almost) the AWS services. If you follow the hands-on approach along with Stephen, you will be equally confident in managing these services.\n2. This [Github notes](https://github.com/keenanromain/AWS-SAA-C02-Study-Guide) 📝 Though the videos are exhaustive, it's impossible to remember everything that's in there. That's where these Github notes helped me. Whenever I had some 15 mins, i goto a section in here and read through that.\n3. [Anki Cards:](https://apps.ankiweb.net/) 📇 Using flashcards to revise and memorise is my habit for a long time. Thanks to this [reddit user](https://www.notion.so/AWS-SAA-Certification-1c3d99ed38ab494ea8ea467cd27ca725), there is an excellent summary of all the services. You can import them to the anki app and you are good to go. 20 cards a day and your certification isn't far away 😁\n4. Took the [practice exams in Udemy](https://www.udemy.com/course/aws-certified-solutions-architect-associate-amazon-practice-exams-saa-c02/). ✍🏼 This is an eye opener for me. I can understand and relate to most of the questions (thanks to the course and notes) but this is where we need to apply the knowledge we learnt and it wasn't easy. I decided to take all the tests in the course as open book types. After a couple of tests, I realised an open book takes a lot of time and decided to make some guesses and mark those guess questions for later review.\n\n## On the day experience\n\n- I took the exam through Pearson Vue online. Make sure to keep the work desk (wherever the laptop/computer is placed) clean. They do check that 😅\n- You can check in 30 mins before the scheduled time, and likely the exam will also start early.\n- One day before the exam, go through the system check and keep things ready.\n- You will have your video and mic turned on throughout the exam, you can't mute or turn off the video.\n- My guess on the evaluation part. There are lots of questions where they offer partial marks. So the score isn't binary (right or wrong). so don't give up halfway, if you have done the preparations well you are likely to pass the exam.\n\n## Some tips if i want to re-do this\n\n- **Book the exam dates upfront.** There isn't enough motivation to run through the gazillion notes, so have a milestone. It isn't a big problem, since you can amend the dates twice.\n- **Pair with a like minded person.** Again not to drop the ball on the certification and have someone to talk to.\n- You can't master concepts along with the details in one go. accept that. so **don't spend too much time on one thing**.\n- **Spend more time on model exams/tests**. From a time perspective, split it 50-50 roughly. Spend 50% time to go over the video courses, understanding the concepts and the remaining 50 to go over the model tests.\n- For non native english speakers, there is a provision to **get 30 mins extra time for the exam.** [](https://aws.amazon.com/certification/policies/before-testing/) [(ESL+30)](https://aws.amazon.com/certification/policies/before-testing/). It's good to take this time, i cut close to the finishing 140 mins.\n\nHappy preparing, and all the best !!\n",
            "url": "https://prasanna.dev/posts/experience-with-aws-saa",
            "title": "What I did to become AWS certified 😎",
            "summary": "My experiences and learnings when i worked on to get my AWS certification. I recently passed the AWS SAA certification. Penned down my learnings and experiences along the way",
            "date_modified": "2021-12-03T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/microservices-pattern-sagas",
            "content_html": "\n> *Cross posted from [Dev.to blog] (https://dev.to/prasann/micro-services-patterns-saga-to-music-or-to-dance-4hio)*\n\n## What is a Saga pattern\n\nSaga design pattern is a way to manage a single business transaction that spans across various micro services. Saga pattern breaks a single business transaction into a sequence of local transactions that updates each service and publishes a message or event to trigger the next local transaction step. If any of these local transaction fails, saga will execute the subsequent flows to  rollback and cleanup the transaction.\n\n## Why do we need this\n\nIn a micro-services architecture, it's hard to trace where the transaction has failed and what needs to be rolled back etc. Saga brings in a structure to deal with this problem and also allows the micro-services to act within the specified boundaries.\n\n## Implementing Saga\n\nSaga is implemented by adding a `transactionId` to all the local transactions. A `transactionId` represent a single business transactions, so with that as an identifier it is possible to trace all the corresponding service interactions.\n\nThis managing of local transactions can be done in 2 styles.\n\n- Orchestration style\n- Choreography style\n\n## Orchestration Style Sagas\n\nOrchestration pattern mimics an Orchestra, where each person (system in this case) waits for the conductor (another system) to give instructions on what needs to be done.\n\n![Orchestration style saga {800xx972}](/assets/posts/images/saga-patterns/orchestration.png \"Orchestration style saga\")\n\nI don't prefer this design usually for the following reasons,\n\n- Tight coupling between the conductor and the other systems in the ecosystem.\n- Conductor is a single point of failure.\n\n## Choreography Style Sagas\n\nChoreography pattern mimics a dance performance where each dancer knows their role and can perform it independently. Hence, there is no need of centralised conductor role.\n\n![Choreography style saga {800xx972}](/assets/posts/images/saga-patterns/choreography.png \"Choreography style saga\")\n\nSome benefits of this pattern are,\n\n- Faster development. Teams can build independently, with defined contracts.\n- Loose coupling, easier to change systems.\n- Better fault tolerance. There is no single point of failure here\n\n## A Sample Scenario\n\nLet us walk through a sample use case and see how we can go about solving it using choreography style sagas.\n\nSince we are talking about music and dance, let me take a use case of booking a movie ticket.\n\n- A customer can purchase a movie ticket by paying online.\n- Once the payment is successful, the movie ticket is confirmed to that customer.\n- If there is a payment failure, no ticket will be issued, and the order stands cancelled.\n- If the show is cancelled, the customer will be fully refunded.\n- If the customer choses to cancel a ticket, then they will be refunded a partial amount only.\n\n## Identifying Events and Commands\n\nNow, to implement them independently by various services, we need to identify the boundaries of various services and also their resposibilities.\n\n[Event storming](https://www.eventstorming.com/) is one activity that the team can do to identify these events aka boundaries of responsibilities.\n\n**Results of the event storming looks like this:**\n\nI will be using the following notion to illustrate various commands and events involved in the above use-case.\n\n![legend {800xx272}](/assets/posts/images/saga-patterns/legend.png \"legend\")\n\n![booking-flow {800xx850}](/assets/posts/images/saga-patterns/booking-flow.png \"booking-flow\")\n\n![cancel-flow {800xx850}](/assets/posts/images/saga-patterns/cancel-flow.png \"cancel-flow\")\n\nOnce these events and commands are identified, teams independently can go ahead and start implementing them. Rollbacks are just another events mapped to a different command.\n\nThis gives a power to the team to act independently and reduces the bottleneck on a single service.\n",
            "url": "https://prasanna.dev/posts/microservices-pattern-sagas",
            "title": "Micro-services Patterns. Saga - to music or to dance?",
            "summary": "Saga pattern is useful to trace a distributed transaction across various micro-services. This post summarizes the 2 patterns of saga and explains a event storming for a sample use-case",
            "date_modified": "2021-06-09T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/slicing-microservices",
            "content_html": "\n> *Cross posted from [Dev.to blog] (https://dev.to/prasann/slicing-microservices-1agj)*\n\nBuilding applications using micro-services are becoming a default go-to architecture these days. I have been part of few teams that build and deploy micro-services in a large scale.\n\n> One pertinent question that often asked is \"***Did we slice it right?***\"\n\n## What is slicing a service mean\n\nSlicing a micro-service refers to defining the boundaries of the micro-service.\n\n- What should the service be responsible for\n- What kind of data should it hold\n- When it should delegate it's responsibility to another micro-service.\n\nBelow are some of my experiences, that i have seen working.\n\n## When to do it\n\nOne of the common behaviour we did in the teams I worked so far, is to let micro-services **evolve** organically. We add feature/capabilities to the existing service and later trim down the service by spawning a new one.\n\n![organic slicing of micro-service](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fz8fj6jt4y0z5rlp1irs.png)\n\n\nSome of the benefits,\n\n- **No need of upfront discussion**. Most likely we will have lesser information about the feature, then likely that our design might reflect the incompetencies.\n- **Spawning a new service will have it's own cost**. How much ever you automate, it still adds up the infrastructure cost, maintenance cost. If we are wrong about the slicing then we had to spend some more time and effort to unify it with some other service.\n- Once we have built a feature, most of the **people in the team will understand the use-case and will appreciate the need of a separate service**. It doesn't become a single person's decision or a group of architect's decision. But a decision that comes from ground up. There  is a better chance for the service the retain it's shape when it grew this way.\n\nThis approach does require a good discipline in having a constant check on the growth of a service. A highly coupled service is very hard to break down later. And if we are too late to cut down a service, it might become an expensive operation too.\n\n## How to do it\n\nHere are some of the themes i have come across. I will try to explain my thoughts using a bare minimum add-to-cart like domain problem.\n\n### Entity based slicing\n\nVery common and obvious start for a new service.\n\nExample: `UserService` dealing with the CRUD of a `User` entity in the system.\n\nIt's easy to conclude **entities (domain models)** as boundaries, since it's intuitive for people to see the separation. But whether it's right? is highly questionable and depends a lot, on the use case.\n\n> It's simple, easy and often end up in chaos\n\nOne of the significant problems, i have seen is that these services will be very much in demand by other service. They entire network becomes very chatty. One can assume the `User` entity will be needed at each and every step of the application and will have lots of interaction. Worst, is when the services decided to retain their copy of the data to enhance the performance of the application.\n\nOrchestrator will become a monolith. Since there will be lots of entity services to do mundane operation, orchestrator will become the one service to hold the business logic.\n\n![entity based slicing {336xx271}](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/si5fohmhiv4aqoa1h7xi.png)\n\n### Journey based slicing\n\nSlicing services based on **user journeys and behaviour**. This is quite popular, especially among the product people. Mostly, the product evolution happens on a feature/journey based. So whenever a new journey is identified it's time to build a new service.\n\nEx: `RatingService` a service that allows you to rate an entity. It can be products, people or article etc. Behaviours can include to make sure you don't rate same article twice, compute average ratings etc.\n\nOne of the advantage of this technique is that usually the teams i have worked in the past, they own the journey and hence it's clear for them what needs to be the part of this service\n\nHuge drawback I have seen with this approach, is that it forces the data being duplicated across services. In order to maintain the true flavour of Microservice, we will end up having independent databases and eventually having duplicate data\n\n### Best of both worlds\n\nIt makes a lot of sense to combine the above 2 approaches. Identify the core entities (domain models) of the system, and have them as either independent or logically grouped service. Apply journey based slicing on top of this entity services. So, the teams will own the journey service and the entity services can be maintained by group of teams.\n\n![best of both worlds {336xx271}](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/degdriqsoudx0xhr4pgt.png)\n\nSome of the conventions that i have seen/worked while slicing Micro-services\n\n**Entity services should be thin and lenient.**\n\nEntity services should be merely act as APIs to the database operations. Do not try bringing in business logic here.\n\nFor example, making the email-address non updatable in the entity service level. It makes sense to not to allow the end-user to allow update the email addresses, but often that might be a need from a back office personnel (admin user). So restricting such operation in the entity level might not be worth it. **Journeys should take care of validations.**\n\n> It's hard to predict the future requirements so keep EntityServices simple and open for extension.\n\n**Avoid journey service calling other journey services**\n\nJourney services, should be independent of others. Store data that are necessary for the journey and use entity services to collaborate with common data.\n\n**Build composite entities wherever needed**\n\n- Now, to answer the immediate question that will raise due to the above constraint. How to manage the duplicate business logic.  To be honest, **DRY principle is overrated in my opinion.** But in case if you are looking for such thing, then try adding another layer of composite entities.\n- These composite entities, will encapsulate multiple entities and some business logic around this. One classic example from the app we built is a tax computation service. It involves the product, and the location of the buyer to calculate the tax.\n\nAll these things are from my past experience, I'm sure I'm going to learn more and course correct myself in this journey. But one thing that i feel will always help in evolving micro-services is to constantly question the slicing decision to get it at a right state. And also a good knowledge on the [Domain driven design](https://martinfowler.com/bliki/DomainDrivenDesign.html) helps a lot to take these decisions.\n",
            "url": "https://prasanna.dev/posts/slicing-microservices",
            "title": "Slicing Microservices",
            "summary": "Some ideas to go about in designing micro-services. How to slice them and some general conventions",
            "date_modified": "2021-04-01T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/scaling-applications-using-microfrontends",
            "content_html": "\n> *Cross posted from [Archimydes blog](https://archimydes.dev/fourthact/blog/scaling-applications-using-micro-frontends)*\n\nThis blog post is a summary of a presentation that I made at the Archimydes Mastermind Event that happened on 17th Dec 2020.\n\nModern web applications tend to have complex and feature-heavy Frontends when compared to backends.\n\nWith so many choices for frameworks and programming paradigms, building a consistent Frontend to scale is a challenging problem to solve. We cover ways in which you can scale your Frontend application and your development teams by using a Micro-Frontends design pattern.\n\nI'll start by introducing the pattern of Micro-frontends first. Then we'll be looking into some of the key decisions that need to be taken while starting a Micro-frontend project. Finally, we will see the circumstances where this pattern will be effective.\n\n## **1. Scaling Applications**\n\n![Scaling applications](/assets/posts/images/scaling-microfrontends/Twitter_post_-_1.png \"Scaling applications\")\n\nIn general, scaling applications implies scaling backend applications to serve an increasing number of users. Usually, it's about how to:\n\n- Increase Performance\n- Reduce Latency\n- Sustain load\n- Manage compute costs\n\nAll these parameters are typically applicable for the backend applications.\n\nFor frontend applications, we typically stop with a good CDN to deliver static assets efficiently. However,\n\n> Scaling frontend apps is also about scaling development teams, both by size (growing a size of a single team) or count (multiple teams)\n\nAdditionally, applications are getting more frontend heavy because:\n\n- backends are getting easier to deploy and get off the ground\n- end-user compute is getting cheaper and more powerful everyday\n- more functionality is being pushed to end-user interfaces and devices\n\nAs a result of this, product teams need to figure out an efficient way to build and deliver frontend applications with multiple development teams working at scale. Product teams need to execute this while reducing bottlenecks in the development process.\n\n## **2. Monoliths, Microservices and Micro-frontends**\n\n> Monoliths are not a bad design choice\n\nIt's always best to start any application as a monolith. Upfront slicing of module boundaries is very hard and tends to go wrong. As the application grows, it's better to identify module boundaries and split them up.\n\n**Microservices**\n\nFrom monoliths, the best choice to evolve the backend services as microservices. We can then guarantee:\n\n- Strong module boundaries\n- Independent deployment\n- Polyglot development and tech diversity\n\nHowever, most of the microservices I have seen are as follows\n\n![Microservices](/assets/posts/images/scaling-microfrontends/Twitter_post_-_2.png \"Microservices\")\n> Independent deployments ! == Independent releases\n\nTeams are able to develop and deploy backends independently. However, they need to wait for the frontend to be developed and deployed.\n\n**Enter Micro-frontends**\n\nMicro-frontends are nothing but taking the concept of micro-services to the frontend. Slice the frontend of the application to respect the module boundaries of the backend, and create an end-end independent release path.\n\n![Microfrontends](/assets/posts/images/scaling-microfrontends/Twitter_post_-_3.png \"Microfrontends\")\n> All of Microservices' promises + Independent releases\n\n## **Gains with Micro-frontends**\n\n- Independent teams\n- Independent releases\n- Simple, decoupled codebases\n- Incremental upgrades\n\n### **Problems that need solving**\n\n- T***o 'share, or not to share'?*** - Code reusability is one of the most overrated principles in software development. The problems of reusability are often ignored or not shared. In going the micro-frontend way, this needs to be discussed among the teams. Out of the gate, a duplicate first strategy works since it allows teams to execute faster initially.\n- **Application loading performance** - Micro-frontends can cause an impact on the loading performance of the application. There are ways to mitigate it, but the effort it takes has to be taken into consideration.\n- **Design consistency across the application -** Having a larger number of people working on an application will lead to inconsistencies. Again, there are ways to mitigate this, however, the effort involved in mitigation needs to be considered.\n\n## **3. Key decisions while doing Micro-frontends**\n\nLet's go over some of the major decisions that we need to take during the early stages of a micro-frontend application. I will try to cover the solution(s) that we took while building an application with distributed teams across 3 regions for 2 years. The decisions can vary based on the project context but nevertheless these problems need to be solved.\n\nIn order to explain the challenges and decision, I will take up the following use-case:\n\n**Building an application to allow user to configure and buy a laptop. Similar to that of Apple's.**\n\nA user can ***configure*** a laptop with various components, accessories, protection plans, etc. The user should be able to ***search*** for accessories, or maybe built-in models, and then finally should be able to ***order*** the product and get it fulfilled.\n\nApart from the 3 services - configure, search, and order, I will have another service called \"Frame\" merely to hold the application together.\n\n- **Frame**: A business logic agnostic orchestrator service that knows how to download the rest of the services' frontend\n\n**A) Composing multiple front-ends into a single application**\n\n> End users don't care about the tech used. Their experience should not be affected due to tech.\n\nComposing multiple frontends into a single application is one of the first problems that needs solving when choosing micro-frontends.\n\n![Composing frontends into single app](/assets/posts/images/scaling-microfrontends/Twitter_post_-_4.png \"Composing frontends into single app\")\n**Composing front-ends**\n\nWe can achieve this composition in 2 ways, let's go over the pros and cons of these approaches.\n\n## **Build-time Composition vs Run-time Composition**\n\n**Build-time composition** is where multiple frontend applications are built as a single big application and served. This can be accomplished using **npm** or **yarn** packages.\n\n![Build time composition {800xx235}](/assets/posts/images/scaling-microfrontends/build-time-composition.png \"Build time composition\")\n\n**Pros:**\n\n- Good dependency management, resulting in smaller bundles\n- Independent cross team development\n\n**Cons:**\n\n- A monolith built by different teams\n- Non atomic deployments\n\n**A Run-time composition** is where the frontends get integrated into the browser directly when the user requests a page. This may be done on the \"Server-Side\" or in the \"Client-Side\"\n\n![Run-time composition {800xx373}](/assets/posts/images/scaling-microfrontends/run_time_frontend_composition_f5076854e1.png \"Run-time composition\")\n\n**Pros:**\n\n- Independent teams with independent deployments\n- Atomic deployments, so no versioning issues\n\n**Cons:**\n\n- Too many API requests from Client(?), with increased bundle size\n\n**Toolkit options for Runtime composition**\n\n**Server side:**\n\n- SSI (Server Side Includes)\n- Tailor (from Zalando)\n\n**Client Side:**\n\n- JSPM\n- SystemJS\n- FrintJS\n- Single-Spa\n\n***We chose Run-time composition for the project we worked on. Since our app was rendered on the client-side, it was simpler for us to achieve this.***\n\n## **B) Communication between the frontends**\n\nMultiple frontends need to share data with each other. Though this needs to be minimal, it's unavoidable. A couple of options to achieve this is by:\n\n- **State management tools**\n\nA global store in the application and all frontends using the same library to access the store.\n\n![State management tools {800xx153}](/assets/posts/images/scaling-microfrontends/state_management_tools_604f976fa9.png \"State management tools\")\n\n- **Window events**\n\nAnother approach could be to utilize the window (DOMs) eventing capability. Below is a sample event.\n\n![Window events {800xx250}](/assets/posts/images/scaling-microfrontends/window_events_46783b22ad.png \"Window events\")\n\n***We used to communicate through common redux store and redux events as all the apps in our micro-frontends were using Redux.***\n\n## **C) Design Consistency**\n\nOne of the hardest problem to solve for is design consistency.\n\nIn our team, we addressed this challenge by forming guilds. Consider that there are three teams, and each team has a designer assigned to it.\n\n![Actual team structure](/assets/posts/images/scaling-microfrontends/Twitter_post_-_5.png \"Actual team structure\")\n\nWe formed a guild comprising of all designers and some interested developers. They encompass a virtual team. They take all the design decisions and make sure their respective teams are abiding by the central design tenets.\n\n![Guild1](/assets/posts/images/scaling-microfrontends/Twitter_post_-_6.png \"Guild1\")\n\nInitially, the guild created a style guide for the application. Mainly CSS and the application teams copy-pasted it from the style guide to build components.\n\nAs we developed more features, we started pulling out Higher-order JS components and made them sharable. This is more of an evolution and works well once you have a stable design system in place.\n\n![Styleguide {800xx400}](/assets/posts/images/scaling-microfrontends/Twitter_post_-_7.png \"Styleguide\")\n\nAnd also, since the teams were using the same frontend framework (React) it was easier for us to build this component library.\n\n## **D) Testing Strategy**\n\nDeciding on \"How to test\" is important. Since it's a relatively newer paradigm and there are lots of moving parts in the application.\n\nPrimarily we will be discussing the \"Integration tests\" and \"Functional tests\" from the testing strategy, as there won't be much difference in the way the \"Unit tests\" are done.\n\n- **Integration tests**\n\nHaving a lightweight \"Consumer Driven Contracts\" (CDC) helped us a lot.\n\n![Integration tests](/assets/posts/images/scaling-microfrontends/testing_strategy_1776956c37.png \"Integration tests\")\n\nA CDC is where the consumer services' give some tests to the provider service. A provider has to run all of its consumer services before publishing an artifact for deployment.\n\nThis doesn't need to be very complex and can be done quickly using some lightweight options without using any big frameworks. But then, it's all case by case.\n\nIn our scenario, Frame was the consumer of all the services and it shared a simple JSON contract and a small JS test with all of its providers. This ensured that the frame wasn't broken when a service deployed automatically.\n\n![Frame test {800xx333}](/assets/posts/images/scaling-microfrontends/sample_of_frames_contract_025a143c30.png \"Frame test\")\n\n- **Functional tests**\n\nThis is one of my least favorite testing methods, however, like everything else in tech, it does have some staunch supporters and followers. In our case, we only had a few critical and successful user journeys automated using Selenium for functional testing.\n\n![Functional tests {800xx400}](/assets/posts/images/scaling-microfrontends/functional_tests_6cdcf7c24a.png \"Functional tests\")\n\nThese journeys cut across multiple services and hence are harder to develop and maintain. Some of the FAQs I usually get on these tests are\n\n## **FAQs**\n\n- **Who owns functional tests?**\n\nAns. The product team and business analysts. They define the scenarios for automation.\n\n- **Who writes functional tests?**\n\nAns. Guild containing QAs from all teams and a few developers.\n\n- **Who fixes functional tests?**\n\nAns. Team which breaks it.\n\n## **When should you opt for Micro-frontends?**\n\nMicro frontends are not for everyone. It adds significant overhead with development and maintenance.\n\n- **A. Distributed self-contained teams, with a need for parallelization**\n\nIf your development teams aren't co-located, and there is a decent amount of parallelization that needs to be done, this could be a reason to implement micro-frontends.\n\n- **B. Collaborate with different frameworks in the frontend**\n\nImagine you are inheriting a legacy application but want to build a new feature with modern design elements, then micro-frontends gives you a good head start.\n\n- **C. Teams that have experience building Microservices application, and are willing to take it to the next step**\n\nMost of the points mentioned here are forward-thinking practices. Micro-frontends needs a good solid understanding of the domain and good discipline to contain things within one's boundary.\n\nFinally, it's worth remembering that:\n\n> It's not a sprint. It's a marathon.\n\nMicro-frontends adds significant overhead to the overall application. This isn't desired for smaller applications or for the application that will be built and managed by a single team. The above mentioned challenges are worth solving, only if you are up for a longer run with multiple teams.\n",
            "url": "https://prasanna.dev/posts/scaling-applications-using-microfrontends",
            "title": "Scaling Applications Using Micro-Frontends",
            "summary": "When starting a project with Micro-Frontends, here are some typical problem that require solving and some possible solutions.",
            "date_modified": "2021-01-21T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/ace-with-react",
            "content_html": "## Atlassian Connect Express (ACE)\n\nACE is a toolkit in node.js to build atlassian connect apps. Like a JIRA, Confluence plugins.\n\nBest way to start with ACE is to bootstrap from `atlas-connect` Y[ou can follow the documentation here to get a sample app up and running in few minutes.](https://bitbucket.org/atlassian/atlassian-connect-express/src/master/)\n\n## ACE and handlebars\n\nACE is a wrapper on top of the [expressjs](https://expressjs.com/) and hence the application is pretty much an express application. The bootstrapped application will have both the frontend and the backend part.  For the frontend, ACE by default uses [handlebars](https://handlebarsjs.com/) as templating option.\n\nWhen you make a API call, the server converts the handlebar templates into HTML and returns.\n\n## Replace Handlebars views with a SPA\n\nAs you can see, the above mechanism isn't tailored to use any single page application. Luckily, ACE doesn't change lots of things from express and hence we can use the same technique to achieve the goal.\n\n***In this example I'm choosing [React](https://reactjs.org/), however the technique is fairly the same for any SPA framework.*** Since the idea is fairly simple. Using the framework build the app and deliver it to the browser on the init api call. From then on, the app will be controlled by the browser.\n\n## Up and running with React.\n\n**1. Add a route to serve static files.**\n\nTo the already bootstrapped ACE project, i added a folder called client, and inside that I used [CRA](https://github.com/facebook/create-react-app) to bootstrap my React project. This will be my SPA. Make sure to keep  the built artifact within this folder. In my case it was `{ProjectRoot}/client/build/*`\n\nNow in the express application's index.js need to modify the route to serve this page. In my case, i used [generalPages module](https://developer.atlassian.com/cloud/jira/software/modules/page/) of ACE. In `atlassian-connect.json` I have mentioned `url` to be `/init`\n\nHere is the modified `/init` to serve the react app.\n\n```jsx\napp.get('/init', addon.authenticate(), (req, res) => {\n  res.sendFile(path.join(__dirname, '/../client/build/', 'index.html'));\n});\n```\n\nyou can import the path like this `import path from 'path';`\n\n2**. Configure static directories for express server.**\n\nNext is to set the path variable for the express. You need to define the static directory for the express server to fetch the files from.\n\nin `app.js` you can configure this.\n\n```jsx\nconst app = express();\nconst addon = ace(app);\n\n/* more config */\n\nconst staticDir = path.join(__dirname, 'public');\n//*** This line is important **********//\n\napp.use(express.static(path.join(__dirname, 'client', 'build')));\n\n//*** This line is important **********//\napp.use(express.static(staticDir));\n```\n\nNow, we have configured express to look into 2 different directories for static files.\n\n**3. Include atlassian JS API as part of the SPA**\n\nLastly, we need to make a change in the client app. Jira/Confluence when they load their plugin, they expect the plugin to have `all.js` . This is the client side logic of ace. So, we need to include this as part of our React's `index.html` without this, ***the loader in Jira will never disappear***\n\nIn `client/public/index.html` add the script tag just below the body.\n\n```jsx\n<script\n    src=\"https://connect-cdn.atl-paas.net/all.js\"\n    type=\"text/javascript\"\n    data-options=\"sizeToParent:true;resize:false\"\n></script>\n```\n\nYou can read about more `data-options` [here](https://developer.atlassian.com/cloud/jira/software/about-the-javascript-api/).\n\nThat's it, you will now see the React application in the connect app.\n\n***Note:** In development mode, there isn't any hot reload in this case. If you make any changes to the react app, need to build the app manually. Ofcourse, you can modify the `package.json` to automate this, but `webpack-dev-server` isn't much helpful.*\n\n## Authentication within the connect app\n\nIf you are planning to bundle few APIs along with the react app, then one of the harder thing to crack is authentication. This isn't documented very clearly anywhere.\n\nACE uses a JWT to authenticate api's and unfortunately the token isn't accessible for React application since it is running inside a iframe by default. If you are using the default Handlebars, ACE provides  helper methods to access the JWT.\n\n### Workaround\n\nThe first call from JIRA/Confluence call will carry the JWT. In the above code snippet `addon.authenticate()` will validate the JWT executes the callback. In this place, we can create a JWT and set it as cookie header. Post that, in all the API calls made from React app, we can validate the JWT against our secret, and it will sort out the issue of authentication.\n\nHere is a sample code for a Hello world with ACE and React. [Github Source.](https://github.com/prasann/ace-with-react)\n",
            "url": "https://prasanna.dev/posts/ace-with-react",
            "title": "ACE with React",
            "summary": "Atlassian connect express comes by default with handlebars, this post describes how to make it work with a SPA.",
            "date_modified": "2020-07-31T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/centralized-error-handling-express",
            "content_html": "\n_**Note: All my examples are in typescript and there are million other ways to achieve similar result, this is just my way of doing things.**_\n\nIf you are looking to start a new application in express, go over to the express site, use [express-generator](https://expressjs.com/en/starter/generator.html) to create an application.\n\n## Error handling\n\nBy default, any errors that are thrown within your application, will be sent as a 500 response code, along with the error stack trace in the body. This was inconvenient for me since,\n\n* I don't want the end-user to know the system stack trace.\n* I want to use exceptions for errors like `BadRequest` `AuthExceptions` etc.\n\nSo, i decided to tweak the default behaviour and take control of error handling. If your use case is similar, then proceed with further steps.\n\n### Custom error handler\nCreate your own custom ErrorHandler class (`error_handler.ts`), this will extend the node's [Error](https://nodejs.org/api/errors.html#errors_class_error) class.\n\n```typescript\n//error_handler.ts\nexport class AppError extends Error {\n    statusCode: number;\n    message: string;\n\n    constructor(statusCode, message) {\n        super(message);\n        this.statusCode = statusCode;\n        this.message = message;\n    }\n}\n```\n\nNow in your application you can invoke this custom error handler by calling,\n\n```typescript\nnew AppError(404, 'Unable to find the resource');\n//or\nnew AppError(403, 'You are not authorized to perform this action');\n\n```\n\n### Wiring error interceptor into express application\n\nOnce you start throwing exceptions within your application, next step is to convert those errors into a meaningful response for the end-user. Express app provides a way to hook up a custom error\nhandler into your application. A middleware that takes in 4 parameters is your way to add your custom error handler.\n\nLet's add the custom error handler function in the same `error_handler.ts` class and export. This generic function will parse the thrown error and constructs appropriate response.\n\n```typescript\n//error_handler.ts\nexport const customErrorHandler = (err, res) => {\n    const { statusCode, message } = err;\n    res.status(statusCode).json({error: {message}});\n};\n```\n\n```typescript\nimport express from 'express';\nimport customErrorHandler from 'error_handler';\nconst app = express();\n\n// Other middlewares, routes... \n\n// Adding your custom error handler.\napp.use((err, req, res, next) => {\n  customErrorHandler(err, res);\n});\n```\n\nNow, whenever any error that is thrown in the application will be caught by this error handler. This will in turn respond back with appropriate status code.\n\n### Dealing with unknown errors\n\nAs you can see, the `customErrorHandler` has a limitation of handling only the errors that are of type `AppError` since it expects `statusCode` to be present in the error.\nHowever, there will be `RuntimeExceptions` that will occur in the application. It's kind of hard to catch all these sort of errors in the application and re-throw them as custom errors.\n\nSo, we will improve our `customErrorHandler` to handle such `RuntimeExceptions`.\n\n```typescript\n//error_handler.ts\nconst handleKnownExceptions = (err, res) => {\n    //log it\n    const { statusCode, message } = err;\n    res.status(statusCode).json({error: {message});\n};\n\nconst handleUnknownExceptions = (err, res) => {\n    //log it\n    res.status(500).json({ error: {message: 'Something went wrong.' }});\n};\n\nexport const customErrorHandler = (err, res) => {\n    err instanceof AppError ? handleKnownExceptions(err, res) : handleUnknownExceptions(err, res);\n};\n```\n\nNow, we introduced one more way of handling errors. If the caught error is not that of ours (`AppError`) then we respond back with a 500 response.\nWe don't want our end user to know about the system internals and hence respond with a static message.\n\n\n### Dealing with asynchronous routes\n\nThis centralized error handling will not work for the errors that are thrown in the `await` methods i.e, any error that are thrown in an async block will not reach our `customErrorHandler`.\nThis is a limitation with respect to express 4.x.\n\nAs a workaround, you have to make the routes to be synchronous. Instead of changing all the routes to synchronous blocks i used this\n[middleware](https://github.com/Abazhenov/express-async-handler) to achieve a similar effect. Post wrapping my routes with this middleware, all the errors in async block will then reach our  `customErrorHandler`\n\nHere is the [gist](https://gist.github.com/prasann/b6ad07b3962b6ea2953fef027df5d10b) to the final `error_handler.ts`\n\n\n",
            "url": "https://prasanna.dev/posts/centralized-error-handling-express",
            "title": "Centralized error handling in Express applications.",
            "summary": "Handling exceptions in an express application, responding back with standard error response.",
            "date_modified": "2020-04-09T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/logging-in-golang-projects",
            "content_html": "\nOne of the common requirement in any project is to have some additional context while logging. And most of us aren't consuming the logs directly these days.\nWe use either ELK stack or some other proprietary tools to consume the logged information.\nIn these cases, it's important to know where those logs specifically came from and also important to log it in a format that's easy to parse and index.\n\nIn our case, we were using splunk and we have built a lots of dashboards based on Splunk logs. So our convention is to log in JSON format and also to log machine information and some more environment based information.\n\nHere is the post describing how we achieved it using go-lang.\n\n\n## Setting up an abstraction for logging\n\nWe decided to go with logrus as our logging library. Instead of using and importing logrus in all the places, we wrote a layer of abstraction.\n\nThis layer, then exposes various public functions to be consumed by the actual callers. So the logrus usage, is hidden and can be later changed too\n\nIn this layer, we can then inject common variables that needs to be logged as part of all log statements.\n\n## Logging the caller\n\nThe moment we introduce the abstraction, we have introduced a problem of losing the actual log position. logrus will log the abstraction layer as the log position for all log statement.\n\nSo, here we are logging the caller as \"ContextLogTag\". The caller will be then identified using the  go runtime. We can navigate through the stack in the go runtime to log the caller.\n\nHere is the code for does that\n\n```go\nfunc getCallerInfo() string {\n\t_, filePath, lineNo, isOk := runtime.Caller(2)\n\tif isOk {\n\t\tpathArray := strings.Split(filePath, \"/\")\n\t\tfileName := pathArray[len(pathArray)-1]\n\t\treturn fmt.Sprintf(\"%s#%d\", fileName, lineNo)\n\t} else {\n\t\treturn \"\"\n\t}\n}\n```\n\n\nHere is our abstraction layer.\n\n```go\npackage logger\n\nimport (\n\t\"fmt\"\n\t\"github.com/sirupsen/logrus\"\n\t\"log\"\n\t\"os\"\n\t\"runtime\"\n\t\"strings\"\n)\n\nvar logger *logrus.Logger\n\ntype Fields map[string]interface{}\n\nconst (\n\tcontextLogTag     string = \"ContextLogTag\"\n\terrorLogTag       string = \"ErrorLogTag\"\n\tdeviceLogTag      string = \"Device ID\"\n)\n\nvar logEntry *logrus.Entry\n\nfunc Setup() {\n\tlevel, err := logrus.ParseLevel(\"<<loglevel from env>>\")\n\tif err != nil {\n\t\tlog.Fatalf(err.Error())\n\t}\n\n\tlogger = &logrus.Logger{\n\t\tOut:   os.Stdout,\n\t\tLevel: level,\n\t}\n\tlogger.Formatter = &logrus.JSONFormatter{}\n\n\tlogEntry = logger.WithFields(logrus.Fields{\n\t\tdeviceLogTag:      \"<<deviceId from env>>\",\n\t})\n}\n\nfunc Error(errMessage string, err error, fields map[string]interface{}) {\n\n\tif fields != nil {\n\t\tfor key, val := range fields {\n\t\t\tlogEntry = logEntry.WithField(key, val)\n\t\t}\n\t}\n\tlogEntry.\n\t\tWithField(contextLogTag, getCallerInfo()).\n\t\tWithField(errorLogTag, err).\n\t\tError(errMessage)\n}\n\nfunc Fatal(errMessage string, err error, fields map[string]interface{}) {\n\tif fields != nil {\n\t\tfor key, val := range fields {\n\t\t\tlogEntry = logEntry.WithField(key, val)\n\t\t}\n\t}\n\tlogEntry.\n\t\tWithField(contextLogTag, getCallerInfo()).\n\t\tWithField(errorLogTag, err).\n\t\tFatal(errMessage)\n}\n\nfunc Info(msg string, fields map[string]interface{}) {\n\tif fields != nil {\n\t\tfor key, val := range fields {\n\t\t\tlogEntry = logEntry.WithField(key, val)\n\t\t}\n\t}\n\tlogEntry.WithField(contextLogTag, getCallerInfo()).Info(msg)\n}\n\nfunc Warn(fields map[string]interface{}, args ...interface{}) {\n\tif fields != nil {\n\t\tfor key, val := range fields {\n\t\t\tlogEntry = logEntry.WithField(key, val)\n\t\t}\n\t}\n\tlogEntry.Warn(args...)\n}\n\nfunc getCallerInfo() string {\n\t_, filePath, lineNo, isOk := runtime.Caller(2)\n\tif isOk {\n\t\tpathArray := strings.Split(filePath, \"/\")\n\t\tfileName := pathArray[len(pathArray)-1]\n\t\treturn fmt.Sprintf(\"%s#%d\", fileName, lineNo)\n\t} else {\n\t\treturn \"\"\n\t}\n}\n```\n\n\n\n\n  \n",
            "url": "https://prasanna.dev/posts/logging-in-golang-projects",
            "title": "Logging in Golang projects",
            "summary": "A log abstraction in go-lang projects, that can then be used to log common information. This also hides the log library inclusion, making it easier to swap out the library for a different one.",
            "date_modified": "2019-10-17T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/add-redux-middleware-dynamically",
            "content_html": "[Redux middlewares](https://redux.js.org/advanced/middleware) can be used for a variety of things. You can basically tap into a redux event and perform some action with it. Logging and analytics are very common use cases for Redux middleware.\n\nIn my case, I have a middleware component, that needs to be injected while initializing the Redux store. The middleware component will be served dynamically when the app loads.\n\n### Exporting the middleware component\n\nThis middleware detects a specific redux action and persist an information to the local storage. It's a custom middleware with a minimal change. This custom function takes in `middlewareAPI` as the parameter instead of having the state.\n```js\n  const persistInfo = middlewareAPI => next => (action) => {\n    if (action.type === \"SOME_ACTION\") {\n      const result = next(action);\n      const state =\n            JSON.stringify(middlewareAPI.getState().listen.value);\n      window.localStorage.setItem('PERSIST_THIS_INFO', state);\n      return result;\n    }\n    return next(action);\n  };\n  \n  export default persistInfo;\n  \n```\n\n### Loading the custom middleware\n\nHere is a small utility function that can take in a custom middleware and initialize the store.\n```js\n  import { createStore, compose } from 'redux';\n  import reducers from './reducers';\n  \n  class Store {\n      constructor() {\n        const composeEnhancers =\n          typeof window === 'object' &&\n          window.__REDUX_DEVTOOLS_EXTENSION_COMPOSE__ ?\n            window.__REDUX_DEVTOOLS_EXTENSION_COMPOSE__({}) : compose;\n  \n        this.store = createStore(reducers);\n      }\n    \n      instance() {\n        return this.store;\n      }\n    \n      addMiddleware(middleware) {\n        const middlewareAPI = {\n          getState: this.store.getState,\n          dispatch: action => this.store.dispatch(action),\n        };\n        this.store.dispatch = compose(middleware(middlewareAPI))(this.store.dispatch);\n      }\n    }\n    export default new Store();\n```      \n\n\nThis is my store class with the store initialization happens in the constructor. Simply, importing this store class and calling the `addMiddleware` function it's possible to inject the custom middleware component to your redux store.\n",
            "url": "https://prasanna.dev/posts/add-redux-middleware-dynamically",
            "title": "Add Redux custom middleware dynamically",
            "summary": "Redux provides options to add behaviour through middlewares. Here is an example of dynamically adding middleware to the store.",
            "date_modified": "2018-04-11T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/start-nginx-when-upstream-unavailable",
            "content_html": "### Upstreams in Nginx\n\n`upstream` is an nginx directive to define groups of servers. Servers can listen on differnt ports, and it is possible to mix and match the UNIX-domain sockets and TCP connections. You can read about it [here.](http://nginx.org/en/docs/http/ngx_http_upstream_module.html)\n\n### Issue with upstream\n\nIf you are using proxy\\_pass with upstream definitions in nginx config, then nginx checks for the server availability during the startup phase. A sample nginx.conf with upstream is here, lots of the .conf file is redacted to focus on the point in discussion.\n```nginx\n    http {\n        ...\n        upstream service-a {\n            server service-a-ip-or-name:3000;\n        }\n        \n        server {\n            ...\n            location /service-a/ {\n                proxy_pass http://service-a/;\n            }\n        }\n    }\n```    \n\nIn the above mentioned scenario, nginx server will check for service-a while start-up phase. If service-a is down, you will see an error like host not found in upstream service-a\n\n### The Workaround\n\nThis workaround is for services running in local setup in different docker containers. So, instead of using `upstream` directive you can directly point your service-discoverable-name in the proxy pass. The only thing while running docker containers, you need to add an additional nginx directive `resolver` and make it point to docker's internal DNS resolver. 127.0.0.11 The above mentioned config can be re-written as mentioned.\n```nginx\n    http {\n        ...\n        resolver 127.0.0.11;\n        \n        server {\n            ...\n            location /service-a/ {\n                proxy_pass http://service-a-ip-or-name:3000/;\n            }\n        }\n    }\n```    \n\n_Note: nginx approach is very valid in production like setups. However, in developer boxes it may not be possible to have all the services running while nginx starts. The workaround mentioned here should be mostly used in local or in dev setup and not advisable to use in prodcution like setup._\n",
            "url": "https://prasanna.dev/posts/start-nginx-when-upstream-unavailable",
            "title": "Start nginx when upstream is unavailable",
            "summary": "nginx will not start if one of the defined upstreams is not available. Here is a workaround to get through with those situations.",
            "date_modified": "2018-03-21T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/soap-call-in-clojure-compojure-with-saaj",
            "content_html": "### Simple Object Access Protocol (SOAP)\n\nSOAP brings its own protocol and focuses on exposing pieces of application logic (not data) as services. SOAP is focused on accessing named operations, each implements some business logic through different interfaces. This image below expresses the difference between a SOAP and normal REST/JSON endpoint very well.\n\n![SOAP explanation](/assets/posts/images/soap-primer.png \"Soap Primer\")\n\nSource: [Stack overflow](https://stackoverflow.com/a/44713574/419448)\n\n### Soap With Attachment API for Java (SAAJ)\n\n[SAAJ](https://docs.oracle.com/javaee/5/tutorial/doc/bnbhg.html) is a lower level API in Java that express SOAP messages. Java developers rarely use SAAJ since the JAX WS and Spring WS provides better abstraction over SAAJ.\n\n### SOAP in Clojure\n\n#### 1\\. Prerequisite\n\nAs a one-time step, convert the WSDL into Java objects. This can be done using \\`wsimport\\` or \\`xjc\\`\n```bash\nxjc -wsdl wsdl-file-name\n```\n\nor\n```bash\nwsimport wsdl-file-name\n```\n\n#### 2\\. Build SOAP Message\n\nFirst step is to build a soap message with header and body. The root element of the SOAP body is one of the Java object created in the first step. Construct the Java object with the necessary data. Finally convert the SOAP Message into string.\n\n#### 3\\. Perform POST\n\nA simple HTTP POST need to be performed with `Content-Type` header set to `text/xml`. This can be done using normal `clj-http` methods.Authentication should be covered ideally in the SOAP header.\n\n#### 4\\. Parse response into Java Object\n\nFinally the response string has to be converted into a SOAP Message again. This is required to parse the SOAP Response Body into one of the generated object.\n\n### Code in action\n\nHere is my [Github repository](https://github.com/prasann/soap-clj) with a small working application.\n",
            "url": "https://prasanna.dev/posts/soap-call-in-clojure-compojure-with-saaj",
            "title": "Dealing with SOAP in clojure",
            "summary": "Dealing with SOAP in clojure is not very straight-forward due to the lack of framework support. This post explains how to perform SOAP call using basic Java libraries.",
            "date_modified": "2018-02-15T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/using-network-call-in-react",
            "content_html": "All network calls that are necessary to load data needed by the component should go inside `componentDidMount()`\n\n> ##### [From React docs](https://facebook.github.io/react/docs/react-component.html#componentdidmount)\n>\n> componentDidMount() is invoked immediately after a component is mounted. Initialization that requires DOM nodes should go here. If you need to load data from a remote endpoint, this is a good place to instantiate the network request. Setting state in this method will trigger a re-rendering.\n\n#### Why not inside `constructor()`?\n\n*   If you make a fetch for a component in constructor, and the user navigates away from the page containing that component before the request completes, it will still try to setState on that component despite being unmounted, and React will throw an error.\n*   If your component fails to load, still you will end up making an unnecessary server-request.\n\n#### Why not in `componenentWillMount()`?\n\nThis function is invoked immediately before mounting occurs. So, obviously this appears to be a best place to place the call to load data. However that's not the case.\n\n*   Even if you add the network call in componentWillMount, your request will almost certainly not finish before the component is rendered. There is no way to pause the rendering till the request returns. So you will end up re-rendering the component anyways.\n*   This is the only lifecycle hook called on server rendering. So, if you are serving from the backend, this will be executed twice.\n",
            "url": "https://prasanna.dev/posts/using-network-call-in-react",
            "title": "Asynchronous calls in React component",
            "summary": "React documentation suggests to use componentDidMount for async calls. Here is the explanation of why you shouldn't do in constructor or in componentWillMount.",
            "date_modified": "2017-09-10T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/flyway-migrations-in-lein-clojure",
            "content_html": "\n[Leiningen](https://leiningen.org/) is the easiest way to start with clojure project automation. The project under discussion is a webservices written in clojure with [compojure-api](https://github.com/metosin/compojure-api) and [ring](https://github.com/ring-clojure/ring) middleware.\n\nWhen it came to Database migrations, I didn't find anything straightforward amongst the lein plugins. So, decided to use [flyway](https://flywaydb.org/). I have worked with flyway in the past with Java applications. But, this is the first time with clojure, leiningen combination.\n\n### Migration utility in clojure\n\nHere is the small migration helper written in Clojure\n\n```clojure\n(ns app.migration\n  (:require [environ.core :refer [env]])\n  (:import org.flywaydb.core.Flyway\n           org.flywaydb.core.internal.info.MigrationInfoDumper))\n\n;; Build DB String from the Environment Variables\n(def db-url (str \"jdbc:postgresql://\"\n                 (env :pg-db-host) \":\"\n                 (env :pg-db-port) \"/\" (env :pg-db-name)))\n\n;; Initialize Flyway object\n(def flyway\n  (let [locations (into-array String [\"classpath:db/migration\"])]\n    (doto (new Flyway)\n      (.setDataSource db-url (env :pg-db-user) (env :pg-db-password) (into-array String []))\n      (.setLocations locations))))\n\n(defn migrate [] (.migrate flyway))\n\n(defn clean [] (.clean flyway))\n\n(defn reset [] (clean) (migrate))\n\n(defn info []\n  (println (MigrationInfoDumper/dumpToAsciiTable (.all (.info flyway)))))\n```\n\n### Running migration during deployment\n\nI'm using [lein-ring](https://github.com/weavejester/lein-ring) plugin, this provided an option to execute function before the handler starts. So, I wired app.migrate to the init block of the handler.\n\nThis helps to run migration everytime before the application deploys. Ofcourse, flyway will take care of what migrations need to run based on the migration version.\n\n### Running migrations for local development\n\nThe above method works perfectly for the application deployment scenarios. However, in local it will be better to execute​ ​migration and clean databases as and when required, rather than re-deploying the application. lein-exec plugin offers​ ​a way to create and execute clojure code from project.clj files. With the above-mentioned migration present, all ​I​​ ha​ve to do is to create some aliases as shown below.\n```clojure\n:aliases { \n    \"db-clean\"   [\"exec\" \"-ep\" \"(use 'deal-picker.migration) (clean)\"]\n    \"db-migrate\" [\"exec\" \"-ep\" \"(use 'deal-picker.migration) (migrate)\"]\n    \"db-info\"    [\"exec\" \"-ep\" \"(use 'deal-picker.migration) (info)\"]\n    \"db-reset\"   [\"exec\" \"-ep\" \"(use 'deal-picker.migration) (reset)\"]\n}\n```\n",
            "url": "https://prasanna.dev/posts/flyway-migrations-in-lein-clojure",
            "title": "Flyway migrations in lein clojure",
            "summary": "Integrating flyway migrations to compojure apps. Flyway is a popular Java based database migration tool. This post describes about integrating flyway seamlessly with lein compojure ring stack in clojure.",
            "date_modified": "2017-07-15T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/card-slider-using-css3",
            "content_html": "[CSS Keyframes](https://developer.mozilla.org/en-US/docs/Web/CSS/%40keyframes) is a powerful feature to create animations in CSS.\n\nBelow is a small snippet I created for slider like animation.\n\n```html\n<div id=\"card\">Click me to animate</div>\n```\n\n```css\n.animate {\n    opacity: 1;\n    animation: slider 1s linear;\n}\n\n@keyframes slider {\n    0% {\n        margin-left: 0;\n        opacity: 1;\n    }\n    25% {\n        margin-left: -200px;\n        opacity: 0;\n    }\n    50% {\n        margin-left: 200px;\n        opacity: 0;\n    }\n    100% {\n        margin-left: 0;\n        opacity: 1;\n    }\n}\n\n#card {\n    background: #1f1f1f;\n    margin: 10px;\n    display: block;\n    border: 1px dashed white;\n    height: 200px;\n    width: 200px;\n    color: white;\n    font-weight: bold;\n    padding: 10px;\n    text-align: center;\n    cursor: pointer;\n}\n```\n\n```js\n$(\"#card\").on(\"click\", () => {\n  $(\"#card\").addClass(\"animate\");\n  setTimeout(() => $(\"#card\").removeClass(\"animate\"), 1000);\n});\n```\nHere is the [link to the codepen](https://codepen.io/prasann/pen/ppNLNL)\n\nMost of the browsers do support keyframes now. [Here](https://caniuse.com/#feat=css-animation) is the \"Can I Use\" page for keyframes.\n",
            "url": "https://prasanna.dev/posts/card-slider-using-css3",
            "title": "Card slider using CSS Keyframes",
            "summary": "Slider animation using css keyframes",
            "date_modified": "2017-05-25T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/store-function-inside-redux-store",
            "content_html": "[Redux](https://redux.js.org/) is a predictable state container for Javascript. Redux state has to be serializable all the time.\n\nObject serialization is the process of converting an object's state to a string from which it can later be restored.\n\nSo, if you are trying to store a   inside the Redux state, you need to serialize them before persisting.\n\n> Storing functions inside redux state is not a best practice in general. So try to avoid it.\n\nJavascript functions can be serialized quite easily, the challenge is in retrieving them from the store to execute.\n\nBelow are the helper functions for persisting functions inside Redux state.\n\n```js\n  //Returns a string\n  export const serializeFunction = (func) => (func.toString());\n  //serializeFunction(()=>console.log('Hello!!'))\n  // Output ==> \"()=>console.log('Hello!!')\"\n```  \n\nThe function to be stored in the state should be converted into string using serializeFunction.\n\n```js\n  //Returns a function\n  export const deserializeFunction = (funcString) => (new Function(\\`return ${funcString}\\`)());\n```\n\nConvert the string from the redux store into a function using deserializeFunction\n",
            "url": "https://prasanna.dev/posts/store-function-inside-redux-store",
            "title": "Storing a function in the Redux store",
            "summary": "Redux state can be very useful to share data across the application. This post is about storing a function inside the redux store.",
            "date_modified": "2017-05-17T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/jest-test-toLocaleString-javasscript",
            "content_html": "We had to use `toLocaleString` with a specific country-code. `toLocaleString('de')`. This works perfectly in all the browsers. However, not in jest tests.\n\nOur Jest tests were running with `--env=jsdom` I got to know that jsdom and phantomJS aren't supporting multiple locale implementations.\n\n[PhantomJS support locale-specific.](https://github.com/ariya/phantomjs/issues/12327)\n\nSo, the only solution I found is to mock these methods and test rest of the logic. Here is a sample mock behaviour.\n\n```js\nimport * as helpers from '../src/helpers';\ndescribe('formatDate', () => {\nit('should invoke localString implementation to format date ', () => {\n    const localStringMock = jest.fn();\n    const mockDate = { toLocaleString: localStringMock };\n    helpers.formatDate(mockDate);\n    expect(localStringMock).toHaveBeenCalledWith('de-DE', {\n            year: 'numeric',\n            month: '2-digit',\n            day: '2-digit',\n            hour: '2-digit',\n            minute: '2-digit',\n        });\n    });\n});\n```\n\n**Note:** This behaviour is applicable for toLocaleDateString() toLocaleTimeString()\n",
            "url": "https://prasanna.dev/posts/jest-test-toLocaleString-javasscript",
            "title": "Mock toLocaleString in Jest",
            "summary": "Found an issue while testing toLocaleString and other related JS prototype function. Described here is the way to mock them.",
            "date_modified": "2017-01-31T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/expire-session-after-timeout-spring",
            "content_html": "\nUsing Spring security we were building an application which has 2 types of users Internal and External. Our requirement was\n\n1.  Internal and External users have different idle timeouts.\n2.  External user's session should be invalidated after 30 mins. Irrespective of whether the user is active or not.\n\n#### Setting up Idle timeout in Spring security\n\nSpring provides out of box option to configure an idle timeout value. This invalidation is done by Spring security and happens while making a request after specified amount of time.\n\nWe were able to achieve this by setting up setMaxInactiveIntervalInSeconds on the session object while creation.\n\n#### Setting up Max timeout in Spring security\n\nThe above technique can be used only for setting the idle time. But our second scenario is to invalidate the session irrespective of whether the user is active or not.\n\nWe ended up writing a custom filter which to invalidate the session manually whenever the session age is greater than the specified value.\n\nThis filter will invalidate the session when the maximum time has reached for that session.\n",
            "url": "https://prasanna.dev/posts/expire-session-after-timeout-spring",
            "title": "Spring security session timeouts",
            "summary": "Setup session timeouts in spring security. This will explain how to setup the idle timeout and also the max timeout for separate sessions.",
            "date_modified": "2016-09-24T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/angular-resource-and-interceptors",
            "content_html": "\nOnce you set up your project with angular and [ngResource](https://docs.angularjs.org/api/ngResource) you will be able to access $resource object.\n\n$resource will serve as a factory which creates a resource object that lets you interact with RESTful services. You can call HTTP methods directly on this resource object.\n\nIn our application we will have a custom wrapper on top of the angular resource. This wrapper will provide ability for us to transform the object differently on success and error response.\n\nAll adapters will use the wrapper and all end points will overwrite the transform logic on success and error response.\n\n```js\n'profile': {\n    method: 'GET',\n        params: {accountId: '@accountId'},\n    transformRequest: (data) => {\n        const moreParams = {newParams: data};\n        return angular.toJson(moreParams);\n    },\n        successTransformResponse: (data, headers, status) => {\n        // Handle parsing for HTTP status 200.\n    },\n        errorTransformResponse: (data, headers, status) => {\n        // Depending on the status code handle transform logic.\n    }\n}\n```\n\n#### Handling generic error codes\n\nSo, next we have to handle generic error responses across the application. Error codes like 401 (Unauthorized), 503 (Service Unavailable) needs to be redirected to different pages.\n\nThe interceptors are service factories that are registered with the $httpProvider by adding them to the $httpProvider.interceptors array. The factory is called and injected with dependencies (if specified) and returns the interceptor.\n\n```js\n$provide.factory('myHttpInterceptor', function($q, dep1, dep2) {\n    return {\n        'request': function(config) {\n            // do something on success\n            return config;\n        },\n        'responseError': function(rejection) {\n            // do something on error\n            if (canRecover(rejection)) {\n                return responseOrNewPromise\n            }\n            return $q.reject(rejection);\n        }\n    }\n}\n\n```\n\nIn the responseError method block, we used to handle all the generic error response code across the application.\n\n#### Observation\n\nI was expecting the code in HttpInterceptor to be executed before my transform logic in the resource wrapper. But i was wrong. Only after the resource transformation http interceptors are called. (Refer this [issue.](https://github.com/angular/angular.js/issues/7594 ))\n\nSo, whenever a service responds with 500 error, Http interceptor will redirect the user to a different page. However, this will not happen if there is an error in transformation logic. In order to circumvent this problem, we started writing our error transform response specifically for the error codes. This means that, our transformation logic will not be executed for our generic error codes and eventually it reaches http interceptor.\n\n```bash\n{{ site.data.comments }}\n```\n",
            "url": "https://prasanna.dev/posts/angular-resource-and-interceptors",
            "title": "Angular resource and http interceptor",
            "summary": "This post describe about the use of angular resource library along with http interceptor.",
            "date_modified": "2016-07-17T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/post-errors-to-an-endpoint-angular",
            "content_html": "\nWe were looking for an efficient way of capturing all the Javascript errors from browsers in our backend so it appears in our Kibana dashboard along with the server logs\n\nWe had a Angular 1.5.8 application in front of multiple micro-services endpoint. Any error in the angular application will appear in the browser console and we planned to push these logs back to the server.\n\n#### Angular's _$exceptionHandler_\n\nIn order to catch all the exceptions, we have to override the $exceptionHandler component provided by angular. Only catch here is that, since we are overriding angular component we may not be able to inject $http or any other angular component in our overrides and doing so will throw a cyclic dependency issue.\n\n#### Initial solution\n\nWe came up with an idea of injecting $injector and fetching $http using the same.\n\n```js\nfactory('$exceptionHandler', \\['$log', '$window', '$injector',\n    ($log, $window, $injector)=> {\n        return (exception, cause) => {\n            $log.error(exception, cause);\n            try {\n                const $http = $injector.get('$http');\n                const logMessage = \\[{\n                    level: 'error',\n                    message: exception.toString(),\n                    url: $window.location.href,\n                    stackTrace: exception.stack,\n                    currentTimestamp: Date.now()\n                }\\];\n                $http.post('/log/message', logMessage);\n            } catch (loggingError) {\n                $log.log(loggingError);\n            }\n        );\n\n```\n\nThe above piece of code will work perfectly and will be able to post all the errors generated to an exposed endpoint.\n\nBut the problem is, if the $http.post throws any exception then it causes unrecoverable recursion and browser will hung.\n\nIn order to come out of that issue, we re wrote our http post logic using native JS syntax.\n\n#### Final solution\n\nSame code re written using native JS functions.\n\n```js\nfactory('$exceptionHandler', \\['$log', '$window', '$injector', ($log, $window, $injector)=> {\n    return (exception, cause) => {\n        $log.error(exception, cause);\n        try {\n            let commonHeaders = $injector.get('$http').defaults.headers.common;\n            const logMessage = \\[{\n                level: 'error',\n                message: exception.toString(),\n                url: $window.location.href,\n                stackTrace: exception.stack,\n                currentTimestamp: Date.now()\n            }\\];\n            let xmlhttp = new XMLHttpRequest();\n            xmlhttp.open('POST', '/log/message');\n            xmlhttp.setRequestHeader('Content-Type', 'application/json;charset=UTF-8');\n            for (let header in commonHeaders) {\n                if (commonHeaders.hasOwnProperty(header)) {\n                    let headerValue = commonHeaders\\[header\\];\n                    if (angular.isFunction(headerValue)) {\n                        headerValue = headerValue();\n                    }\n                    xmlhttp.setRequestHeader(header, headerValue);\n                }\n            }\n            xmlhttp.send(angular.toJson(logMessage));\n        } catch (loggingError) {\n            $log.log(loggingError);\n        }\n    };\n});\n\n```\n",
            "url": "https://prasanna.dev/posts/post-errors-to-an-endpoint-angular",
            "title": "Post browser logs to server in an Angular app",
            "summary": "This post describes about posting all the browser errors in an angular application to an endpoint. This will be helpful to analyse or debug issues.",
            "date_modified": "2016-06-24T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/expanding-amazon-ebs-volumes",
            "content_html": "I had an AWS image which was created using an EC2 instance of size 8 GB. Whenever i try to launch an instance i usually change the storage size to something say 20 GB. But once the system is launched when i do a\n```bash\ndf -h\n```\ni still see 8 GB and not 20 GB.\n\nOn further reading i understood i need to resize the disk size. So i did the same using\n```bash\nsudo resize2fs /dev/xvde1\n```\nBut i was getting the following error:\n\nThe filesystem is already \\*\\*\\* blocks long. Nothing to do!\n\nThen to reolve this issue i have to perform the following steps.\n\n*   SSH to the machine.\n```bash\nfdisk /dev/xvde\n```\n*   You should be seeing this message.\n\nWARNING: DOS-compatible mode is deprecated. It's strongly recommended to switch off the mode (command 'c') and change display units to sectors (command 'u')\n\n*   Enter 'u' to change display units\n*   Enter 'p' to view the current paritions.\n*   Enter 'd' to delete current partitions.\n*   Enter 'n' to create a new partition.\n*   Enter 'p' to set it as primary partitions.\n*   Enter '1' to set it as primary partitions.\n*   Set the desired space. If nothing is given the entire space is allotted.\n*   Enter 'a' to make it bootable.\n*   Enter '1' and 'w' to write and save the changes.\n*   Reboot the instance from AWS console.\n*   Now if you resize the parition it worked all fine.\n```bash\nsudo resize2fs /dev/xvde1\n```\nCheck the partition size, it should be all set with more space.\n",
            "url": "https://prasanna.dev/posts/expanding-amazon-ebs-volumes",
            "title": "Expanding Amazon EBS Volume in a EC2 instance.",
            "summary": "Even after increasing the size of the EBS volume in the AWS console, the actual size of the EBS wasn't increased. Have to follow the following steps to grow the EBS size.",
            "date_modified": "2016-02-03T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/capistrano-set-deployed-revision",
            "content_html": "\nWe use Capistrano to deploy our Rails application. Recently i upgraded our capistrano version from 2 to 3\n\nCapistrano 3 has a complete DSL changeover. Apart from this one other major change I figured out was the way a Git repository is been deployed.\n\nPreviously a Git repository is cloned in the deploy location. Now in Cap 3 a Git archive is been downloaded to the deploy location. This means the deploy directory is no more a Git repository. During Cap 2 times, we used to run a 'git log' command in the deployed driectory to find the deployed revision. Now after upgrade I am unable to do this.\n\nCap 3 has got a REVISION file, which contain the SHA of the deployed commit. This wasn't useful in our case, as we show this message in our web application.\n\nSo i ended up writing a Cap task using a similar logic to create a REVISION file with our custom formatted Git message.\n```ruby\n    namespace :deploy do\n      task :add\\_revision\\_file do\n        on roles(:app) do\n          within repo\\_path do\n            execute(:git, :'log', :\"--pretty=format:'%h | %ai | %d %s'\", :'-1',\n            :\"#{fetch(:branch)}\", \">#{release\\_path}/REVISION\")\n          end\n        end\n      end\n    end\n  \n\n    after 'deploy:updating', 'deploy:add\\_revision\\_file'\n\n```\nThis will overwrite the REVISION file created by Cap with our custom message. Which will be consumed by our application.\n",
            "url": "https://prasanna.dev/posts/capistrano-set-deployed-revision",
            "title": "Set deployed Git revision using Capistrano 3",
            "summary": "While deploying Rails application using Capistrano 3, recording the current deployed git revision to be used by Rails applicaiton.",
            "date_modified": "2016-01-02T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/url-generation-error-after-upgrading-rails",
            "content_html": "On upgrading my rails app from 4.0 to 4.2.5 i steeped onto a wierd issue where my form\\_for tag breaks and starts throwing exception.\n\nA REST model on new action raised an UrlGenerationError exception because of the form\\_for tag.\n\nFor ex: if User is a model my form\\_for looked like this\n```ruby\n    form\\_for(@user, url: user\\_path(@user)) do |f|\n```\n\nRaised exception was\n```bash\n    No route matches {:action=>\"show\", :controller=>\"users\", :id=>nil} missing required keys: \\[:id\\]\n```\n\nThe @user object’s id is nil since it’s not yet saved in the database. Previously if it was nil that is been skipped by the the url generation. All these occurrences started throwing errors.\n\nI have to change the form\\_for tag to\n```ruby\n    form\\_for(@user) do |f|\n```\n\nThis posts the form to default users\\_path.\n\n### Nested objects:\n```ruby\n    form\\_for(@user, url: user\\_address\\_path(@user, @address)) do |f|\n```\n\nwas changed to\n```ruby\n    form\\_for(\\[@user, @address\\]) do |f|\n```\n",
            "url": "https://prasanna.dev/posts/url-generation-error-after-upgrading-rails",
            "title": "UrlGenerationError after upgrading to Rails 4.2",
            "summary": "On a REST model new action, form_for tag breaks and raises UrlGenerationError after upgrading to Rails 4.2",
            "date_modified": "2015-12-08T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/add-ckeditor-to-rails-pipeline",
            "content_html": "\nWe are using [ckeditor](http://ckeditor.com/) in our rails application (Rails 4.2).\n\nNumber of network calls made by the ckeditor and its plugins are quite alot and we were facing difficulty in integrating them with the Rails asset pipeline.\n\nMy initial approach is to use a [ckeditor rails gem](https://github.com/tsechingho/ckeditor-rails). However getting it to work was complicated. On top of it we had some custom plugins written for ckeditor and making it to work with ckeditor rails gem was almost impossible.\n\nTaking some pointers from this [issue](https://github.com/galetahub/ckeditor/issues/307) finally could get into some working solution.\n\n1.  Move all the CKEditor files into vendor/assets/javascript/ckeditor\n2.  In application.js add\n\n    //= require ckeditor/ckeditor\n\n3.  ckeditor.js looks up for other ckeditor relative to CKEDITOR\\_BASEPATH location. So before loading ckeditor in JS add a line to set that environment variable.\n\n    window.CKEDITOR\\_BASEPATH = '/assets/ckeditor/';\n\n4.  Add\n\n    config.assets.precompile << \\['ckeditor/\\*'\\]\n\n    to your application.rb file.\n5.  Finally add a file called precompile\\_hook.rake This rake task will help in compiling the ckeditor files and add it to the assets folder. The content of the rake task is here. [precompile\\_hook.rake](https://gist.github.com/prasann/c8978041777cb443fb77)\n\n\n\nHere is the screenshot of the network calls before and after adding ckeditor to asset pipeline.\n\n\n\n[![Before adding to asset pipeline](/assets/images/posts/add_ckeditor_to_rails/thumbs/before.png)](/assets/images/posts/add_ckeditor_to_rails/full/before.png \"Before adding to asset pipeline\") [![After adding to asset pipeline](/assets/images/posts/add_ckeditor_to_rails/thumbs/after.png)](/assets/images/posts/add_ckeditor_to_rails/full/after.png \"After adding to asset pipeline\")\n\nEven after adding ckeditor to asset pipeline the it did not effectively reduce all calls into one. Still the ckeditor's plugin calls are been fired separately.\n",
            "url": "https://prasanna.dev/posts/add-ckeditor-to-rails-pipeline",
            "title": "Integrating CKEditor with Rails asset pipeline.",
            "summary": "Integrating CKEditor plugin into rails asset pipeline.",
            "date_modified": "2015-05-26T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/deploying-jekyll-blog-through-rake-script",
            "content_html": "\nThis blog is powered by Jekyll and I use Github pages as web server.\n\n#### Branch structure in Github\n\nGithub by default publish the contents of the master branch as a github page. So i have created two branches in the repository.\n\n**source:** contains Jekyll based folder structure. \\_drafts, \\_posts, \\_site etc. All the new posts are added in the drafts folder first and then once its written fully it is then moved to \\_posts folder and are then ready to be published.\n\n**master:** is simply a generated content from the rake script. This branch has all the HTML files that are generated using Jekyll gem.\n\n#### Folder structure in Dev box\n\nI have both the branches checked out in different folders. Both these folders are present in the same level (will be useful while generating output)\n\n#### Rake script\n\n##### To generate HTML\n\nI have the Rakefile in the root level of my source branch. The rake task mentioned below will create HTML equivalent inside the \\_site folder.\n\n```ruby\ntask :generate do\nJekyll::Site.new(Jekyll.configuration({\n    \"source\"      => \".\",\n    \"destination\" => \"\\_site\"\n    })).process\nend\n```\n\n##### To publish in Github\n\nThis task copies the entire \\_site folder into the master branch (locally). This is why i need to checkout both master and source branches separately and keep them in the same level.\n\nAfter copying the contents, simply it switches to the master branch and does a git push.\n\nOnce the changes are pushed into github's master branch the changes are then reflected in your site immediately.\n\n```ruby\ntask :publish => \\[:generate\\] do\n    cp\\_r \"\\_site/.\", LOCAL\\_DIR\\_NAME\n    cp \".travis.yml\", LOCAL\\_DIR\\_NAME\n    pwd = Dir.pwd\n    Dir.chdir LOCAL\\_DIR\\_NAME\n    system \"git add --all\"\n    message = \"Site updated at #{Time.now.utc}\"\n    system \"git commit -m #{message.inspect}\"\n    system \"git push origin master:refs/heads/master\"\n    Dir.chdir pwd\nend\n```\n",
            "url": "https://prasanna.dev/posts/deploying-jekyll-blog-through-rake-script",
            "title": "Deploying Jekyll site for Github pages through rake script",
            "summary": "Deploying jekyll blog or site for Github pages using rake script.",
            "date_modified": "2014-08-30T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/integrating-netflix-hystrix-to-a-spring-application",
            "content_html": "\nAfter reading Martin Fowler's [Circuit breaker post](http://martinfowler.com/bliki/CircuitBreaker.html) i thought of implementing the same to my application. While looking for possible approaches to this solution i stepped onto Hystrix.\n\n[Hystrix](https://github.com/Netflix/Hystrix) is a latency and fault tolerance library designed to isolate points of access to remote systems, services and 3rd party libraries, stop cascading failure. Using this implementing CircuitBreakers are quite straight forward.\n\nTo start with i wanted to measure the latency of the 3rd party calls that goes through my application. Mine was Java Spring applcation running in Tomcat. As per Hystrix documentation all the third party calls that need to be monitored are to be wrapped within a command. This command will be executed in a separate thread. Since all the 3rd party calls go through this layer it is easy to monitor those calls. It is also possible to define a fallback approach when a particular service call fails. And there by isolating these scenarios from the application code. More of how this works is explained in detail on [Hystrix wiki](https://github.com/Netflix/Hystrix/wiki)\n\nThe problem i had was i already have my application up and running. And all i need to do is just monitoring the 3rd party calls (as of now) Now integrating Hystrix meant i need to re design the 3rd party calls to introduce a middle layer to wrap them up.\n\nI was thinking of writing Aspect based solution to wrap these calls throughout my application with an annotation.\n\nTo do this, make sure you have included Spring AOP in your application. Declare and define an annotation as shown below.\n\n```java\n@Aspect\n@Component\npublic class CircuitBreakerAspect {\n    @Around(\"@annotation(com.example.Monitor)\")\n    public Object monitoringAround(final ProceedingJoinPoint aJoinPoint) throws Throwable {\n        String theShortName = aJoinPoint.getSignature().toShortString();\n        HystrixCommand.Setter theSetter =\n                HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(theShortName));\n        theSetter = theSetter.andCommandKey(HystrixCommandKey.Factory.asKey(theShortName));\n        HystrixCommand theCommand = new HystrixCommand(theSetter) {\n            @Override\n            protected Object run() throws Exception {\n                try {\n                    return aJoinPoint.proceed();\n                } catch (Exception e) {\n                    throw e;\n                } catch (Throwable e) {\n                    throw new Exception(e);\n                }\n            }\n        };\n        return theCommand.execute();\n    }\n}\n```\n\nUsing Hystrix dashboard you can able to wire the views and could able to monitor the application. More about this is written [here](http://www.mirkosertic.de/doku.php/architecturedesign/springhystrix)\n\nHowever this annotation approach can be used only for monitoring purposes or when you need to do similar actions for all the services. When i moved onto writing circuit breakers this cannor be done since all the service calls need its own fallback approaches. So in that case it was better to implement them as separate commands so all the code will fit within.\n",
            "url": "https://prasanna.dev/posts/integrating-netflix-hystrix-to-a-spring-application",
            "title": "Integrating Netflix Hystrix to a Spring Application",
            "summary": "Hystrix is a latency and fault-tolerance library from Netflix. This post describes how to integrate it with Spring Aspects to make the implementation simpler.",
            "date_modified": "2014-07-14T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/testing-apiary-using-github-travis",
            "content_html": "\n[API Blueprint](http://apiblueprint.org/) is a documentation-oriented API description language. A couple of semantical assumptions over the plain Markdown.\n\n[Dredd](https://github.com/apiaryio/dredd) is a command-line tool for testing API documentation written in API Blueprint format against its backend implementation.\n\nI could able to setup dredd quite easily on my Mac by installing Node and npm. However its not quite straight forward in Windows. I faced lots of difficulties while installing node, npm and dredd.\n\nSo i decided to use [Travis](http://travis-ci.org/) to setup the testing pipeline for my jobs. All i needed to do is to have a .travis.yml file to install node\\_js and install dredd using npm.\n\nAdded a simple script file to run the dredd tool inside the job. And that's it. As part of the code i also checked in the API markdown files which will run againt the APIs\n",
            "url": "https://prasanna.dev/posts/testing-apiary-using-github-travis",
            "title": "Testing APIary using Dredd.",
            "summary": "Test API blueprint mardown files by simply hosting them on GitHub and setting up a pipeline in Travis CI.",
            "date_modified": "2014-07-01T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/making-https-call-using-apache-httpclient",
            "content_html": "\nThis post details about making Secure HTTP(HTTPs) call from a server using Apache HTTPClient library.\n\nThe simplest will be to ignore the ssl certificates and to trust any connection. This approach is not acceptable for production code as it defeat the purpose of using HTTPS. However in some use cases if you want to try out something quickly you can go with this route.\n\n#### Trust any certificate approach (Simple, not recommended for production code.)\n```java\nimport javax.net.ssl.SSLContext;\nimport javax.net.ssl.X509TrustManager;\n\nimport org.apache.http.client.HttpClient;\nimport org.apache.http.conn.ssl.SSLConnectionSocketFactory;\nimport org.apache.http.conn.ssl.SSLContexts;\n\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\n\nimport java.security.SecureRandom;\n\npublic class HttpClientFactory {\n\n    private static CloseableHttpClient client;\n\n    public static HttpClient getHttpsClient() throws Exception {\n\n        if (client != null) {\n            return client;\n        }\n        SSLContext sslcontext = SSLContexts.custom().useSSL().build();\n        sslcontext.init(null, new X509TrustManager\\[\\]{new HttpsTrustManager()}, new SecureRandom());\n        SSLConnectionSocketFactory factory = new SSLConnectionSocketFactory(sslcontext,\n                SSLConnectionSocketFactory.BROWSER\\_COMPATIBLE\\_HOSTNAME\\_VERIFIER);\n        client = HttpClients.custom().setSSLSocketFactory(factory).build();\n\n        return client;\n    }\n\n    public static void releaseInstance() {\n        client = null;\n    }\n}\n```\nThe above method will return httpClient object which can be used to make any HTTPS calls. Performing HTTPS call is no different from making HTTP call from now on. So you can have a factory with two methods, one for secure and one for non-secure.\n\nHere we have used HttpsTrustManager, which will do nothing more than trusing all clients. This is done by simply implementing X509TrustManager and auto generating all the methods.\n```java\nimport java.security.cert.CertificateException;\nimport java.security.cert.X509Certificate;\n\nimport javax.net.ssl.X509TrustManager;\n\npublic class HttpsTrustManager implements X509TrustManager {\n\n\t@Override\n\tpublic void checkClientTrusted(X509Certificate\\[\\] arg0, String arg1)\n\t\t\tthrows CertificateException {\n\t\t// TODO Auto-generated method stub\n\n\t}\n\n\t@Override\n\tpublic void checkServerTrusted(X509Certificate\\[\\] arg0, String arg1)\n\t\t\tthrows CertificateException {\n\t\t// TODO Auto-generated method stub\n\n\t}\n\n\t@Override\n\tpublic X509Certificate\\[\\] getAcceptedIssuers() {\n\t\treturn new X509Certificate\\[\\]{};\n\t}\n\n}\n```\n#### Importing a keystore (Recommended)\n\nIf you are writing produciton quality code, then you should be looking at this approach. Have a all the keys in your application and create a SSLContext using those keystores. The created SSLContext can then be injected to SSLConnectionSocketFactory and remaining steps will be the same.\n```java\nimport javax.net.ssl.SSLContext;\n\nimport org.apache.http.client.HttpClient;\nimport org.apache.http.conn.ssl.SSLConnectionSocketFactory;\nimport org.apache.http.conn.ssl.SSLContexts;\n\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.security.KeyStore;\nimport java.security.KeyStoreException;\nimport java.security.NoSuchAlgorithmException;\nimport java.security.cert.CertificateException;\nimport java.security.KeyManagementException;\n\npublic class HttpClientFactory {\n\n    private static CloseableHttpClient client;\n\n    public static HttpClient getHttpsClient() throws Exception {\n\n        if (client != null) {\n            return client;\n        }\n        SSLContext sslcontext = getSSLContext();\n        SSLConnectionSocketFactory factory = new SSLConnectionSocketFactory(sslcontext,\n                SSLConnectionSocketFactory.BROWSER\\_COMPATIBLE\\_HOSTNAME\\_VERIFIER);\n        client = HttpClients.custom().setSSLSocketFactory(factory).build();\n\n        return client;\n    }\n\n    public static void releaseInstance() {\n        client = null;\n    }\n\n    private SSLContext getSSLContext() throws KeyStoreException, \n    NoSuchAlgorithmException, CertificateException, IOException, KeyManagementException {\n        KeyStore trustStore  = KeyStore.getInstance(KeyStore.getDefaultType());\n        FileInputStream instream = new FileInputStream(new File(\"my.keystore\"));\n        try {\n            trustStore.load(instream, \"nopassword\".toCharArray());\n        } finally {\n            instream.close();\n        }\n        return SSLContexts.custom()\n                .loadTrustMaterial(trustStore)\n                .build();\n    }\n}\n```\nThe only difference between the two approaches are the way the SSLContext been created.\n",
            "url": "https://prasanna.dev/posts/making-https-call-using-apache-httpclient",
            "title": "Making HTTPS call using Apache HttpClient.",
            "summary": "Perform Https calls from server using Apache HttpClient library.",
            "date_modified": "2014-06-26T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/skeleton-gradle-spring-mvc-app",
            "content_html": "\nWill try to list down few of my learnings to set up a Spring MVC app with Gradle. And also this time i tried using Servlet 3.0 spec which means no .xml files for Spring configuration.\n\nRefer to the [GitHub repo](https://github.com/prasann/GradleSpringApp) for the source code. I will be giving some notes on the code.\n\n#### Create build.gradle file\n\nbuild.gradle can be pretty much simple to start with. I started with adding java and war plugin followed by adding dependencies to the Spring artifacts. I have to define\n\nruntime 'javax.servlet:jstl:1.2'\n\nto make sure it doesn't get packaged as part of the war.\n\nThen thought it will be awesome to start the application in one command instead of building the war and deploying it in local instance. After some initial searching landed onto this [Cargo plugin](https://github.com/bmuschko/gradle-cargo-plugin) This lets you to configure the server of your choice and get it working. So after doing some basic configuration got this working.\n\nNow\n\ngradle war cargoRunLocal\n\nsince the task name is not so user friendly, just added an alias to it.\n\ntask serve(dependsOn: cargoRunLocal) << {\n}\n\n\n#### Setup Spring\n\nI decided to play around with Servlet 3.0 style of Spring configuration. This means that i do not need to create web.xml or applicationContext.xml files. Instead i can go with complete Java style configuration.\n\nApplication containers (tomcat 7+ in my case) will look for implementation of WebApplicationInitializer and will load that class on the startup. Initializer.java in my src will be equivalent for web.xml\n\nMvcConfig.java will be equivalent to applicationContext.xml file. This contains all the bean initialization, property place holders and more.\n\nAs you can see most of the configurations are handled by annotations.\n\n#### Setup Unit Tests\n\nSetting up Unit tests are no different to Gradle. As i mentioned i have used Java style configuration for my Spring classes. So the style of testing my controllers will also be different.\n\nInitControllerTest.java will be my controller test. I have initialized a mock web application in the\n\n@Before\n\nmethod and the rest of the stuff are handled by annotations.\n\n#### Setup Logging\n\nSetting up slf4j is quite straight forward. You have to add slf4j-log4j, log4j jars and add a log4j.properties to the\n\nsrc/main/resources\n\nIn the log4j.properties you can define the way your appenders should work.\n",
            "url": "https://prasanna.dev/posts/skeleton-gradle-spring-mvc-app",
            "title": "Gradle, Spring MVC App.",
            "summary": "A skeleton sample app demostrating gradle set up with Spring MVC along with basic logging and deployment in tomcat environment.",
            "date_modified": "2014-06-14T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/cross-site-http-requests",
            "content_html": "\nBrowsers have a default security mechanism to prevent http(s) request from one domain to other. Since there are tons of possibilities to misuse them.\n\nThe same origin policy prevents a document or script loaded from one origin from getting or setting properties of a document from another origin. This policy dates all the way back to Netscape Navigator 2.0.\n\nHowever, there are lots of genuine use cases for this scenario to occur which got to be handled by the application and the browsers.\n\nI had to face one such genuine case, and got to deal with one of the miserable browser of my time IE :(\n\nJust documenting few techniques to overcome this problem. I am not elaborating the techniques since that can be figured out, all i wanted is to document the possible solutions and when to use them.\n\n#### 1\\. CORS - Cross Origin Resource sharing\n\nA detailed description of how to implement is on this [Mozilla docs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS)\n\nThis is a very common and an elegant solution that you can find on the net for this problem. All you will need is to add few headers on the application server. This will allow application to allow requests from other application (domain). And also all the response from the parent application will include a header\n\nAccess-Control-Allow-Origin:\n\nWhich will tell the browser to allow those specified domains to talk to the application.\n\nThis solution works well with most of the modern browsers. Check out for the CORS browser support [here.](http://caniuse.com/cors)\n\n**Limitations with IE:**\n\nAs you can see the browser compatibility chart, IE8 and IE9 has a partial implementation to CORS.\n\nModern browsers will be able to support CORS for XMLHttpRequest. However IE8 and IE9 supports CORS using XDomainRequest object. What this means is that they have few limitations of their own.\n\nSome of the most important constraints are,\n\n*   Your requests should be only GET, POST HTTP methods and not PUT, DELETE etc.\n*   Both the domain (the calling and the caller) uses the same protocol. Either HTTP or HTTPS.\n*   Your request should not have any custom headers.\n\nThe exhaustive list is been detailed out in this [MSDN blog](http://blogs.msdn.com/b/ieinternals/archive/2010/05/13/xdomainrequest-restrictions-limitations-and-workarounds.aspx).\n\n**Workaround for IE**\n\nIf you think you can live with the constraints mentioned above, then the workaround is quite simple. You got to change all the XHR to XDR to make it work. Luckily if you are using jQuery you don't need to go through changing all the requests. Instead you can use this [jQuery plugin](https://github.com/MoonScript/jQuery-ajaxTransport-XDomainRequest) . I guess there are more of these available just check out before breaking your brains.\n\n#### 2\\. JSONP Solution\n\nUsing [JSONP](http://json-p.org/) response instead of JSON response.\n\n**Advantage:** No need of any specific workaround for IE8.\n\n**Limitation:** Works only for HTTP(S) GET request. If you are planning to use POST/PUT/DELETE this solution is not for you.\n\n#### 3\\. iFrame Hack.\n\nThis is a creepy hack. Lets say, if you want to make a call from appA to appB. In appA's landing page load a hidden iFrame with some URL of appB. Then perform all the requests to appB from that iFrame. Since iFrame's domain is appB browsers' will not complain.\n\n**Limitation:** Here the challenge is to consume the response. Your landing page should wait for an even in the iFrame and should consume the iFrame content. Don't even think of this solution if you want to make more than 1 cross site request in a page.\n\n#### 4\\. Reverse Proxy solution\n\nIf you want your appA to make a call to appB. Set up a simple reverse proxy to the appA. And use relative paths for the Ajax requests, while the server would be acting as a proxy to any remote location.\n\nSo in appA the relative path of the request will be _/cors-ajax_. The browser will not complain since this is not pointing to a different domain. And the reverse proxy rule will redirect anything of _cors-ajax_ to appB.\n\nMore reference to this implementation:\n\n*   [Configuring the Proxy](http://www.askapache.com/htaccess/reverse-proxy-apache.html#Configuring_Proxy)\n*   [Configuring Mod Proxy - SO](http://stackoverflow.com/questions/7807600/apache-mod-proxy-configuring-proxypass-proxypassreverse-for-cross-domain-ajax)\n\n**Limitation:** The server config are quite hard (at least for me) to understand and perform.\n\n#### 5\\. App based solution\n\nThis solution is very similar to that of the reverse proxy but you don't need to make any server config changes. The initial CORS approach sounds reasonable, but few limitations like same protocol might stop us from using it. Applications like [AnyOrigin](http://anyorigin.com/), [WhateverOrigin](http://whateverorigin.org/) does that for you. They support http and https so you can use the protocol of the main window and consume the response. If you feel unsafe of using a different domain, you can deploy it in your own infrastructure.\n\n**Limitation:** One more app to maintain :(\n\n#### 6\\. Add a generic controller/servlet in your parent domain.\n\nHave a controller/servlet in your app which actually does the external domain call. Have only one GET, POST method. Keep posting all your requests to the same end-point with an additional header containing the actual end-point. Inside the method extract the header, make a call and go around about it. This means that browser doesn't know its an external domain call as your app will serve as a wrapper to that external domain call.\n\n**Limitation:** Multiple HTTP calls for single request/response.\n\n**More on:** [How to circumvent same origin policy?](\nhttp://stackoverflow.com/questions/3076414/ways-to-circumvent-the-same-origin-policy)\n",
            "url": "https://prasanna.dev/posts/cross-site-http-requests",
            "title": "Cross Site HTTP(S) Requests - CORS Issue",
            "summary": "Some tried out solutions for the cross site request issue. Should be a good place to look out for which solution to be used under a circumstance.",
            "date_modified": "2014-06-01T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/re-running-testng-failed-tests",
            "content_html": "We were running our Selenium functional tests using [testNG](http://testng.org/doc/index.html) runner in [Jenkins](http://jenkins-ci.org/). However the problem was we were having too many failures on our initial run, and lot of these failures were classified as random or not reproducable. Mainly these are test script issues. Some times the testers tend to put some static wait conditions which might work on their machine but not in the Jenkins agent. Sometimes the environment against which our tests run might be a bit slow which pushes our pass percentage well behind. The ideal appropriate fix will be is to go through the failed test cases and figure out the randomness and fix it. But we thought of adding a re run mechanism to our test job to identify these random failures. As the no. of test cases grew, we ended up in re running the failed tests multiple times, something like this.\n\nName Total Failed\n\nInitialRun  100 30\n\nReRun1 30 20\n\nReRun2 20 10\n\nIn this case we will interpret the last 10 failures as a legitimate failures and rest as intermittent ones.\n\nPlease note that this is not a correct approach to reduce the failure count. This make the testers more lazy since they always analyse only the failed tests in the final run. And this increases the running time of the job inadvertently since the sure fail cases runs for n times and failing always.\n\n### Re Running testNG failed tests using ANT\n\nAfter every test run, testng will create a file called testng-failed.xml in the report directory which will contain the failed tests of that run. There was an issue with this file however. If you define multiple suites in your initial test suite file then this outer testng-failed.xml will contain the failed tests of the first run alone. The remaining suite's failed tests will be in the inner directories under their corresponding suite names.\n\nThis testng-failed.xml will also inherit all the properties from the original test suite file. For example if we have defined thread-count value then the same value will be retained.\n\nSo what I did was copied this file to a location and fed this to the testng task to run again. This can be achieved by any means, since we were using ant as our build tool i configured this in our build.xml file itself.\n```xml\n\t<target name=\"runTests\" depends=\"compile\" description=\"Running tests\">\n\t\t<echo>Running Tests...</echo>\n\t\t<taskdef resource=\"testngtasks\" classpath=\"lib/testng-6.8.jar\" />\n\t\t<testng outputDir=\"${report.dir}\" useDefaultListeners=\"true\" classpathref=\"build.classpath\" \n\t\t\tlisteners=\"org.uncommons.reportng.HTMLReporter,org.uncommons.reportng.JUnitXMLReporter\">\n\t\t\t<classpath location=\"${class.dir}\" />\n\t\t\t<xmlfileset dir=\".\" includes=\"testng-suite.xml\" />\n\t\t\t<sysproperty key=\"org.uncommons.reportng.title\" value=\" Test report\" />\n\t\t\t<sysproperty key=\"properties\" value=\"${properties}\" />\n\t\t</testng>\n\t\t<copy file=\"${report.dir}/testng-failed.xml\" todir=\"${basedir}/test-output-rerun/0\"/>\n\t\t\n\t\t<antcall target=\"multiReRun\"/>\n\t</target>\n\t<target name=\"multiReRun\" description=\"Multiple rerun tests\">\n\t\t<antcall target=\"runFailedTests\">\n\t\t\t<param name=\"rerun.report.dir\" value=\"${rerun.base.dir}/1\"/>\n\t\t\t<param name=\"src.rerun.dir\" value=\"${rerun.base.dir}/0\"/>\n\t\t</antcall>\n\t\t<antcall target=\"runFailedTests\">\n\t\t\t<param name=\"rerun.report.dir\" value=\"${rerun.base.dir}/2\"/>\n\t\t\t<param name=\"src.rerun.dir\" value=\"${rerun.base.dir}/1\"/>\n\t\t</antcall>\n\t</target>\n```\n\nHere output folder of the init run will be test-output. And am copying testng-failed.xml from test-output to test-output-rerun/0 . This is just to make my multiReRun more convenient. Now i have repeated the block of code for two times. This is due to the fact that ant doesn't support the regular for..counter loop.\n\nWhen i went through some ant docs figured out that ant script supports JavaScript!! May be i can use that to constuct a string like \"1,2,3,4,5\" and pass it onto the for loop of ant.\n",
            "url": "https://prasanna.dev/posts/re-running-testng-failed-tests",
            "title": "Re running testNG failed times n times.",
            "summary": "Steps to set up re-runn of testNG failed tests for n number of times, using ant task.",
            "date_modified": "2014-04-30T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/wiring-jasmine2-with-phantom",
            "content_html": "\nBriefed out the steps that i did to run my Jasmine test suite in my CI.\n\n1.  Download phantomjs.exe (Our CI server was running in a Windows server). [Download link](http://phantomjs.org/download.html)\n2.  Use [run-jasmine.js](https://gist.github.com/prasann/9972777). This runner code is taken from phantomJS example and modified to run Jasmine 2.0 and to format the output as we needed.\n3.  Assuming phantomjs.exe, run-jasmine.js and SpecRunner.html (Specrunner file) are in the same level in a directory, execute this command\n```bash\n phantomjs.exe run-jasmine.js SpecRunner.html \\[--debug\\]\n```\n\nThe --debug is optional. If run on the debug mode it will print the stack trace of the failed specs and also prints all the specs that are been executed.\n\nSpecRunner.html is very similar to the one that comes along with Jasmine 2.0 samples. The SpecRunner.html that comes with Jasmine 1.3 will not work, as the way of booting Jasmine is changed in the latest version. The only changes I made to the SpecRunner.html is to modify my src and spec file locations.\n",
            "url": "https://prasanna.dev/posts/wiring-jasmine2-with-phantom",
            "title": "Wiring Jasmine 2.0 with Phantom JS",
            "summary": "This post describes the steps that are necessary for to wire Jasmine 2.0 test suites with phantomJS.",
            "date_modified": "2014-04-28T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/utc-time-android-device-ntp-server-sync",
            "content_html": "\nI had a requirement to persist the current UTC time of a request in Android device for future reference.\n\nGetting the time from the Android device and converting it to UTC will not be efficient since, user might have set wrong time in the device and it might mislead the data.  \nSo we decided to sync the device with NTP server before converting the time to UTC.\n\n**Step 1 :** Copy this [SntpClient.java](https://gist.github.com/prasann/9003350 \"SntpClient.java\") into your source.  \n**Step 2 :** The SntpService.java to compute the current UTC is here below.\n\n```java\npublic String getUTCTime(){\n        long nowAsPerDeviceTimeZone = 0;\n        SntpClient sntpClient = new SntpClient();\n        if (sntpClient.requestTime(\"0.africa.pool.ntp.org\", 30000)) {\n            nowAsPerDeviceTimeZone = sntpClient.getNtpTime();\n            Calendar cal = Calendar.getInstance();\n            TimeZone timeZoneInDevice = cal.getTimeZone();\n            int differentialOfTimeZones = timeZoneInDevice.getOffset(System.currentTimeMillis());\n            nowAsPerDeviceTimeZone -= differentialOfTimeZones;\n        }\n        return DateUtils.getFormattedDateTime(new Date(nowAsPerDeviceTimeZone));\n    }\n```\nSome more details on SntpService code:\n\nConnect to any of the prominent ntp servers. There were lots of recommendation to place this in config file, however i thought it doesn't make sense for Android since i have to repackage this anyways.\n\n```\nsntpClient.getNtpTime()\n```\n\ngives you the current NTP time as per the device time zone.\n\nThen identify the device's time zone,\n```\ncal.getTimeZone()\n```\nand calculate the offset difference between UTC and the current device time.\n\n```\nDateUtils.getFormattedDateTime(date)\n```\n\nis our custom method to format date into String.\n",
            "url": "https://prasanna.dev/posts/utc-time-android-device-ntp-server-sync",
            "title": "UTC time in Android device. With NTP server sync.",
            "summary": "Using NTP time in the anroid application. This involves calling the SNTP server and also converting the time to UTC format.",
            "date_modified": "2014-02-13T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/whats-new-in-apple-passbook-ios7",
            "content_html": "\nI'm currently working in a project that integrated with iOS Passbook application to deliver digital passes to their users. Spent some time in investigating the changes to passbook application in iOS7 (beta 4). Here is a quick summary of it.\n\nApple listened to its user's feedback and have come with some features which increases the usability of the application.\n\n**Add multiple passes to Passbook.**\n\nYes, you can able to add multiple passes to the Passbook application in one go. So, if you are issuing multiple passes to your users, probably have a page with all the passes and you can have download all link, which downloads all the passes in one shot.\n\n**Delivery through Barcode.**\n\nApple have added one more delivery mechanism to the passbook. Currently you can deliver a pass through Safari browser (with vnd.apple.pkpass as header) or Through email attachment or to stream from your native iOS application. Now in the new passbook app they have added a barcode scanner. So the content of the pass can be crisped into a barcode and can be delivered to the Passbook application.\n\n**Anchor tags at the back of the pass.**\n\nCurrently you can have links, but you cannot have link text. For example if you want to link [http://www.google.com](http://google.com) to 'Click here' it is not possible. But it will be possible from iOS7\n\n**Expiration date for Passes.**\n\nNow passes can have its own expiration date. It's a meta data that you can set and after that the passes will be destroyed from the Apple passbook automatically.\n\n**Usage restriction by Geo location.**\n\nYou can restrict the usage of passes, within a specific geo location. For example if you are issuing a coupon, then you can make sure that your users could able to access the coupons within a specific geo co-ordinates.\n\nAll these are nice features that are provided in iOS7. So far i was happy with all these news, until i read that, ([Dev forum link](https://devforums.apple.com/thread/190987?tstart=0))\n\n**The rendering algorithms are significantly different from the previous versions.**\n\nThis seems to be a major issue to me, since i have to now test the appearance of my pass with new version. And have to think about optimising the design across all the versions.\n",
            "url": "https://prasanna.dev/posts/whats-new-in-apple-passbook-ios7",
            "title": "What's new in Apple Passbook iOS7",
            "summary": "New features in passbook application in the iOS7.",
            "date_modified": "2013-08-06T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/setting-up-cucumber-jvm",
            "content_html": "\n[Cucumber JVM](https://github.com/cucumber/cucumber-jvm \"Cucumber JVM\") is Java implementation of Cucumber BDD.\n\n### **Integrating into the Project**\n\nThe installation using maven is super simple, just add the dependency and you are ready to go. Make sure you add both command line interface (cucumber-core) and the IDE interface (cucumber-junit)\n\nI was using Intellij and add Intellij Cucumber plugin, to make the navigations easier.\n\nOne thing i liked very much is the ability to add custom annotations to the feature. You can add a custom annotation and can create Before and After hook for them.\n\nIn `.feature` file\n```\n@Email\nFeature:\n```\nIn the step definitions file.\n```\n@Before({\"@Email\"})\n@After({\"@Email\"})\n```\n### **Integrating with Spring**\n\nFor Spring integration you need to add one more component of the cucumber-jvm _(cucumber-spring)_\n\nIt is advisable to have a test runner class which can run all the feature files in one go especially when you are runnning in the CI.\n\nThe structure of the test runner class will be :\n```java\n@RunWith(Cucumber.class)\npublic class CucumberAdapterTest {\n}\n```\nMake sure to place all the feature files in the same package as of this Runner class. Or you can specify the path using the cucumber options, like this.\n```java\n@RunWith(Cucumber.class)\n@Cucumber.Options(features = \"classpath:\\*\\*/\\*.feature\")\npublic class CucumberAdapterTest {\n}\n```\n\nIf you are placing all the step definition in other package you can add that to the annotation using glue attribute.\n```java\n@RunWith(Cucumber.class)\n@Cucumber.Options(features = \"classpath:\\*\\*/\\*\", glue = {\"path of the step definitions\"})\npublic class CucumberAdapterTest {\n}\n```\nThis will look up for cucumber.xml file in the classpath. This xml file can hold all the bean definitions. My cucumber.xml was super simple.\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxmlns:context=\"http://www.springframework.org/schema/context\"\nxsi:schemaLocation=\"http://www.springframework.org/schema/beans\nhttp://www.springframework.org/schema/beans/spring-beans-3.0.xsd\nhttp://www.springframework.org/schema/context\nhttp://www.springframework.org/schema/context/spring-context-3.0.xsd\">\n\n    <import resource=\"classpath\\*:/application-context.xml\"/>\n\n    <context:component-scan base-package=\"path of the step definition\"/>\n    <context:annotation-config/>\n</beans>\n```\n\nThe step defnitions can lie in a different package and make sure you use glue attribute to wire them in the Runner class.\n\n```java\npublic class StepDefinitions {\n@Autowired\nEntityRepository entityRepository;\n\n\t@Given(\"^Register a user$\")\n\tpublic void registerUser() throws Throwable {\n\n\t}\n}\n```\n### **Integrating with Spring Transactions**\n\nOne last thing that i wanted to do is to hook up Spring transactions. So all the data created by the features have to be removed after the test completes. So you can write independent tests without bothering about the data.\n\nYou can use '_txn_' annotation that comes with Cucumber-JVM. All you need to do is to wire up that package along with your adapter class.\n```java\n@RunWith(Cucumber.class)\n@Cucumber.Options(glue = {\"cucumber.api.spring\"})\npublic class CucumberAdapterTest {\n}\n```\n\nand\n\n```\n@txn\nScenario: Some scenario to test\n```\n",
            "url": "https://prasanna.dev/posts/setting-up-cucumber-jvm",
            "title": "Setting up Cucumber-jvm",
            "summary": "Setting up cucumber BDD framework in your Java project.",
            "date_modified": "2013-07-09T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/link-your-sublime-text-2-instances-with-dropbox",
            "content_html": "\nAfter started using **Sublime Text 2 (ST2)**, was completely head over heels for it. After using it for some time, figured out that ST2 uses a settings file to remember the plugins, open tabs etc. I was using my laptop (Mac) and sometimes uses my PC (Windows) do type blog post or some other stuff. So thought of giving a try in syncing these two using Dropbox folder.\n\nThe idea is to have the settings file in the Dropbox folder and to have a symlink in the OS to point to. This means whatever changes i did to a ST2 instance will reflect in my other instance too.\n\n**Prerequisites:**\n\n*   Install ST2 in both the machines.\n*   Have a DropBox account and install the software in both the machines.\n\n### **In Mac:**\n\nMove the entire Sublime Text folder into the Dropbox folder.\n\n```bash\nmv '~/Library/Application\\\\ Support/Sublime\\\\ Text\\\\ 2/' '~/Dropbox/Sublime\\\\ Text\\\\ 2'\n```\nNext step is to create a symlink in the original location so that it the folder in the Dropbox will be used to store the settings.\n\n```bash\nln -s '~/Library/Application\\\\ Support/Sublime\\\\ Text\\\\ 2' '~/Dropbox/Sublime\\\\ Text\\\\ 2'\n```\n### **In Windows:**\n\nWindows don't have symlink concept. So we have to settle with [NTFS symbolic link](http://en.wikipedia.org/wiki/NTFS_symbolic_link \"NTFS Junction Point\")\n\n```bash\nmklink /J 'C:/Users/user\\_name/Dropbox/Sublime Text 2' 'C:/User/user\\_name/Applications/Sublime Text 2'\n```\n\nThat's it, now you don't need to worry about the sync between these two instances.\n",
            "url": "https://prasanna.dev/posts/link-your-sublime-text-2-instances-with-dropbox",
            "title": "Link your Sublime Text 2 instances with Dropbox",
            "summary": "Sublime text has a key mapping file where it stores all the shortcut. Here is a way to share your preferences and key maps across two machines using a dropbox account.",
            "date_modified": "2013-01-09T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/scribblings-socket-io",
            "content_html": "\nI was trying my hands on socket.io. On my first glance it looked extremely simple to get going. The app i was working was on node.js, so i had no trouble in including socket.io into my project.\n\nMy app had client and server component. For the server component i could able to do the npm install and got the socket.io working. Whereas for the client component i couldn't able to find the stand alone js available for download. Basically the js comes in along with the npm which means you got to take it out separately if you want to use it. Then i used the js file from their [Github repo](https://github.com/learnboost/socket.io).\n\nBy default Socket.io doesn't perform broadcast\n----------------------------------------------\n\nThis is my first learning. Though it seems to be obvious after taking a good look onto the API, it wasn't very clear for me in the beginning.\n\n_For example:_\n\n**Server**\n\n```js\nvar app = require('express')(),\n    server = require('http').createServer(app),\n    io = require('socket.io').listen(server);\n\nserver.listen(80);\n\nio.sockets.on('connection', function(socket) {\n    socket.on('first\\_msg', function(data) {\n        socket.emit('reply', {\n            hello: 'world'\n        });\n    });\n});\n```\n**Client 1:**\n\n```js\n<script src = \"/socket.io/socket.io.js\" > </script>\n<script>\n\tvar socket = io.connect('http:/ / localhost ');\n\tsocket.emit('first\\_msg ', { my: 'data1 ' });\n</script>\n```\n\n**Client 2:**\n\n```js\n<script src = \"/socket.io/socket.io.js\" > </script>\n<script>\n\tvar socket = io.connect('http:/ / localhost ');\n\tsocket.on('reply ', function (data){\n\t\tconsole.log(\"Client1 had pinged server.\");\n\t}\n</script>\n```\n\nIn this case i was expecting my _Client2_ console.log to execute but that never happened. Reason being whenever _Client1_ emits '_first\\_msg_' it was _Client1_ who was receiving the reply too (obvious i know !!).\n\nSo in these cases socket.io provides an API to broadcast messages.Hence instead of\n```js\nsocket.emit('reply', { hello: 'world' });\n```\nit should have been\n\nsocket.broadcast.emit('reply', { hello: 'world' });\n\nExposed events in socketIO are just defined for socket.on methods\n-----------------------------------------------------------------\n\nI was trying to emit a custom message from my client. I need to perform some actions on its success and failure. Now i need to attach success and error callbacks. For this i found this [Exposed events](https://github.com/LearnBoost/socket.io/wiki/Exposed-events \"Exposed Events\") doc. The funda is that all these exposed events are defined only for socket.on which means while emitting a message i cannot bind any callbacks to it.\n\nFor error callback it is straight forward. We have\n```js\nsocket.on('error', () -> console.log(\"Error Occured\"))\n```\nwhich can be bound on the socket so whenever an error is been thrown on the socket the defined behaviour gets executed.\n\n**Client** emits the custom message and sends JSON data to the socket via socket.emit, also he gets an update function that handles the success callback\n```js\nsocket.emit ('message', {hello: 'world'});\nsocket.on ('messageSuccess', function (data) {\n//do something\n});\n```\n**Server**\\-side Gets a call from the message emit from the client and emits the messageSuccess back to the client\n```js\nsocket.on ('message', function (data) {\n    io.sockets.emit ('messageSuccess', data);\n});\n```\n",
            "url": "https://prasanna.dev/posts/scribblings-socket-io",
            "title": "Scribblings on Socket.io",
            "summary": "When i tried out socket.io for the first time, it was quite an interesting learning of few new paradigms and techniques.",
            "date_modified": "2012-12-03T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/hamcrest-conflict-junit",
            "content_html": "\nUsing jUnit 4.8 i got into an issue when testing the few [lambda](http://code.google.com/p/lambdaj/ \"LambdaJ\") expressions. I was constantly getting this error while testing code involving Hamcrest matchers.\n\njava.lang.NoSuchMethodError: org.hamcrest.core.AllOf.allOf(Lorg/hamcrest/Matcher;Lorg/hamcrest/Matcher;)Lorg/hamcrest/Matcher;\n\nHowever when i ran the application it was all fine. So i could sense the issue with the jUnit.\n\nThe problem is due to hamcrest versioning issue with jUnit. jUnit uses an old version while other libraries (in my case LambdaJ) was using the latest version. The fix will be to download the junit-dep-4.\\*.jar from the jUnit [download](https://github.com/KentBeck/junit/downloads \"gitHub kent beck\") page. Since you app already have Hamcrest class the test will run smoothly.\n",
            "url": "https://prasanna.dev/posts/hamcrest-conflict-junit",
            "title": "Hamcrest conflict in jUnit.",
            "summary": "Hamcrest matchers are used in jUnit for assertions. There is a weird problem with the version conflict between Hamcrest and jUnit. The solution is been discussed here.",
            "date_modified": "2012-06-26T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/handle-maxuploadsizeexceededexception-spring",
            "content_html": "\nI was doing a AJAX file upload using jQuery and Spring 3. Spring provides a way to limit the file being uploaded and this can be configured while creating the multipart bean by specifying the maxUploadSize parameter.\n\nSo whenever an user tries to upload a file with size size greater than that of the specified one then Spring will throw _'MaxUploadSizeExceededException'_ exception and returns back. The problem for me is that i was doing the file upload using AJAX so i wanted a custom error to be thrown rather than the Spring's default error.\n\nAnd also because of this exception the control will not even reach your specified controller so there is no chance to catch it in your Controller. After some lookup i found this simple fix for it.\n\nFileUploadController: Controller which will handle the file upload request.\n\n_**Make this FileUploadController to implement HandlerExceptionResolver. This will force you to define resolveException() method.**_\n\n```java\n@ResponseBody\npublic ModelAndView resolveException(HttpServletRequest httpServletRequest,\n        HttpServletResponse httpServletResponse, Object o, Exception e) {\n    if (e instanceof MaxUploadSizeExceededException) {\n        ModelAndView modelAndView = new ModelAndView(\"inline-error\");\n        modelAndView.addObject(\"error\", \n        \"Error: Your file size is too large to upload. Please upload a file of size < 5 MB and  continue. \");\n    return modelAndView;\n    }\n    e.printStackTrace();\n    return new ModelAndView(\"500\");\n}\n```\n** How to show the error on the same page:**\n\nThe call to the controller is from a jQuery ajax method. But the problem here is that even with this approach your jQuery POST method is going to receive a HTTP\\_OK message from the controller. Hence if you are waiting at the error callback then you have no chance of catching this error.\n\nSo what i have done here is to return inline-error view back as the response. On the success callback of the jQuery i check for the presence of the error\\_div in the response and display the field in the page. Else show the success message.\n\n_inline-error.jsp_\n```html\n<div class=\"error\" id=\"error\\_div\">${error}</div>\n```\n\n_PS: This is definitely not the cleanest approach, but this solved my problem :)_\n",
            "url": "https://prasanna.dev/posts/handle-maxuploadsizeexceededexception-spring",
            "title": "Handle MaxUpload SizeExceededException in Spring",
            "summary": "Handling MaxUploadExceedException in Ajax call with Spring controllers. This exception occurs when the file size greate than what is expected is been uploaded by the user.",
            "date_modified": "2012-06-23T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/make-ntfs-write-mac-lion",
            "content_html": "\nJust now i got my new Mac book pro, pre loaded with Lion OSX and one of the surprises i stepped onto is NTFS write issue on the Mac.\n\nBased on a few blog posts and comments I managed to find a way that worked for me, so I thought I’d put it all here in one place for others.\n\n**Pre requisites:**\n\nGet [HomeBrew](http://mxcl.github.com/homebrew/ \"homebrew\") installed in your machine. And of course this needs XCode tools to be installed.\n\n**Steps:**\n\n1) Install latest Fuse4X (a fork of MacFUSE) and NTFS-3G packages:\n```bash\nbrew install fuse4x\nbrew install ntfs-3g\n```\n2) Type brew info fuse4x-kext in the terminal. You will be shown a message similar to this:\n\nIn order for FUSE-based filesystems to work, the fuse4x kernel extension\nmust be installed by the root user:\n```bash\nsudo cp -rfX /usr/local/Cellar/fuse4x-kext/0.8.13/Library/Extensions/fuse4x.kext /System/Library/Extensions\nsudo chmod +s /System/Library/Extensions/fuse4x.kext/Support/load\\_fuse4x\n```\nPerform both the operation.\n3) And after this i simply followed this [blog post entry](http://fernandoff.posterous.com/ntfs-write-support-on-osx-lion-with-ntfs-3g-f). Since you have already installed Fuse4x and ntfs-3g you can directly jump to\n\n> \"Ok, at this point you should have a functional fuse4x and ntfs-3g install.\"\n\nand create an alternative\n```bash\n/sbin/ntfs\\_mount\n```\nscript as described there.  \nAnd at last you got make one change to get things working.  \nThe script in the bog post is for MacPort users. For HomveBrew users you got to make this change.  \nreplace\n```bash\n/opt/local/bin/ntfs-3g\n```\nwith\n\n```bash\n/usr/local/bin/ntfs-3g\n```\nAnd that's it. Just try mounting a NTFS drive and you should have write permissions to your drive. If you face any issues check out the log @\n```bash\n/var/log/ntfsmnt.log\n```\nor try re-booting the machine in the worst case.\n",
            "url": "https://prasanna.dev/posts/make-ntfs-write-mac-lion",
            "title": "Make your NTFS drive writable under Mac Lion",
            "summary": "Make your NTFS drive writabe under mac lion.",
            "date_modified": "2011-12-26T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/slot-machine-effect-jquery",
            "content_html": "\n**Slot Machine Effect:**\n\n**Requisite:**\n\n*   jQuery 1.5+\n\n**Idea:**\n\n*   Create an element to display the animation.\n*   Next create an empty element say <div></div> and set its position: 'fixed'\n*   Set the position of the empty element to the Start Value of the slot.\n*   Now use jQuery animate to move the empty element from Start value to the specified End Value in a given duration.\n*   jQuery animate has a step() method which gives you the current position of the div for every unit of time.\n*   Now inside this step() method set the display element's text to the current position value of the empty element.\n*   Since empty element moves from start value to end value, you will see the numbers changing from start value to end value in the display area.\n\n**Javascript Code:**\n\n```js\n$('#animate\\_btn').click(function() {\n    cashFlow($('.value'), $('#startVal').val(), $('#endVal').val(),\n        $('#duration').val() \\* 1000, $('#decimal').val());\n});\n\ncashFlow = function(elem, from, to, duration, decimal) {\n    var magicObject;\n    if (typeof magicObject === 'undefined') {\n        magicObject = $('<div></div>').appendTo('body');\n    }\n    magicObject.css({\n        position: \"fixed\",\n        left: from\n    }).animate({\n        left: to\n    }, {\n        duration: duration,\n        step: function(currentLeft) {\n            elem.html(Number(currentLeft).toFixed(decimal));\n        },\n        complete: function() {\n            magicObject.remove();\n        }\n    });\n};\n\n```\n",
            "url": "https://prasanna.dev/posts/slot-machine-effect-jquery",
            "title": "Slot machine effect using jQuery",
            "summary": "A cool widget that looks like a slot machine. Small piece of code and a nice trick to animate the numbers to achieve a slot machine effect.",
            "date_modified": "2011-09-24T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/writing-custom-tags-for-jstls",
            "content_html": "\nFirst start with writing a tag library descriptor(TLD). A TLD is a XML document that contains information about a library as a whole and about each tag contained in the library.  \nThe structure of the TLD file is pretty readalbe.\n\nBelow is an implementation of tag which takes in a section name(value) of a web page and checks whether the logged-in user has rights to view the section.\n\n**Step 1:** custom.tld\n```xml\n<?xml version=\"1.0\" encoding=\"ISO-8859-1\" ?>\n<!DOCTYPE taglib PUBLIC \"-//Sun Microsystems, Inc.//DTD JSP Tag Library 1.1//EN\"\n        \"http://java.sun.com/j2ee/dtds/web-jsptaglibrary\\_1\\_1.dtd\">\n<taglib xmlns=\"http://java.sun.com/j2ee/dtds/web-jsptaglibrary\\_1\\_1.dtd\">\n    <tlibversion>1.0</tlibversion>\n    <jspversion>1.1</jspversion>\n    <shortname>custom</shortname>\n    <info>Custom tag library</info>\n    <tag>\n        <name>permission</name>\n        <tagclass>com.prasans.PermissionTag</tagclass>\n        <info>\n            Checks the User Permission to access the content.\n        </info>\n        <attribute>\n            <name>value</name>\n            <required>true</required>\n        </attribute>\n        <attribute>\n            <name>invertCondition</name>\n            <required>false</required>\n        </attribute>\n    </tag>\n</taglib>\n```\nHere we have implemented a tag called permission within the 'custom' tag library.\n\nUsage: _<custom:permission value=\"\">{section}</custom:permission>_  \nSimilarly you can add more tags to your library by adding more <tag> nodes.\n\nAfter done with defining TLD, next step is to implement the conditional logic. Below is a piece of Java code that does the implementation of the TLD.\n\n**Step 2:** PermissionTag.java\n\n```java\npackage com.prasans;\n\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.jsp.jstl.core.ConditionalTagSupport;\n\npublic class PermissionTag extends ConditionalTagSupport {\nprivate String value = null;\nprivate boolean invertCondition;\n\n    public void setValue(String value) {\n        this.value = value;\n    }\n\n    public String getValue() {\n        return value;\n    }\n\n    public boolean isInvertCondition() {\n        return invertCondition;\n    }\n\n    public void setInvertCondition(boolean invertCondition) {\n        this.invertCondition = invertCondition;\n    }\n\n    @Override\n    protected boolean condition() {\n        // If needed you can access Request Object like this.\n        HttpServletRequest request = (HttpServletRequest) pageContext.getRequest();\n        boolean permission = checkForThePermission(value);\n        return invertCondition ? !permission : permission;\n    }\n}\n```\n**Explanation:**\n\n\\* Since the expectation of this tag is to return true or false, we are extending the _ConditionalTagSupport_ class. Based on the need you can choose upon your class implementation.\n\n\\*Note that all tag attributes are member variables of the class and all of them should have getters and setters.\n\n\\*Here we have overridden the condition() from ConditionalTagSupport to return the needed boolean result.\n\n\\* InvertCondition is an attribute that helps us in simulating negative scenarios.\n\nFor ex: \"Show the section _If User A do not have 'X' permission_\"\n\nAfter building the TLD and its corresponding logic, the next step is to integrate with your application.  \nAdd the custome tag library to your web.xml to integrate with your web app.\n\n**Step 3:** web.xml\n\n```xml\n<jsp-config>\n    <taglib>\n        <taglib-uri>/custom</taglib-uri>\n        <taglib-location>/WEB-INF/tags/custom.tld</taglib-location>\n    </taglib>\n</jsp-config>\n```\nThe taglib-uri is the _<shortname>_ defined in the TLD file. And _<taglib-location>_ is the location of the tld. Make sure that you are bundling the TLD along with your WAR.\n\nThats it. You can start using your custom tags in your JSPs now.\n",
            "url": "https://prasanna.dev/posts/writing-custom-tags-for-jstls",
            "title": "Writing Custom Tags for JSTLs",
            "summary": "Writing a custom JSTL tag and integrating with the application. A sample code to do the same.",
            "date_modified": "2011-09-10T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/reloading-an-activity-in-android",
            "content_html": "\nMore often i wanted to reload an activity to refresh the contents of an page. I have seen many ways to do this in Android world and i always puzzled about the best approach.\n\nHowever these are some of the approaches that i took . I found the following two approaches a lot cleaner as they kill the existing intent and restart.\n\n**Approach 1:**\n\n```java\nIntent intent = getIntent();\nfinish();\nstartActivity(intent);\n```\n\n**Approach 2:**\n```java\nIntent intent = getIntent();\noverridePendingTransition(0, 0);\nintent.addFlags(Intent.FLAG\\_ACTIVITY\\_NO\\_ANIMATION);\nfinish();\noverridePendingTransition(0, 0);\nstartActivity(intent);\n```\n_**Note:** The second approach works only from API 5+_\n",
            "url": "https://prasanna.dev/posts/reloading-an-activity-in-android",
            "title": "Reloading an activity in Android",
            "summary": "Refreshing or reloading activity in Android application. This might be important in the case of refreshing data in the activity.",
            "date_modified": "2011-07-12T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/configuring-context-name-for-an-applicationtomcat-war",
            "content_html": "\nMany times i have faced this necessity of  having a different Context name than that of the WAR name. Especially when i use Maven. And i also found that it is quite simple in Tomcat. While looking for it i found two approached for doing it. One using Context.xml and the other using Server.xml\n\n**Problem Statement:** I have a WAR file myapp-build123.war and the app can be referred using _http://localhost:8080/myapp-build123_ but i need to refer my application as _http://localhost:8080/myapp_ So here is what im gonna do.\n\n**Approach 1: _Context.xml_ (Recommended way)**\n\n*   Create a Context file in ${TOMCAT\\_HOME}/conf/Catalina/localhost directory. Name the file as myapp.xml (The file name should be the same as desired context name) The content of the file is given below.\n\n```bash\n<Context path=\"/somepath\" docBase=\"/home/myapp-build123\"/>\n```\n\n_Basically the path attribute is been ignored by Tomcat so if you want you can ignore it too._\n\n_The docBase will contain the path of the WAR. Here i have placed my WAR file in the home directory._\n\n*   An important thing to note here is that the WAR file cannot be placed inside the ${TOMCAT\\_HOME}/webapps folder. If you place the WAR in the webapps folder then the war will get exploded with the same name as the WAR file and after that it is not possible to configure the Context name. So place the WAR anywhere in the system apart from ${TOMCAT\\_HOME}/webapps folder.\n\n\n*   Now its time to start the server and you will see a folder named myapp in the ${TOMCAT\\_HOME}/webapps folder, containing the application files.\n\n\n**Approach 2: Server.xml ( [Not Recommended](http://tomcat.apache.org/tomcat-6.0-doc/config/context.html) after Tomcat 4.x )**\n\n*   Open the Server.xml file from ${TOMCAT\\_HOME}/conf folder. Search for the Host tag and place the Context tag inside it.\n\n```xml\n<Host name=\"localhost\"  appBase=\"webapps\"\n    unpackWARs=\"true\" autoDeploy=\"true\"\n    xmlValidation=\"false\" xmlNamespaceAware=\"false\">\n    <Context path=\"/myapp\" docBase=\"/myapp-build123\"/>\n</Host>\n```\n*   In this scenario you can place the WAR file in the ${TOMCAT\\_HOME}/webapps folder itself. And also it is possible to access the application by both the URLs, http://localhost:8080/myapp-build123 and http://localhost:8080/myapp.\n",
            "url": "https://prasanna.dev/posts/configuring-context-name-for-an-applicationtomcat-war",
            "title": "Configuring Context name for an application",
            "summary": "Setting up a different context name than the WAR name for the Java application deployed in Tomcat server.",
            "date_modified": "2011-07-08T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/primary-key-onetoone-mapping-in-hibernate",
            "content_html": "\nToday was trying to join two tables using its Primary keys using Hibernate. Here is what I tried to do:\n\n```java\n@Table(name = \"customer\")\n@Entity\npublic class Customer {\n    @Id\n    @OneToOne\n    @JoinColumn(name = \"customer\\_id\", updatable = false)\n    private Credentials credentials;\n}\n```\n\nI was constantly getting an error stating invalid column name. Later then I realized that its not possible have Join in the Primary-Key and bind to a custom object. This forced me to have a Auto-generated Id as a key to the table and named it as the primary key. This is how my code looked after modification.\n\n```java\n@Table(name = \"customer\")\n@Entity\npublic class Customer {\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private long id;\n\n    @OneToOne\n    @JoinColumn(name = \"customer\\_id\",updatable = false)\n    private Credentials credentials;\n}\n```\n",
            "url": "https://prasanna.dev/posts/primary-key-onetoone-mapping-in-hibernate",
            "title": "Primary-key @OneToOne mapping in Hibernate",
            "summary": "Joining two tables using Primary keys in Hibernate.",
            "date_modified": "2011-05-17T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/play-age-of-empires-aoe-in-windows-7",
            "content_html": "\nDue to an issue in the Windows 7 Aero theme , the map of the Age Of Empires (AOE) actually renders poorly. The playing area will be laid in patches and the small map in the bottom right won't show the correct player's color.\n\nThe fix for this issue is pretty simple.\n\n* Start the AOE game.\n\n* Get back to your desktop without closing the game,(by pressing the Windows Key (or) Show Desktop key).\n\n* Start your task manager . Select the Processes tab. Look for process explorer.exe and kill it.\n\n* This will hide your desktop icons, taskbar and all other opened windows.\n\n* Using Alt +Tab get back to the Game. Now the map will be rendered nicely. Enjoy Playing :)\n\n* After finishing the game , Start Task Manager again. Choose File -> New Task and type explorer.exe in the dialog box.\n\n*Everything is back to normal.\n",
            "url": "https://prasanna.dev/posts/play-age-of-empires-aoe-in-windows-7",
            "title": "Age Of Empires (AOE) Fix in Windows 7",
            "summary": "Age of Empires (a popular strategy game) has got some problems with color rendering while played in Window 7. A quick harmless solution for that is here.",
            "date_modified": "2011-04-27T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/add-edittexts-dynamically-and-retrieve-values-android",
            "content_html": "\nAdding EditText to your Android application is no different from adding any other form elements except for one thing. Retrieving values from them is slightly different and of course nothing impossible. Just a little more bit of coding and thats it.\n\n![Android-EditText {307xx578}](/assets/posts/images/android-edittext.png \"Android-EditText\")\n\nFollowing code snippet creates a series of EditTexts and also let you to access its values.\n\n```java\nimport android.app.Activity;\nimport android.os.Bundle;\nimport android.view.View;\nimport android.view.ViewGroup;\nimport android.widget.Button;\nimport android.widget.EditText;\nimport android.widget.LinearLayout;\nimport android.widget.TableLayout;\nimport android.widget.TableRow;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport static android.view.ViewGroup.LayoutParams.FILL\\_PARENT;\nimport static android.view.ViewGroup.LayoutParams.WRAP\\_CONTENT;\nimport static android.widget.LinearLayout.VERTICAL;\n\npublic class Sample extends Activity {\n    private List<EditText> editTextList = new ArrayList<EditText>();\n\n    @Override\n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n\n        LinearLayout linearLayout = new LinearLayout(this);\n        ViewGroup.LayoutParams params = new ViewGroup.LayoutParams(FILL\\_PARENT, WRAP\\_CONTENT);\n        linearLayout.setLayoutParams(params);\n        linearLayout.setOrientation(VERTICAL);\n\n        int count = 10;\n        linearLayout.addView(tableLayout(count));\n        linearLayout.addView(submitButton());\n        setContentView(linearLayout);\n    }\n\n    private Button submitButton() {\n        Button button = new Button(this);\n        button.setHeight(WRAP\\_CONTENT);\n        button.setText(\"Submit\");\n        button.setOnClickListener(submitListener);\n        return button;\n    }\n\n    // Access the value of the EditText\n\n    private View.OnClickListener submitListener = new View.OnClickListener() {\n        public void onClick(View view) {\n            StringBuilder stringBuilder = new StringBuilder();\n            for (EditText editText : editTextList) {\n                stringBuilder.append(editText.getText().toString());\n            }\n        }\n    };\n\n    // Using a TableLayout as it provides you with a neat ordering structure\n\n    private TableLayout tableLayout(int count) {\n        TableLayout tableLayout = new TableLayout(this);\n        tableLayout.setStretchAllColumns(true);\n        int noOfRows = count / 5;\n        for (int i = 0; i < noOfRows; i++) {\n            int rowId = 5 \\* i;\n            tableLayout.addView(createOneFullRow(rowId));\n        }\n        int individualCells = count % 5;\n        tableLayout.addView(createLeftOverCells(individualCells, count));\n        return tableLayout;\n    }\n\n    private TableRow createLeftOverCells(int individualCells, int count) {\n        TableRow tableRow = new TableRow(this);\n        tableRow.setPadding(0, 10, 0, 0);\n        int rowId = count - individualCells;\n        for (int i = 1; i <= individualCells; i++) {\n            tableRow.addView(editText(String.valueOf(rowId + i)));\n        }\n        return tableRow;\n    }\n\n    private TableRow createOneFullRow(int rowId) {\n        TableRow tableRow = new TableRow(this);\n        tableRow.setPadding(0, 10, 0, 0);\n        for (int i = 1; i <= 5; i++) {\n            tableRow.addView(editText(String.valueOf(rowId + i)));\n        }\n        return tableRow;\n    }\n\n    private EditText editText(String hint) {\n        EditText editText = new EditText(this);\n        editText.setId(Integer.valueOf(hint));\n        editText.setHint(hint);\n        editTextList.add(editText);\n        return editText;\n    }\n}\n```\n",
            "url": "https://prasanna.dev/posts/add-edittexts-dynamically-and-retrieve-values-android",
            "title": "Add EditText(s) dynamically and retrieve values - Android",
            "summary": "Adding multiple edit text boxes to the android application dynamically through code and controlling them.",
            "date_modified": "2011-03-21T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/test-your-controllers-modelattribute-methods",
            "content_html": "\nI was about to write some unit tests around my Spring’s controller classes and also i wanted to write the test using MockHttpRequest and MockHttpResponse.\n\nMy controller had a method to which i was using ModelAttribute as one of the parameter. I just want to simulate the same scenario in my Unit Tests.\n\nUnfortunately i could not see any methods in MockHttpRequest to help me with this. So i had to take a simple different approach as an workaround for this.\n\nMy Controller code looks similar to this:\n\n```java\n@Controller\n@RequestMapping(value = \"/register\")\npublic class MyController {\n    @RequestMapping(value = \"/save\", method = RequestMethod.POST)\n    public ModelAndView save(@ModelAttribute User user) {\n        //Code to save the User object\n        return new ModelAndView();\n    }\n}\n```\nMy Unit Tests:\n\n```java\npublic class MyControllerTest {\nMockHttpServletResponse response;\nMockHttpServletRequest request;\nAnnotationMethodHandlerAdapter handler;\n\n    @Before\n    public final void init() {\n        response = new MockHttpServletResponse();\n        request = new MockHttpServletRequest();\n        handler = new AnnotationMethodHandlerAdapter();\n    }\n\n    @Test\n    public void shouldTestSaveUser() {\n        final User mockUser = new UserTestBuilder().withName(\"John\").build();\n        request.setMethod(\"POST\");\n        request.setRequestURI(\"/register/save\");\n\n        MyController myController = new MyController() {\n            @ModelAttribute\n            public User mockModel() {\n                return user;\n            }\n        }\n        ModelAndView model = handler.handle(request, response, myController);\n    }\n}\n```\n_**Explanation:**_\n\nWhenever a method in a controller is annotated with @ModelAttribute , it will be invoked for every request made to that controller. So while creating the mycontroller object i am overriding a sample method which has this annotation and returns a User object as a ModelAttribute.\n",
            "url": "https://prasanna.dev/posts/test-your-controllers-modelattribute-methods",
            "title": "Test your Controller's ModelAttribute methods.",
            "summary": "Injecting a ModelAttribute to the controller's method in Unit tests using Spring and jUnit.",
            "date_modified": "2011-01-19T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/hadoop-version-in-aws-map-reduce",
            "content_html": "Creating job flows using AWS MapReduce's GUI is pretty simple and very straight forward. But i wanted to use Java SDK to create/run jobs in MapReduce. I could successfully able set up the job and configured all the parameters except for a weird error.\n```java\njava.lang.NoSuchMethodError:\norg.apache.hadoop.mapred.JobConf.\nsetBooleanIfUnset(Ljava/lang/String;Z)\n```\nI was constantly getting this error while running the job. Initially i had no idea why this error occurs and none of the forum talks about it either. Then i figured out that the default Hadoop version used by the Ec2 instances was 0.18 and i was expecting 0.20. Interestingly i didn't face this issue when i did it through GUI.\n\nAs a solution i need to explicitly set the version number as 0.20 to the Instances object so that it will use the same while running the job.\n```java\nJobFlowInstancesConfig instances = new JobFlowInstancesConfig();\ninstances.setHadoopVersion(\"0.20\");\n```\n",
            "url": "https://prasanna.dev/posts/hadoop-version-in-aws-map-reduce",
            "title": "Hadoop Version in AWS Map Reduce",
            "summary": "Performing Map Reduce operation using Amazon AWS interface.",
            "date_modified": "2010-11-14T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/sax-parser-characters-method",
            "content_html": "Was playing around SAX parsing some Gigs of XML file 😅 Here are few learnings from the game.\n\nMy intention was to read values between a corresponding tag. I initially went after using characters() in SAX parser which actually worked fine for initial feeds. But as i keep increasing the size of the XMLs and if the size of the tagContent was large the problem arises. characters() function not always gives back the entire value in a single shot. It may return the value in multiple chunks. So need to be careful in assigning and using the values from characters() method.\n\nSo the better way to use characters() method is to keep appending all the values to a buffer and use the value in the corresponding end tag section. Also need to make sure that the buffer has to be flushed in the corresponding start element.\n\n**Sample Xml:**\n```xml\n<Sample>\n    <StudentCollection>\n        <Student>\n            <Name>Jack</Name>\n            <Age>12</Age>\n        </Student>\n        <Student>\n            <Name>Jill</Name>\n            <Age>13</Age>\n        </Student>\n        <Student>\n            <Name>Rose</Name>\n            <Age>14</Age>\n        </Student>\n    </StudentCollection>\n</Sample>\n```\n**Sample SAX handler code to print the Names:**\n\n**Initial Code: (Works fine for small values & small files)**\n```java\n\npublic void startElement(String uri, String tag, String qName, Attributes attributes) {\n}\n\npublic void characters(char ch[], int start, int length) {\n  System.out.println(\"Name of a student: \" + new String(ch, start, length));\n}\n\npublic void endElements(String uri, String tag, String qName) {\n}\n```\n**Final Code:**\n\n```java\n\npublic void startElement(String uri, String tag, String qName, Attributes attributes) {\n  if (\"Name\".equals(tag)) {\n    tagContentBuffer = new StringBuffer();\n  }\n}\n\npublic void characters(char ch[], int start, int length) {\n  tagContentBuffer.append(new String(ch, start, length));\n}\n\npublic void endElements(String uri, String tag, String qName) {\n  if (\"Name\".equals(tag)) {\n    System.out.println(\"Name of a student: \" +\n       tagContentBuffer.toString());\n  }\n}\n```\n",
            "url": "https://prasanna.dev/posts/sax-parser-characters-method",
            "title": "SAX parser characters() method.",
            "summary": "Implementing `characters()` method for SAX parsing huge files. With a sample code in Java",
            "date_modified": "2010-10-06T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/ftp-client-ubuntu",
            "content_html": "Today was about to download multiple files from an authenticated FTP server. The native ftp client of Ubuntu didn't help me as expected. I was trying to log into the FTP server and was constantly getting disconnected when trying to perform some operation. When browsed for some alternative found this ncftp client. This actually worked instantly and was pretty easy in installing.\n\n**_Install NCFTP:_**  \n`sudo apt-get install ncftp`\n\n**_Log into FTP server:_ **  \n`ncftp -u username hostname -p`\n\nit will prompt for password enter it. Type '?' in the terminal for the list of commands and there you go :)\n\nCheck out the NCFTP client for other platforms.\n",
            "url": "https://prasanna.dev/posts/ftp-client-ubuntu",
            "title": "FTP Client (Ubuntu)",
            "summary": "Download multiple files from authenticated FTP server in Ubuntu.",
            "date_modified": "2010-08-30T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/stop-halting-at-assertions",
            "content_html": "When I was writing functional tests i stepped into a scenario where i have to continue with the test even after an assertion failure. The idea here is that you find an assertion failing, still you may need to go ahead and check out all the assertions as these tests generally take long time to complete. At the end of the test it will be good to have the summary of all the failures along with the stack trace. I was looking for some testing framework to help out with this functionality but unfortunately cant find any.\n\nFinally using TestNg I implemented this feature. All you need is to write a listener that listens whenever a test fails and simply logs the stack trace without failing the test. There is a neat step by step tutorial given [here ](http://seleniumexamples.com/blog/guide/using-soft-assertions-in-testng)\n\nThough this example has some specific information for Selenium based tests, it works fine with normal functional tests too.\n\nHappy Testing :)\n",
            "url": "https://prasanna.dev/posts/stop-halting-at-assertions",
            "title": "Stop halting at Assertions",
            "summary": "Generally assertions cause the test to halt. But sometimes we need to continue further and evaluate all the asserts and expect a comprehensive report of all the asserts.",
            "date_modified": "2010-03-04T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/printing-multiple-divs-in-a-page",
            "content_html": "I stumbled upon a situation where i have to print multiple sections of a page into a single document. I actually tried out my own javascript to render only the required divs into an iframe and then the idea is to print that iframe calling _window.print()_ in that iframe.\n\nHaving limited Javascript knowledge the above mentioned task was a bit tedious to me , then i sat upon googling to get a ready made plugin that implements this feature and my search ended upon [Jqprint](http://plugins.jquery.com/project/jqPrint). Jqprint is a plugin written in Jquery and it does the same that i mentioned above. Using this jqprint was really very easy. what all you need to do is that just mark all the divs that you want to print with a specific class name or with id. If your classname is printdiv then all you need to do is just call  _$(\".printdiv\").jqprint();_\n",
            "url": "https://prasanna.dev/posts/printing-multiple-divs-in-a-page",
            "title": "Printing multiple divs in a page.",
            "summary": "When an user tries to print a HTML page, allow multiple divs from the page to appear in the print and not the entire page. Not using media CSS query",
            "date_modified": "2010-02-10T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/install-inconsolata-ttf-in-ubuntujaunty",
            "content_html": "As developers tend to spend most of the time in front of IDEs it makes sense to pick up the best suited font for development. And i have seen most of the developers prefer to use monochrome fonts as it yields better feel while looking at the code. It is been now widely accepted by many developers to use Inconsolata as their development font. So better start using it and prove yourself geeky ;)  \nWhen i tried to use Inconsolata with my IntellijIDEA i couldn't find the ttf type inconsolata. And Intellij supports only ttf types. After a long search i downloaded thr ODf type and converted it to ttf using a converter and then i had the issue of installing it to my Jaunty. And i took help of my dev friends out here to resolve stuffs. So thought of consolidating the steps together as it may reduce someone else's pain.\n\n**Steps to install Inconsolata.ttf in Ubuntu(Jaunty).**\n\n**Step 1:** Start with downloading inconsolata font. [Inconsolata.ttf](http://www.4shared.com/file/xnMYNL0w/Inconsolata.html)\n\n**Step 2:**\n\nmkdir /usr/share/fonts/truetype\n\ncd /usr/share/fonts/truetype\n\nsudo mkdir  ttf-inconsolata\n\n**Step 3:** Copy the Inconsolata.ttf into the directory.\n\nsudo cp ~/Desktop/Inconsolata.ttf ttf-inconsolata\n\n**Step 4:** Now modify the permissions to allow it to be accessed by IDE's\n\ncd ttf-inconsolata\n\nsudo chmod 777 Inconsolata.ttf\n\n**Step 5:**Its not over yet. Finally before going to the IDE you need to cache the font to make it accessible.\n\nsudo fc-cache -f -v\n\nThis will show a list of fonts cached recently. Check whether Inconsolata.ttf is been cached.\n\nNow you can keep staring at your code for a long time as it feels a lot better :)\n",
            "url": "https://prasanna.dev/posts/install-inconsolata-ttf-in-ubuntujaunty",
            "title": "Install Inconsolata.ttf in Ubuntu(Jaunty).",
            "summary": "Inconsolata is dev friendly font used by devs for their code. This post is about installing Inconsolata tru type in Ubuntu - Jaunty.",
            "date_modified": "2009-12-10T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/test-smtp-server",
            "content_html": "I came across testing a feature that ends up sending a mail. And to my misfortune i do not have access to the smtp server from my test environment. So i was looking out for a mock smtp server kind of stuff and ended up happily with test smtp server.\n\n[Devnull SMTP server](http://www.aboutmyip.com/AboutMyXApp/DevNullSmtp.jsp) is a dummy SMTP server that can be used for testing purposes. It helps you see all communication between a client and the server and is very useful if you are trying to find problems with your email server or a client that you wrote. And the best part is it is very simple to use and it is free of cost too. So if you have any such necessity do try this out.\n",
            "url": "https://prasanna.dev/posts/test-smtp-server",
            "title": "Test SMTP server",
            "summary": "How can you set up an SMTP server in your local environment for testing purpose.",
            "date_modified": "2009-10-20T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/first-post",
            "content_html": "So after writing lots of shits and craps in various blogs now I have decided to have my own blog (another one :P) to continue writing those same shits and craps.\n\n**Disclaimer:** All the writings made in this blog are my own thoughts, opinions, views, suggestions, impressions, feelings, beliefs, faiths, sentiments, notions, thinking and ideas. In short this blog is written by me considering myself as its only reader. If you accidently landed on any of the posts here I’m not responsible for its consequences.\n",
            "url": "https://prasanna.dev/posts/first-post",
            "title": "First Post",
            "summary": "An introductory post to start off my blog.",
            "date_modified": "2009-08-28T18:30:00.000Z"
        }
    ]
}