{
    "version": "https://jsonfeed.org/version/1",
    "title": "Random Presence",
    "home_page_url": "https://prasanna.dev",
    "feed_url": "https://prasanna.dev/feeds/feed.json",
    "description": "Random presence of my thoughts and learning...",
    "icon": "https://prasanna.dev/assets/logo.png",
    "author": {
        "name": "Prasanna Venkatesan",
        "url": "https://prasanna.dev"
    },
    "items": [
        {
            "id": "https://prasanna.dev/posts/a2a-vs-http",
            "content_html": "<h1 id=\"understandinggooglesa2aprotocolbeyondtraditionalhttp\">Understanding Google's A2A Protocol: Beyond Traditional HTTP</h1>\n<p>Google's <a href=\"https://google.github.io/A2A/\">Agent-to-Agent (A2A) protocol</a> represents a significant evolution in how AI systems communicate with each other and with client applications. As AI becomes more integrated into our digital interactions, the need for standardized, efficient communication protocols becomes critical. Let's explore how A2A differs from traditional HTTP and HTTP Server-Sent Events (SSE), and why these differences matter.</p>\n<h2 id=\"thelimitationsoftraditionalhttp\">The Limitations of Traditional HTTP</h2>\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview\">Traditional HTTP</a> follows a simple request-response pattern:</p>\n<ul>\n<li>Client sends a request</li>\n<li>Server processes the request</li>\n<li>Server returns a response</li>\n<li>Connection closes</li>\n</ul>\n<p>For AI agents that may need seconds or minutes to process complex requests, this creates significant inefficiencies as clients must repeatedly poll for updates.</p>\n<h2 id=\"serversenteventsastepforward\">Server-Sent Events: A Step Forward</h2>\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events\">HTTP Server-Sent Events (SSE)</a> improved on traditional HTTP by enabling:</p>\n<ul>\n<li>One-way server-to-client streaming</li>\n<li>Long-lived connections</li>\n<li>Real-time updates without polling</li>\n</ul>\n<p>Basic SSE works by:</p>\n<ul>\n<li>Client initiating a connection with <code>Accept: text/event-stream</code> header</li>\n<li>Server keeping the connection open</li>\n<li>Server pushing updates as they become available</li>\n</ul>\n<p>While this addresses some issues with traditional HTTP, it lacks structured support for complex AI interactions.</p>\n<h2 id=\"a2aprotocolpurposebuiltforaicommunication\">A2A Protocol: Purpose-Built for AI Communication</h2>\n<p>The A2A protocol builds upon SSE but adds sophisticated layers designed specifically for AI agent communication:</p>\n<h3 id=\"1taskbasedcommunicationmodel\">1. Task-Based Communication Model</h3>\n<p>Unlike basic HTTP or SSE, A2A is built around the concept of tasks:</p>\n<ul>\n<li>Each task has a unique identity</li>\n<li>Tasks have defined lifecycles (SUBMITTED → WORKING → COMPLETED/INPUT_REQUIRED)</li>\n<li>Tasks maintain history and context</li>\n<li>Tasks can produce multiple artifacts and status updates</li>\n</ul>\n<p>This task-oriented approach matches how AI systems actually work—processing may take time, require intermediate steps, and produce multiple types of outputs.</p>\n<h3 id=\"2richeventtypes\">2. Rich Event Types</h3>\n<p>While basic SSE can only stream simple text events, A2A defines structured event types:</p>\n<ul>\n<li><code>TaskStatusUpdateEvent</code> for state changes</li>\n<li><code>TaskArtifactUpdateEvent</code> for outputs</li>\n<li>Error events with standardized formats</li>\n</ul>\n<p>Each event carries meaningful context about what happened and why.</p>\n<h3 id=\"3conversationalsupport\">3. Conversational Support</h3>\n<p>A2A excels at handling conversational flows by:</p>\n<ul>\n<li>Maintaining session context across interactions</li>\n<li>Supporting explicit INPUT_REQUIRED states</li>\n<li>Preserving conversation history</li>\n<li>Enabling mid-conversation resubscription</li>\n</ul>\n<h3 id=\"4connectionlongevity\">4. Connection Longevity</h3>\n<p>A key differentiator of A2A is its connection management:</p>\n<ul>\n<li>Connections persist for the entire lifecycle of a task</li>\n<li>Multiple turns of conversation use the same connection</li>\n<li>Connections only terminate when:</li>\n<li>The task reaches a final state (COMPLETED)</li>\n<li>The task requires user input (INPUT_REQUIRED)</li>\n<li>An error occurs</li>\n<li>The client disconnects</li>\n</ul>\n<p>This means a single connection can support an entire multi-turn conversation with an AI agent, not just a single request-response exchange.</p>\n<h3 id=\"5pushnotificationintegration\">5. Push Notification Integration</h3>\n<p>Beyond streaming, A2A incorporates webhook-style push notifications:</p>\n<ul>\n<li>Clients can register notification endpoints</li>\n<li>Servers push updates to these endpoints when task states change</li>\n<li>Enables background processing without maintaining open connections</li>\n</ul>\n<h2 id=\"implementationconsiderations\">Implementation Considerations</h2>\n<p>When building systems with A2A:</p>\n<ul>\n<li><strong>Connection Management</strong>: Plan for longer-lived connections than in traditional HTTP systems</li>\n<li><strong>State Handling</strong>: Leverage the protocol's built-in state management rather than reinventing it</li>\n<li><strong>Error Recovery</strong>: Implement the resubscription pattern for resilience</li>\n<li><strong>Scaling</strong>: Consider how to scale connection-heavy workloads</li>\n</ul>\n<h2 id=\"lookingforward\">Looking Forward</h2>\n<p>The A2A protocol represents a shift in how we think about AI communication—moving from transactional exchanges to ongoing, stateful interactions. As AI systems become more sophisticated, such purpose-built protocols will become increasingly important in unlocking their full potential.</p>\n<p>By providing a standardized way for AI agents to communicate, A2A opens the door to more complex agent ecosystems where specialized AIs can seamlessly collaborate, creating experiences that feel coherent and intelligent rather than disjointed.</p>",
            "url": "https://prasanna.dev/posts/a2a-vs-http",
            "title": "Understanding Google's A2A Protocol: Beyond Traditional HTTP",
            "summary": "Explore how Google's Agent-to-Agent protocol revolutionizes AI communication with task-based models, rich events, and persistent connections—solving HTTP's limitations for complex AI interactions.",
            "date_modified": "2025-04-29T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/distributed-runtime-autogen",
            "content_html": "<h2 id=\"autogenanditsdistributedruntime\">Autogen and its Distributed Runtime</h2>\n<p>Autogen (v0.4+) is a framework for building agent-based applications, supporting the orchestration of multiple agents and their intercommunication.</p>\n<p>One of the nice features in Autogen is its <a href=\"https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/framework/distributed-agent-runtime.html\">distributed runtime</a>, which enables seamless communication and coordination between agents. This makes it possible to deploy and scale agents independently in a distributed manner. The <a href=\"https://grpc.io\">gRPC (Google Remote Procedure Call)</a> backed implementation of the distributed runtime allows a robust and high-performance communication channel between agents, making it suitable for various applications, from simple tasks to complex workflows.</p>\n<p>In this blog post, will do a deep dive into the architecture of Autogen's distributed runtime, focusing on its GRPC-based implementation. If you aren't familiar with gRPC, take a look at the <a href=\"https://grpc.io/docs/what-is-grpc/introduction/\">gRPC documentation</a> for a better understanding before diving into the details here.</p>\n<p><em>Note: This is the state of things as of February 2025 in the main stable branch (v0.4). Autogen is under active development, and things might have changed since. Please refer to the <a href=\"https://microsoft.github.io/autogen/stable\">Autogen documentation</a>. All this information is based on my understanding and might be incorrect, as there isn't much documentation available on how this works in Autogen.</em></p>\n<h2 id=\"overviewofautogensdistributedagentruntimearchitecture\">Overview of Autogen's Distributed agent runtime architecture</h2>\n<p>The distributed runtime of Autogen is designed to support multiple agents working concurrently, managed by a GRPC host. The architecture facilitates communication and coordination among various components using gRPC. </p>\n<h3 id=\"architecturediagram\">Architecture Diagram</h3>\n<p><img src=\"/assets/posts/images/distributed-runtime/image-1.png\" alt=\"Autogen's Distributed Runtime Architecture\" /></p>\n<h3 id=\"components\">Components</h3>\n<ol>\n<li><strong>GRPC Host:</strong> The central component that manages communication between different runtimes and agents. It acts as a server to which GRPC runtimes connect.</li>\n<li><strong>GRPC Runtime (Client):</strong> There are multiple instances of GRPC runtime, each functioning as a client connected to the GRPC host. Each runtime is responsible for managing one or more agents. The diagram shows two such runtimes, but there can be more.\nInner Workings of a GRPC Runtime (Client)</li>\n</ol>\n<h4 id=\"someofthemainconstituentsofagrpcruntimeincludes\">Some of the main constituents of a GRPC Runtime includes,</h4>\n<ul>\n<li><strong>Host Connection:</strong> Manages the connection to the GRPC Host. This is crucial for communication between the runtime and the host, allowing the runtime to send and receive messages. This has a <code>read_loop</code> which allows you to read and write (I know 😃) messages to the gRPC host.</li>\n<li><strong>Subscription Manager:</strong> The Subscription Manager plays a crucial role in this setup by managing which agents are subscribed to specific topics. When agents publish messages to topics, the Subscription Manager looks up the current subscriptions and ensures that events are relayed to all subscribed agents. This process involves updating subscription lists dynamically and ensuring that the correct agents receive the relevant messages.</li>\n<li><strong>Serialization Registry:</strong> Responsible for serializing and deserializing messages and data. Currently supports JSON format only.</li>\n</ul>\n<h3 id=\"communication\">Communication</h3>\n<ul>\n<li><strong>Unary Agent Worker Messages:</strong> These are direct, one-way messages sent from the runtimes to the Host. They can include messages like RegisterAgent, AddSubscription, and RemoveSubscription.</li>\n<li><strong>Bidirectional Stream (to send cloudevents):</strong> This is a continuous, two-way communication channel between the GRPC Host and the runtimes. It allows for real-time exchange of cloud events, which can be critical for coordinating actions across different agents and runtimes.</li>\n</ul>\n<h3 id=\"agents\">Agents</h3>\n<p>Agents in general are unaware of the distributed nature of the runtime. They interact with the GRPC Runtime as if it were a local <code>AgentRuntime</code>, allowing for a seamless experience. You can extend the <code>RoutedAgent</code> and build out custom agent ensure to pass on the GRPC Runtime while the agent performs <code>register</code> operation. The BaseAgent has to manage the state (chat_history) of the agent. </p>\n<h2 id=\"crosslanguageagents\">Cross language agents</h2>\n<p>One of the main reasons I delved into this topic is to understand the effort involved in building and supporting cross-language agents.</p>\n<p>For example, how can I build a TypeScript agent that can be integrated into the distributed runtime and communicate with a Python agent? I believe they chose gRPC to enable such language-agnostic communication. The documentation also hints at this.</p>\n<p><em>\"The process described above is largely the same, however all message types MUST use shared protobuf schemas for all cross-agent message types.\"</em></p>\n<p>Here are my thoughts on how this can be achieved:</p>\n<ul>\n<li>They have proto files checked into the repository: <code>agentworker.proto</code> for unary messages and <code>cloudevent.proto</code> for the bidirectional stream.</li>\n<li>The language of choice can generate the client and server code from these proto files. For example, in TypeScript, you can use the <code>grpc-tools</code> package to generate the necessary code.\nThere should be an equivalent of <code>GrpcWorkerAgentRuntime</code> in your language of choice. This runtime doesn't have to support the entire functionality currently implemented in Python, but the communication part should be implemented as specified using the aforementioned proto files. This is no easy task given the complexity of the runtime, but <em>where there is a <a href=\"https://github.com/features/copilot\">GH Copilot</a>, there is a way</em> 😎</li>\n<li><code>HostConnection</code> has to be implemented to support the communication with the GRPC host.</li>\n<li><code>RoutedAgent</code> like implementation has to be present. Since the <code>BaseAgent</code> provides the <code>message_handler</code> decorator, you can use that to implement the message handlers for the various messages sent by the GRPC host.</li>\n<li>One other thing that could potentially complicate this is the behaviours of the <code>BaseAgent</code> and <code>AgentRuntime</code> to see if there are any other dependencies that needs to be implemented.</li>\n<li>and more…</li>\n</ul>\n<h2 id=\"challengesiseeasofnow\">Challenges I see as of now</h2>\n<p><em>Most of these are my understandings from the code, so please take them with a grain of salt.</em></p>\n<ul>\n<li>The proto files aren't complete (yet.). For example, if you are looking for a <code>functionCall</code> that isn't present there at this moment.</li>\n<li>Currently, the <code>cloudevents</code> are being passed on to all registered runtimes. The runtimes then decide whether to process the request or not. So, in a scenario with too many agents, the runtimes will be bombarded with events that they don't care about.</li>\n<li>This also makes me wonder how the currently implemented orchestration pattern works out. For example, handoffs between agents. I'm not sure how the distributed runtime will handle that. But this might work since the runtime is abstracted within the agent and the teams aren't aware of the distributed nature of the runtime. (I think!)</li>\n<li>Implementing <code>GrpcWorkerAgentRuntime</code> in a different language is no easy task. There is no guide (yet). So you will have to figure it out yourself by looking into the source code and hopefully there isn't any coupling with other objects.</li>\n</ul>\n<h2 id=\"codesample\">Code sample</h2>\n<p>I started off with great hopes of implementing this distributed runtime in TypeScript, but I quickly realized that it is no easy task. So I backed off. Here is just a refactored version of the example code from the documentation.</p>\n<p><a href=\"https://github.com/prasann/autogen-distributed-chat\">Autogen's distributed runtime example</a></p>",
            "url": "https://prasanna.dev/posts/distributed-runtime-autogen",
            "title": "Understanding Autogen's Distributed agent runtime implementation.",
            "summary": "A deep dive into the GRPC-based architecture that powers seamless agent coordination and communication in Autogen.",
            "date_modified": "2025-02-20T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/realtime-api-intro-and-learnings",
            "content_html": "<h2 id=\"realtimemessagingapi\">Real time messaging API</h2>\n<p>The RealTime API allows you to have live, back-and-forth conversations with an AI model. It takes in text or audio as input and responds with text or audio—all in real-time.</p>\n<p>Unlike traditional API calls, where you send a request and wait for a full response, the RealTime API starts responding instantly as you speak or type, making interactions feel more natural and fluid.</p>\n<p>✅ <strong>Multimodal Input & Output</strong> – You can send text or speech and get back text or speech in any supported language.</p>\n<p>✅ <strong>Streaming Responses</strong> – The model starts responding immediately, instead of waiting to generate the full response.</p>\n<p>✅ <strong>Real-Time Translation</strong> – You can speak in one language, and the model can reply in another, making it great for live translations.</p>\n<p>✅ <strong>Interactive Conversations</strong> – It feels more like chatting with a real person because of its speed and responsiveness.</p>\n<h3 id=\"communicationmodes\">Communication Modes</h3>\n<p>OpenAI's RealTime API supports two communication methods: WebRTC and WebSockets</p>\n<h4 id=\"webrtcultralowlatencyaudiostreaming\">WebRTC – Ultra-Low Latency Audio Streaming</h4>\n<ul>\n<li>Designed for real-time voice conversations.</li>\n<li>Uses peer-to-peer (P2P) connections, reducing latency.</li>\n<li>Ideal for live voice assistants, AI-powered calls, and instant translations.</li>\n<li>Best when you need: natural, low-latency spoken conversations.</li>\n</ul>\n<p><img src=\"/assets/posts/images/realtime-api/image.png\" alt=\"realtime using WebRTC\" /></p>\n<h4 id=\"websocketsflexiblerealtimemessaging\">WebSockets – Flexible Real-Time Messaging</h4>\n<ul>\n<li>Uses a client-server model instead of direct P2P connections.</li>\n<li>Supports text-based chat and controlled audio streaming.</li>\n<li>Easier to integrate into existing web applications.</li>\n<li>Best when you need: chatbot-like interactions, AI narration, or real-time text communication.</li>\n</ul>\n<p><img src=\"/assets/posts/images/realtime-api/image-1.png\" alt=\"realtime using websockets\" /></p>\n<h4 id=\"azureopenaisupportsonlywebsocketsforrealtimeaiinteractions\">‼ Azure OpenAI - Supports only WebSockets for real-time AI interactions.</h4>\n<h3 id=\"sdksupportforrealtimeapi\">SDK Support for Realtime API</h3>\n<p>At the time of authoring (not publishing 😁) this article, LangChain and Semantic Kernel primarily support interactions with OpenAI's APIs through traditional HTTP requests. The Realtime API, which facilitates low-latency, streaming interactions via WebSockets (and WebRTC in some cases), is relatively new. Consequently, direct support for the Realtime API in these SDKs may be limited or under development.</p>\n<p>For instance, the Azure OpenAI Realtime Audio SDK provides code samples demonstrating how to interact with the Realtime API using WebSockets. I started off with this initially for this application.</p>\n<h4 id=\"sneakpeekintothesdklikecodesample\">Sneak peek into the SDK (like) code sample</h4>\n<ul>\n<li>Works with text messages, function tool calling, and many other existing capabilities from other endpoints like /chat/completions</li>\n<li>Has samples in multiple languages, python, java, javascript, dotnet (not sure why this is called as language though 🤔)</li>\n</ul>\n<p>Flow of this code sample looks like this,</p>\n<p><img src=\"/assets/posts/images/realtime-api/image-2.png\" alt=\"alt text\" /></p>\n<p>This looks great, however I needed 2 things on top of this!!</p>\n<ul>\n<li>A client application which can interact with the realtime host using voice.</li>\n<li>Use my own data as part of the conversation (aka Retrieval-Augmented Generation - RAG)</li>\n</ul>\n<h3 id=\"ragwithrealtime\">RAG with RealTime</h3>\n<p>When building a conversational AI, I wanted the bot to engage with my own data rather than just relying on the model’s built-in knowledge. This meant using a Retrieval-Augmented Generation (RAG) approach, where the AI could dynamically fetch relevant information from my custom dataset.\nHowever, I initially had concerns about how to integrate RAG into a real-time interaction, especially when dealing with audio input. How would the model know when to retrieve external data? How could it seamlessly pull information while maintaining a fluid conversation?\n\"The best approach for incorporating RAG in RealTime APIs is through Tools and Function Calling. Instead of manually injecting external data into every request, I expose a custom tool that allows the model to query my vector database whenever necessary.\"</p>\n<p>Here’s how it works:</p>\n<ul>\n<li>Storing Knowledge: A vector database contains my personal or domain-specific knowledge, encoded as embeddings for efficient retrieval.</li>\n<li>Defining the Tool: I define a tool (or function) that provides the model with access to this vector database.</li>\n<li>Dynamic Decision-Making: The model autonomously decides when to use the tool—similar to how a person might decide to \"Google something\" when they lack certain information.</li>\n<li>Real-Time Retrieval: If the model determines that external context is required, it queries the vector database in real time, retrieves the relevant information, and seamlessly incorporates it into its response.\nThis method keeps the conversation dynamic, ensures accuracy, and allows the AI to stay up to date with your specific information.</li>\n</ul>\n<h4 id=\"ragusingazureaisearch\">RAG using Azure AI Search</h4>\n<p>Now moving into building this out. I stumbled upon this fantastic quickstart solution from Azure-Samples that fits my needs perfectly!</p>\n<p><a href=\"https://github.com/azure-samples/aisearch-openai-rag-audio\">AI Search RAG Audio</a></p>\n<p>This quick-start solution sets up a Retrieval-Augmented Generation (RAG)-powered conversational AI. It includes a sample document in Azure Blob Storage, indexed for efficient retrieval. A custom tool handles the RAG process, enabling the AI to fetch relevant information dynamically. The React-based frontend offers an intuitive user interface. This customizable setup serves as a foundation for deploying RAG-enabled conversational systems.</p>\n<p><img src=\"/assets/posts/images/realtime-api/image-3.png\" alt=\"architecture of rag audio\" /></p>\n<p>On top of this,</p>\n<ul>\n<li>Note taking feature:</li>\n<li>Takes notes, and i can use that information to build conversation.</li>\n<li>Cosmos DB store with an indexer to AI Search</li>\n<li>Azure AD - Google authentication</li>\n<li>Progressive Web Application with the React boilerplate from the quick-start.</li>\n</ul>\n<p><img src=\"/assets/posts/images/realtime-api/image-4.png\" alt=\"tech stack of ai-assistant\" /></p>\n<p>Here is the link to my repository, if you are interested to check it out: GitHub - <a href=\"https://github.com/prasann/ai-assistant\">VASI: Digital Assistant</a></p>\n<h3 id=\"keylearningsintherealtimeapi\">Key learnings in the Realtime API</h3>\n<h4 id=\"eventseventsandevents\">Events, Events, and Events</h4>\n<p>Unlike the traditional Chat Completions API, where a request-response model is followed, the Realtime API is fundamentally event-driven. Everything—whether it's receiving responses, detecting pauses in speech, or transcribing input—is handled through events over a single persistent connection.</p>\n<h4 id=\"bidirectionalcommunicationwithevents\">Bidirectional Communication with Events</h4>\n<ul>\n<li>The client sends audio input as an event stream.</li>\n<li>The server processes the speech in real-time and responds with multiple events such as transcriptions, intermediate responses, and final outputs.</li>\n<li>This enables a fluid, low-latency interaction, much closer to how a real-time assistant should behave.</li>\n</ul>\n<h4 id=\"turndetectionknowingwhentorespond\">Turn Detection – Knowing When to Respond</h4>\n<ul>\n<li>Unlike traditional APIs where explicit end-of-input markers are needed, the Realtime API can intelligently detect when a user has finished speaking.</li>\n<li>This is powered by turn detection, which allows for natural pauses in speech before processing the response, mimicking human-like interaction.</li>\n</ul>\n<h4 id=\"builtinspeechtotextnoadditionalserviceneeded\">Built-in Speech-to-Text (No Additional Service Needed!)</h4>\n<ul>\n<li>The session itself handles input transcription, eliminating the need for an external speech-to-text service like Whisper.</li>\n<li>This means seamless integration of voice interactions without additional API calls or dependencies.</li>\n</ul>\n<p>In conclusion, a quick summary on the key differences between OpenAI’s Realtime API and the usual Chat Completions API:</p>\n<table>\n  <thead style=\"text-align: left;\">\n    <tr>\n      <th>Feature</th>\n      <th>Realtime API</th>\n      <th>Chat Completions API</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Interaction Model</td>\n      <td>Event-driven (Streaming & Events)</td>\n      <td>Request-response</td>\n    </tr>\n    <tr>\n      <td>Latency</td>\n      <td>Ultra-low (Real-time processing)</td>\n      <td>Higher (Batch-based)</td>\n    </tr>\n    <tr>\n      <td>Input Modality</td>\n      <td>Speech & Text</td>\n      <td>Text-only</td>\n    </tr>\n    <tr>\n      <td>Built-in Speech-to-Text?</td>\n      <td>Yes, auto-transcribed (input_audio_transcription)</td>\n      <td>No, requires Whisper or another STT service</td>\n    </tr>\n    <tr>\n      <td>Turn Detection?</td>\n      <td>Yes, detects natural pauses</td>\n      <td>No, explicit input-end needed</td>\n    </tr>\n    <tr>\n      <td>Best Use Case</td>\n      <td>Real-time voice assistants, AI companions</td>\n      <td>Text-based chatbots, async responses</td>\n    </tr>\n  </tbody>\n</table>",
            "url": "https://prasanna.dev/posts/realtime-api-intro-and-learnings",
            "title": "OpenAI's Realtime API - Intro and Key Learnings",
            "summary": "Worked on a personal digital assistant using the Realtime API from Azure OpenAI. Here are some key learnings and insights from the experience.",
            "date_modified": "2025-02-10T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/google-maps-timeline-viewer",
            "content_html": "<h2 id=\"googlemapstimeline\">Google Maps Timeline</h2>\n<p>If you use an Android phone, Google Maps is probably your default location service. In 2023, Google <a href=\"https://blog.google/products/maps/updates-to-location-history-and-new-controls-coming-soon-to-maps/\">announced</a> that timeline information would be stored on the device itself to protect user privacy.</p>\n<p>After this update, timeline information is no longer stored on Google servers but only on your mobile device. This means you can't view your timeline on the web anymore.</p>\n<p>If you use timeline information to remember places you've visited, you now have to do this within the mobile app. While this can be inconvenient, it enhances user privacy, which is a positive step.</p>\n<h2 id=\"whatistimelineinformation\">What is Timeline Information?</h2>\n<p>There are two types of timeline data:</p>\n<ol>\n<li><p><strong>Google Takeout Data</strong>: Includes your timeline edits, contributions to Google Maps (like reviews and photos), favorited places, and created collections. It doesn't include the timeline paths showing where you visited and when.</p></li>\n<li><p><strong>Mobile App Data</strong>: Includes the coordinates of places you visited, the times you visited them, and the modes of transport you used. This helps track your visits and correlate them with your photos and notes.</p></li>\n</ol>\n<p>To download this data on your Android phone, go to Location -&gt; Location Services -&gt; Timeline -&gt; Export Timeline Data. This file will be large and contains sensitive information, so handle it carefully.</p>\n<h2 id=\"workaroundfortheweb\">Workaround for the Web</h2>\n<p>I got the idea of using <a href=\"https://kepler.gl/demo\">Kepler</a> to visualize timeline data on the web from this <a href=\"https://gist.github.com/devdattaT/018f7fc153d9a82d83775351576965f3\">gist</a>. However, at the time of writing this article, the gist didn't work with the new data format.</p>\n<p>Instead, you can use the <a href=\"https://github.com/prasann/google-timeline-parser\">Google Maps Timeline Parser</a> repository to parse the recent data format and output CSV files.</p>\n<p>There are two outputs that you get out of the scripts in the repository.</p>\n<ol>\n<li><strong>Timeline Path</strong>: Contains all the paths Google thinks you have taken. Plotting this on Kepler gives you many points, showing a cluster of paths.</li>\n<li><strong>Places</strong>: Contains all the places you have visited. Plotting this on Kepler gives you a clear picture of the places you have visited.</li>\n</ol>\n<p>Using the script in the repo, generate the csv files and upload them to <a href=\"https://kepler.gl/demo\">Kepler</a>. You can now visualize your timeline data on the web.</p>",
            "url": "https://prasanna.dev/posts/google-maps-timeline-viewer",
            "title": "View your Google timeline information on the Web in 2025",
            "summary": "Learn how to view your Google Maps timeline and places information on the web.",
            "date_modified": "2025-01-01T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/auto-function-calling-semantic-kernel",
            "content_html": "<h2 id=\"semantickernel\">Semantic Kernel</h2>\n<p><a href=\"https://github.com/microsoft/semantic-kernel\">Semantic Kernel</a> is an SDK that integrates Large Language Models (LLMs) like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C#, Python, and Java. It is a drop-in alternative for LangChain, enabling developers to leverage the power of LLMs in their applications.</p>\n<h2 id=\"autofunctioncallingwithgptmodels\">Auto function calling with GPT models</h2>\n<p>Function calling in Large Language Models (LLMs) is a powerful technique that allows these models to interact with external systems, APIs, and tools. This capability extends the functionality of LLMs beyond text generation, enabling them to perform actions, retrieve real-time information, and execute tasks by leveraging external resources.</p>\n<p>In this article, we explore a sample application that demonstrates how to automatically call functions using GPT (gpt-4o) models and Semantic Kernel. Here we let GPT model to decide what is the appropriate function to call based on the user input, and performs the function call. All these will be orchestrated by Semantic Kernel.</p>\n<h2 id=\"scenario\">Scenario</h2>\n<p>In this sample application, GPT is provided with 2 functions.</p>\n<ol>\n<li>Bing Search - This function takes a search query as input and returns the top search results from Bing.</li>\n<li>Azure AI Search on a custom dataset - This function invokes the Azure AI Search endpoint to perform a vector search on a custom pre-indexed data. For this example, we have a refrigerator-manual being indexed and the function returns the most relevant section from the manual based on the user query.</li>\n</ol>\n<p>When a user inputs a query, the GPT model predicts the function to call based on the input and Semantic Kernel orchestrates the function call.</p>\n<h2 id=\"approach\">Approach</h2>\n<p>In Semantic Kernels, these functions can be represented as <a href=\"https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/\">plugins</a>. Each plugin is a self-contained unit that encapsulates the function's logic, input, and output. The GPT model predicts the plugin to call based on the user input, and Semantic Kernel executes the corresponding plugin.</p>\n<p>For our scenario, there are 2 plugins.</p>\n<ol>\n<li><p>Bing Search Plugin: This comes with Semantic Kernel SDK and is used to search Bing. This uses Bing Service in Azure, and the plugin is configured to call the Bing Search API.</p></li>\n<li><p>A Custom Plugin to call Azure AI Search: This custom plugin will allow GPT to make calls to the Azure AI Search service and perform a vector search on a custom dataset.</p></li>\n</ol>\n<p>The approach is pretty straightforward:</p>\n<ol>\n<li>Initialize the Semantic Kernel with the GPT model and the plugins.</li>\n</ol>\n<pre><code class=\"python language-python\">kernel = Kernel()\n\n# add bing plugin\n\nconnector = BingConnector()\nkernel.add_plugin(WebSearchEnginePlugin(connector), \"BingIt\")\n\n# add custom plugin\n\nsearch_client = SearchClient(os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\"), \n                                 os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\"), \n                                 AzureKeyCredential(os.getenv(\"AZURE_AI_SEARCH_API_KEY\")))\n    kernel.add_plugin(RefrigeratorInfoPlugin(search_client), \"RefrigeratorInfo\")\n</code></pre>\n<ol>\n<li>Enable Auto function calling in the Semantic Kernel.</li>\n</ol>\n<pre><code class=\"python language-python\">execution_settings = AzureChatPromptExecutionSettings(tool_choice=\"auto\")\nexecution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(auto_invoke=True)\n</code></pre>\n<ol>\n<li>Interact with the user and let the GPT model predict the function to call.</li>\n</ol>\n<h2 id=\"codesnippet\">Code Snippet</h2>\n<p>You can view the whole working code sample here: <a href=\"https://github.com/prasann/auto-fn-invocation-sk\">Semantic Kernel Auto Function Calling</a></p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>With about 80 lines of code, we are able to create a sample application that demonstrates how to automatically call functions using GPT models and Semantic Kernel. This approach can be extended to a wide range of applications, including chatbots, virtual assistants, and automation tools.</p>",
            "url": "https://prasanna.dev/posts/auto-function-calling-semantic-kernel",
            "title": "Auto function calling with GPT models using Semantic Kernel",
            "summary": "Writeup on a sample application that demonstrates how to automatically call functions using GPT models and Semantic Kernel. This notebook",
            "date_modified": "2024-09-23T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/quota-management-for-dalle-apim",
            "content_html": "<p>Enterprises often work with applications that use OpenAI's technologies. Their teams typically want to explore these applications but need to limit API usage to avoid excessive costs. This means controlling the number of API calls and setting constraints on API usage. Such tasks are usually handled at the Gateway layer of applications, which is where API Management becomes important.</p>\n<h2 id=\"scenario\">Scenario</h2>\n<p>Consider a scenario where an enterprise uses OpenAI's DALL-E API to generate images. They want to limit the number of images generated, which could be set on a per-minute, daily, weekly, or another time basis.</p>\n<h2 id=\"apimgatewayforgenerativeai\">APIM Gateway for Generative AI</h2>\n<p><a href=\"https://learn.microsoft.com/en-us/azure/api-management/api-management-key-concepts\">Azure API Management (APIM)</a> is a service offering a scalable, multi-cloud API management platform for securing, publishing, and analyzing APIs. It acts as a gateway for APIs, allowing management and usage constraints. Here is a useful article on using APIM as a gateway for Generative AI APIs. <a href=\"https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/dev-starters/genai-gateway/reference-architectures/apim-based\">GenAI Gateway: Managing Generative AI APIs with Azure API Management</a></p>\n<h2 id=\"quotamanagementfordalleapi\">Quota Management for DALL-E API</h2>\n<p>In this article, we will discuss how to manage quotas for the DALL-E API using Azure API Management. We'll explore how to limit the number of images that can be generated through this API.</p>\n<p>To accomplish this with APIM, we can use <code>rate-limit</code> or <code>quota</code> policies. For more details on the differences between these policies, refer to the <a href=\"https://learn.microsoft.com/en-us/azure/api-management/api-management-sample-flexible-throttling#rate-limits-and-quotas\">official documentation</a>.</p>\n<p>The following logic can be applied to both policies:</p>\n<h3 id=\"approach\">Approach</h3>\n<ul>\n<li>The requests made to the DALL-E endpoints contain the number of images to be generated. <a href=\"https://platform.openai.com/docs/api-reference/images/create#images-create-n\">Ref</a></li>\n<li>APIM can track this number and decrease a counter as specified by the policy.</li>\n<li>Ensure the counter is updated only after successful responses from the DALL-E API, and reset the counter at the end of the time period.</li>\n</ul>\n<h2 id=\"policysnippet\">Policy Snippet</h2>\n<p><a href=\"https://gist.github.com/prasann/f81d3ecc30729e6d6f8744622336cf83\">Link to the Gist</a></p>\n<pre><code class=\"xml language-xml\">&lt;policies&gt;\n    &lt;inbound&gt;\n        &lt;base /&gt;\n        &lt;set-variable name=\"body\" value=\"@(context.Request.Body.As&lt;string&gt;(preserveContent: true))\" /&gt;\n        &lt;choose&gt;\n            &lt;when condition=\"@(context.Request.Body.As&lt;JObject&gt;()[\"n\"] != null)\"&gt;\n                &lt;rate-limit-by-key calls=\"1\" renewal-period=\"60\" counter-key=\"@(context.Request.IpAddress)\" increment-condition=\"@(context.Response.StatusCode == 200)\" increment-count=\"@(int.Parse(context.Request.Body.As&lt;JObject&gt;()[\"n\"].ToString()))\" retry-after-header-name=\"x-apim-custom-retry\" remaining-calls-header-name=\"x-apim-remaining\" /&gt;\n            &lt;/when&gt;\n            &lt;otherwise&gt;\n                &lt;rate-limit-by-key calls=\"1\" renewal-period=\"60\" counter-key=\"@(context.Request.IpAddress)\" increment-condition=\"@(context.Response.StatusCode == 200)\" increment-count=\"1\" retry-after-header-name=\"x-apim-custom-retry\" remaining-calls-header-name=\"x-apim-remaining\" /&gt;\n            &lt;/otherwise&gt;\n        &lt;/choose&gt;\n        &lt;set-body&gt;@((string)context.Variables[\"body\"])&lt;/set-body&gt;\n    &lt;/inbound&gt;\n    &lt;!-- Control if and how the requests are forwarded to services  --&gt;\n    &lt;backend&gt;\n        &lt;base /&gt;\n    &lt;/backend&gt;\n    &lt;!-- Customize the responses --&gt;\n    &lt;outbound&gt;\n        &lt;base /&gt;\n    &lt;/outbound&gt;\n    &lt;!-- Handle exceptions and customize error responses  --&gt;\n    &lt;on-error&gt;\n        &lt;base /&gt;\n    &lt;/on-error&gt;\n&lt;/policies&gt;\n</code></pre>\n<p>Here is the link to the gist, containing the policy snippet and an explanation <a href=\"https://gist.github.com/prasann/f81d3ecc30729e6d6f8744622336cf83\">APIM Policy Snippet</a>. This can be achieved by using the <code>quota</code> policy as well.</p>",
            "url": "https://prasanna.dev/posts/quota-management-for-dalle-apim",
            "title": "Quota management for DallE API using APIM",
            "summary": "This article explores how enterprises can use Azure API Management to implement usage limits and quotas for the DALL-E API, ensuring cost-effective and controlled access to OpenAI's image generation capabilities.",
            "date_modified": "2024-06-25T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/exploring-ondc",
            "content_html": "<h2 id=\"decodingindiasdigitalcommercerevolutionacloselookatondc\">Decoding India's Digital Commerce Revolution: A Close Look at ONDC</h2>\n<p>Upon reflecting on my recent presentation at the Retail Learning Hangout, I delved into an initiative poised to redefine India’s digital commerce landscape—the Open Network for Digital Commerce (ONDC).</p>\n<h3 id=\"pioneeringdigitalinclusioninindia\">Pioneering Digital Inclusion in India</h3>\n<p>India's journey towards digital inclusivity didn't start with ONDC. It's built on the success of initiatives like Aadhaar and the Unified Payments Interface (UPI), showcasing the potential of government-backed technology in driving social and financial inclusion. </p>\n<p><img src=\"/assets/posts/images/ondc/01-dpi-circle.jpg\" alt=\"DPI circles \" title=\"DPI circles\" /></p>\n<p>ONDC steps into this arena, aiming to push the digital frontier further and extend e-commerce access to millions of small traders and consumers nationwide who are yet to establish an online presence. Surprisingly, out of the 18 million registered traders in India, only 1.5 million have an online presence. Herein lies the gap—and the opportunity—that ONDC seeks to address.</p>\n<h3 id=\"revolutionizingecommercethroughondc\">Revolutionizing E-commerce through ONDC</h3>\n<p>ONDC stands out due to its shift from the prevailing platform-based e-commerce model to a network-centric approach. This transition isn't just technical but philosophical, prioritizing openness and accessibility. ONDC's model removes barriers, enabling direct interaction between buyers and sellers through a decentralized network.</p>\n<p><img src=\"/assets/posts/images/ondc/02-ondc-intro.jpg\" alt=\"ONDC Introduction \" title=\"ONDC Introduction\" /></p>\n<h3 id=\"keychallengesinthecurrentecommercelandscape\">Key Challenges in the Current E-commerce Landscape</h3>\n<p><img src=\"/assets/posts/images/ondc/03-problem-statement.jpg\" alt=\"Buyer and Seller problems \" title=\"Buyer and Seller problems\" /></p>\n<h3 id=\"empoweringsellersandenrichingbuyerexperience\">Empowering Sellers and Enriching Buyer Experience</h3>\n<h4 id=\"forsellers\">For Sellers</h4>\n<ul>\n<li><strong>Affordable Access:</strong> Sellers gain cost-effective entry into the entire value chain. A single registration grants them visibility across all buyer applications on the network.</li>\n<li><strong>Flexible Terms:</strong> Sellers can set their preferred transaction conditions.</li>\n<li><strong>Increased Profitability:</strong> With no intermediaries, sellers enjoy higher earnings and reduced risks.</li>\n<li><strong>Seamless Transition:</strong> Sellers uphold their network-wide reputation and can switch between seller apps effortlessly. ONDC provides detailed business analytics to aid strategic planning.</li>\n</ul>\n<p><img src=\"/assets/posts/images/ondc/06-seller.jpg\" alt=\"Seller gains \" title=\"Seller gains\" /></p>\n<h4 id=\"forbuyers\">For Buyers</h4>\n<ul>\n<li><strong>Expanded Access:</strong> Buyers can explore a vast catalog through various channels (chatbots, IVR, SMS, etc.), even without an internet connection.</li>\n<li><strong>Enhanced Selection:</strong> The network offers a wide range of sellers for a given product, improving the selection process.</li>\n<li><strong>Convenient Transactions:</strong> Buyers can review delivery options, settle payments conveniently, and choose from multiple sellers in a single transaction.</li>\n<li><strong>Trustworthy Environment:</strong> ONDC ensures reliable grievance management, fostering a secure shopping environment.</li>\n</ul>\n<p><img src=\"/assets/posts/images/ondc/05-buyer.jpg\" alt=\"Buyer experience \" title=\"Buyer experience\" /></p>\n<h3 id=\"unveilingthetechlandscape\">Unveiling the Tech Landscape</h3>\n<p>Central to ONDC's proposition is its network-centric model, democratizing digital commerce for small traders. Through technical illustrations, the presentation showcased how ONDC integrates various players across the commerce value chain, from buyers and sellers to logistics service providers, via a decentralized registry and gateway system.</p>\n<p><img src=\"/assets/posts/images/ondc/07-a-inter-connectivity.jpg\" alt=\"Inter connectivity of Networks \" title=\"Tech Landscape\" /></p>\n<h3 id=\"thebecknprotocol\">The Beckn Protocol</h3>\n<p>At the core of ONDC's mechanics lies the Beckn protocol, facilitating seamless interoperability within the network. Similar to SMTP for email, Beckn revolutionizes online commerce by emphasizing openness, community governance, and secure transactions. The protocol ensures that any entity, from a local grocery store to a large e-commerce retailer, can connect, interact, and transact within the ONDC network without platform dependence.</p>\n<h4 id=\"explainingbecknthroughsmtp\">Explaining Beckn through SMTP</h4>\n<p><img src=\"/assets/posts/images/ondc/08-smtp-01.jpg\" alt=\"SMTP Explained \" title=\"SMTP Explained\" /></p>\n<p><img src=\"/assets/posts/images/ondc/09-beckn-01.jpg\" alt=\"Beckn Explained \" title=\"Beckn explained\" /></p>\n<h4 id=\"comparisonofhttpandbeckn\">Comparison of HTTP and Beckn</h4>\n<p><img src=\"/assets/posts/images/ondc/10-beckn.jpg\" alt=\"HTTP vs Beckn \" title=\"HTTP vs Beckn\" /></p>\n<h3 id=\"designandarchitectureinsights\">Design and Architecture Insights</h3>\n<p>The layered architecture of Beckn and ONDC, reminiscent of the OSI model of network communication, highlights the sophistication behind this concept. Beckn provides APIs and models for digital commerce transactions across various phases, supporting an ecosystem where data privacy, security, and global interoperability are foundational.</p>\n<p><img src=\"/assets/posts/images/ondc/07-architecture.jpg\" alt=\"The architecture \" title=\"Architecture\" /></p>\n<h3 id=\"addressingchallenges\">Addressing Challenges</h3>\n<p>The journey towards widespread ONDC adoption is not without challenges, particularly surrounding post-purchase grievance handling and trust mechanisms within the decentralized network. However, drawing parallels with UPI's trajectory offers hope. Like UPI, ONDC holds the promise of overcoming obstacles progressively.</p>\n<p>In conclusion, ONDC represents a bold step towards reshaping India’s digital commerce ecosystem. It's a movement I'm proud to be part of, fostering dialogue and sharing insights on platforms like the Retail Learning Hangout, where ideas converge to shape the future.</p>\n<p>You can view the full presentation here: <a href=\"https://docs.google.com/presentation/d/e/2PACX-1vRuJRGH288x5i7T5RcDHyjrIvI7sHiz_z5w4sKC8E9CqmkyW4aL4Xjaa92sw8anQh5sgH7VwrjzBtJA/embed?start=false&loop=true&delayms=3000\">ONDC - India's Digital Commerce Revolution</a></p>\n<p><strong>Note: All the images are sourced mostly from <a href=\"https://becknprotocol.io/\">Beckn</a> and from <a href=\"https://ondc.in/\">ONDC</a></strong></p>",
            "url": "https://prasanna.dev/posts/exploring-ondc",
            "title": "ONDC - India's Digital Commerce Revolution",
            "summary": "Drawing upon insights from a recent presentation, I examine ONDC's potential to bridge the digital divide, its network-centric model, and the collaborative push towards a more inclusive digital marketplace.",
            "date_modified": "2023-12-07T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/autogen-conversational-framework",
            "content_html": "<p><a href=\"https://github.com/microsoft/autogen\">Autogen framework</a> offers developers with a toolkit that harnesses the power of functions, enabling the creation of conversational agents capable of solving diverse tasks. Apart its simplicity of APIs, it also offers multiple modes including the human-in-the-loop model, thereby simplifying the development of complex applications.</p>\n<h2 id=\"differentiatingautogenfromlangchainorsemantickernel\">Differentiating Autogen from Langchain or Semantic Kernel</h2>\n<ol>\n<li><p><strong><a href=\"https://microsoft.github.io/autogen/\">Autogen</a></strong>: Tailored for multi-agent conversations, Autogen streamlines complex LLM workflows, optimizing performance and supporting diverse conversation patterns. Its strength lies in enhancing LLM inference within multi-agent frameworks. It also offers a human-in-the-loop, meaning that the user can intervene in the conversation at any point, providing feedback or additional information to the agents and controlling the conversation flow.</p></li>\n<li><p><strong><a href=\"https://python.langchain.com/docs/get_started/introduction\">Langchain</a></strong>: Focused on ease and integration, Langchain offers a streamlined approach for creating LLM-based applications. It uniquely utilizes a pipeline combining question vector representation and similarity search for efficient information retrieval. Langchain is ideal for developers seeking a simple yet powerful tool for LLM application development.</p></li>\n<li><p><strong><a href=\"https://learn.microsoft.com/en-us/semantic-kernel/overview/\">Semantic Kernel</a></strong>: An open-source SDK, Semantic Kernel simplifies the integration of conventional programming languages with LLMs. It stands out by allowing the chaining of plugins to call existing code, bridging LLMs with various sources of context. Semantic Kernel is a versatile tool for developers looking to leverage LLMs within existing codebases with C#, Python.</p></li>\n</ol>\n<h2 id=\"aconversationalusecaseforautogenframework\">A conversational Use-case for Autogen Framework</h2>\n<p>Planning a trip can be hard, needing careful planning for staying, things to do, and eating. Microsoft's Autogen makes this easy by using AI. It also allows for human intervention to guide the process when needed.</p>\n<p>Imagine planning a weekend trip with friends. With Autogen, planning becomes easy because of its AI agents and the option for human control. Here's how they help:</p>\n<p><strong>HotelBooking Agent</strong>: This agent helps you find a place to stay. It understands what you want and quickly finds options that fit your needs. If needed, a person can step in to adjust choices.</p>\n<p><strong>Activities Agent</strong>: Next, this agent finds things to do based on your interests. It gives suggestions for all kinds of activities to make your trip fun. Humans can also guide these suggestions.</p>\n<p><strong>FoodOptions Agent</strong>: This agent helps you decide where to eat by recommending places that match your taste and diet. Again, there's room for human input to refine choices.</p>\n<p><strong>Collaborative Coordination with Human in the Loop</strong>: Autogen's strength is making these agents work together well, with the option for human intervention. They adjust suggestions based on your choices, making planning easy and tailored to you, with a human touch when needed.</p>\n<p>A sample of this workflow by this twitter user,</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">yo wtf, Microsoft&#39;s newly released AutoGen is fk&#39;in dope..<br><br>- allows for multiple agents that align to one goal<br>- human input allowed<br><br>🤖 I created 5 agents to help me plan a 30-day iternary in Bangkok<br><br>(results below ↓) <a href=\"https://t.co/klW3Go1d1c\">pic.twitter.com/klW3Go1d1c</a></p>&mdash; peter! 🥷 (@pwang_szn) <a href=\"https://twitter.com/pwang_szn/status/1707334415691686227?ref_src=twsrc%5Etfw\">September 28, 2023</a></blockquote>\n<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n<h2 id=\"endlesspossibilitieswithautogen\">Endless Possibilities with Autogen</h2>\n<p>Beyond trip planning, Autogen finds applications in a plethora of domains. From customer service automation to virtual assistants in healthcare, Autogen's customizable agents prove invaluable in streamlining operations and enhancing user experiences. In e-commerce, Autogen facilitates personalized product recommendations and seamless order management. In education, it enables interactive learning experiences tailored to individual students' needs. With Autogen, the possibilities are limitless, revolutionizing how we interact with technology across various sectors.</p>\n<p>In conclusion, Microsoft's Autogen framework represents a paradigm shift in conversational AI, empowering developers to craft sophisticated agents tailored to diverse tasks and environments. With its emphasis on customizable functions and collaborative modes, Autogen heralds a new era of conversational agents, redefining the boundaries of human-machine interaction.</p>",
            "url": "https://prasanna.dev/posts/autogen-conversational-framework",
            "title": "Autogen - Make your agents to collaborate",
            "summary": "Autogen Framework simplifies the construction of complex applications by enabling customizable conversational agents that operate smoothly together to carry out tasks efficiently.",
            "date_modified": "2023-10-11T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/langchain-llm-app-development",
            "content_html": "<p>This is a personal notes from the course <a href=\"https://learn.deeplearning.ai/langchain/lesson/1/introduction\">Langchain for LLM application development</a>. This one of the short courses offered by the <a href=\"https://deeplearning.ai\">Deeplearning</a>.</p>\n<p>It is mentioned as a 1 hour course, but it took me around 5 hours to complete the course. The course content was using openAI. I tried using Azure OpenAI in all the workbooks, attached the chapter workbook links along with a short excerpt for each chapter.</p>\n<p>This course, will give a good introduction to the Langchain and how to use it to build LLM applications using them. Here is the summary of what is being covered in these chapters.</p>\n<h2 id=\"chapter1modelspromptsandparsers\">Chapter 1: Models, Prompts and Parsers</h2>\n<ul>\n<li><strong>Models</strong> refers to the large language models</li>\n<li><strong>Prompts</strong> refers to the text that you give to the model. Langchain offers an elegant way to construct these prompts. You can also construct your own prompts.</li>\n<li><strong>Parsers</strong> refers to the code that you write to parse the output from the model. Langchain provides conventions, to define the parsers.</li>\n</ul>\n<p><a href=\"https://github.com/prasann/langchain-deeplearning/blob/main/L1-prompts-models-parsers.ipynb\">Link to the workbook</a></p>\n<h2 id=\"chapter2memory\">Chapter 2: Memory</h2>\n<p><strong>Memory</strong> refers to the ability to store information and retrieve it later. This enables the user to have a chat like conversation with the LLMs.</p>\n<p>Some of the <code>memory</code> options discussed in the course are:</p>\n<ul>\n<li><p><strong>ConversationBufferMemory</strong> - It is the simplest option. Keeps storing all the conversations in the memory and keep sending them in as a context for the subsequent calls. However, this is not a good option because the memory keeps growing and the model will not be able to handle the large memory. Things gets slow and also the cost of the inference will be high.</p></li>\n<li><p><strong>ConversationBufferWindowMemory</strong> - It is similar to the <code>ConversationBufferMemory</code>, but it keeps only the last <code>n</code> conversations in the memory. This is a better option than the <code>ConversationBufferMemory</code>, but still might not be the best. The first conversation of the chat could be very important, and losing that context might have consequences on the conversation.</p></li>\n<li><p><strong>ConversationTokenBufferMemory</strong> - It is similar to that of <code>ConversationBufferWindowMemory</code>, but instead of storing <code>n</code> conversations it stores <code>n</code> tokens. Still have the same drawback of the <code>ConversationBufferWindowMemory</code>.</p></li>\n<li><p><strong>ConversationSummaryMemory</strong> -  Here a summary of the previous conversations are generated and used that as a context for the subsequent calls. This is a better option than the previous ones.</p></li>\n</ul>\n<p><a href=\"https://github.com/prasann/langchain-deeplearning/blob/main/L2-memory.ipynb\">Link to the workbook</a></p>\n<h2 id=\"chapter3chains\">Chapter 3: Chains</h2>\n<p>Chains are simple wrapper around the Langchain components. It takes the user input, prompts, models and parsers and returns the output. It is a simple wrapper around the Langchain components.</p>\n<p><strong>LLMChain</strong> - A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser.</p>\n<h3 id=\"sequentialchains\"><strong>Sequential Chains</strong></h3>\n<p>Sequential chains allows to chain multiple <code>LLMChain</code> together.</p>\n<p><strong>SimpleSequentialChain</strong> - this allows the output of the first chain is used as the input for the second chain and so on.</p>\n<p><img src=\"/assets/posts/images/langchain-deeplearning/simple-sequential-chain.png\" alt=\"simple sequential chain {400x120}\" title=\"Simple sequential chain\" /></p>\n<p><strong>SequentialChain</strong> -  this allows you to connect multiple <code>LLMChain</code> together and also allows you to connect them (input/output) as you wish.</p>\n<p><img src=\"/assets/posts/images/langchain-deeplearning/sequential-chain.png\" alt=\"sequential chain {475x355}\" title=\"sequential chain\" /></p>\n<h3 id=\"routerchains\"><strong>Router Chains</strong></h3>\n<p>Router chains allows you to map the user input to a specific chain.</p>\n<p><img src=\"/assets/posts/images/langchain-deeplearning/router.png\" alt=\"router chain {480x330}\" title=\"router chain\" /></p>\n<p><a href=\"https://github.com/prasann/langchain-deeplearning/blob/main/L3-chain.ipynb\">Link to the workbook</a></p>\n<h2 id=\"chapter4questionandanswer\">Chapter 4: Question and answer</h2>\n<p>In order to have a Q&amp;A on top of the user's documents. For this to happen, we need to build <a href=\"https://js.langchain.com/docs/modules/data_connection/text_embedding/\">custom embeddings</a> for the documents.</p>\n<p>Langchain provides interfaces to create and interact with the embeddings. My workbook will contain the Azure OpenAI version of these embeddings and the code uses <code>VectorstoreIndexCreator</code> to create the embeddings and <code>DocArrayInMemorySearch</code> to store the embeddings.</p>\n<p><img src=\"/assets/posts/images/langchain-deeplearning/embeddings.png\" alt=\"embeddings 101 {480x460}\" title=\"Embeddings\" /></p>\n<p><a href=\"https://github.com/prasann/langchain-deeplearning/blob/main/L4-questions-answers.ipynb\">Link to the workbook</a></p>\n<h2 id=\"chapter5evaluation\">Chapter 5: Evaluation</h2>\n<p>It is possible to perform an evaluation of the LLMs. The evaluation is done by comparing the output of the LLMs with the expected output. Langchain offers interfaces to create these evaluation and perform evaluations.</p>\n<ul>\n<li><p><strong>Generate examples for LLMs</strong> - This is like creating a test cases for the application under test. Langchain will be able to generate the sample queries and their expected results.</p></li>\n<li><p><strong>Manual evaluation (and debuging)</strong> -  With the sample queries generated, it is possible to manually evaluate the results and debug the LLMs.</p></li>\n<li><p><strong>LLM-assisted evaluation</strong> - Langchain offers <code>QAEvalChain</code>, which will allow you to evaluate the LLMs. It will take the sample queries and the expected results and build a graded results against the input data.</p></li>\n</ul>\n<p><a href=\"https://github.com/prasann/langchain-deeplearning/blob/main/L5-evaluation.ipynb\">Link to the workbook</a></p>\n<h2 id=\"chapter6agents\">Chapter 6: Agents</h2>\n<p>The course discuss only about <a href=\"https://python.langchain.com/docs/modules/agents/agent_types/react.html\">Zero-Shot ReAct</a> agent. Agent in general are responsible for deciding what step to take next. They can use <a href=\"https://python.langchain.com/docs/modules/agents/tools/\">tools</a> to interact with the world and <a href=\"https://python.langchain.com/docs/modules/agents/memory/\">memory</a> to store information.</p>\n<ul>\n<li><p><strong>Built-in toolkit</strong> - Langchain offers some built in tools, which can be loaded and the agent can use them to drive the decision making. Example: </p></li>\n<li><p><strong>Building custom tools</strong> - Most often, we will need to build tools as a wrapper to our data or APIs, so that the agent can use them. Here you can read about <a href=\"https://python.langchain.com/docs/modules/agents/tools/custom_tools\">building the custom tools</a>.</p></li>\n</ul>\n<p><a href=\"https://github.com/prasann/langchain-deeplearning/blob/main/L6-agents.ipynb\">Link to the workbook</a></p>",
            "url": "https://prasanna.dev/posts/langchain-llm-app-development",
            "title": "Langchain for LLM application development",
            "summary": "This is a personal notes from the course Langchain for LLM application development. This one of the short courses offered by the Deeplearning.ai. Used Azure OpenAI for the workbooks, instead of OpenAI.",
            "date_modified": "2023-08-13T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/docker-architecture-specific-images",
            "content_html": "<p><strong><em>\"Build once and run anywhere\"</em></strong> sounds like the perfect way to explain Docker to the newbies. But unfortunately this is not true always. Docker images are architecture specific. You can't run an image built for <code>amd64</code> architecture on a <code>arm64</code> machine. Let's understand the problem here,</p>\n<h2 id=\"whydoweneeddifferentimagesfordifferentarchitectures\">Why do we need different images for different architectures?</h2>\n<p>There are a few reasons why we need to have different Docker images for different architectures:</p>\n<ul>\n<li><strong>Differences in instruction sets:</strong> For example, an x86_64 machine has a different instruction set than an ARM machine. This means that the same Docker image cannot be run on both architectures without being compiled for each architecture.</li>\n<li><strong>Differences in hardware capabilities:</strong> For example, an ARM machine may have less RAM than an x86<em>64 machine. This means that a Docker image that is designed to run on an ARM machine may not be able to run on an x86</em>64 machine without being modified.</li>\n<li><strong>Differences in operating systems:</strong> For example, a Docker image that is designed to run on Linux cannot be run on Windows without being modified.</li>\n</ul>\n<p>By creating separate Docker images for different architectures and operating systems, we can ensure that our images are portable and that they can be run on a wide variety of platforms. This makes it easier for us to deploy our applications and to reach a wider audience.</p>\n<h2 id=\"isthisacommonpracticedoesallimagesarebuiltformultiplearchitectures\">Is this a common practice? Does all images are built for multiple architectures?</h2>\n<p>Generally, the web applications (that most of us work on) aren't built for multiple architectures. Because most of these are typically servers and they are built for <code>amd64</code> architecture because most of the servers are <code>amd64</code> architecture.</p>\n<p>But, if you are building an application for IoT devices, then you need to build for multiple architectures. Because IoT devices are built for different architectures. Example, Raspberry Pi is built for <code>arm64</code> architecture. So, if you are building an application for Raspberry Pi, then you need to build for <code>arm64</code> architecture.</p>\n<p>Applications like Redis, MySQL, nginx etc. are built for multiple architectures. Because these applications are used in servers and IoT devices. So, they need to be built for multiple architectures.</p>\n<h2 id=\"howtobuildadockerimageformultiplearchitectures\">How to build a Docker image for multiple architectures?</h2>\n<p>Docker has a feature called <a href=\"https://docs.docker.com/buildx/working-with-buildx/\">buildx</a> which allows you to build images for multiple architectures. You can use the <code>buildx</code> command to build images for multiple architectures.</p>\n<pre><code class=\"bash language-bash\">docker buildx build --platform linux/amd64,linux/arm64 -t &lt;image-name&gt; .\n</code></pre>\n<p>This command will build the image for both <code>amd64</code> and <code>arm64</code> architectures. You can also build for other architectures like <code>arm/v7</code>, <code>arm/v6</code>, <code>s390x</code>, <code>ppc64le</code> etc.</p>\n<h2 id=\"howtorunanarchitecturespecificdockerimage\">How to run an architecture specific docker image?</h2>\n<p>In general, docker pulls in the image based on the architecture. For example, if you are running on Mac M1/M2(apple processors) docker automatically pulls in <code>arm64</code> images. If you are running on a Linux machine, docker pulls in <code>amd64</code> images. However, if you want to run a specific architecture image, you can do so by specifying the platform.</p>\n<pre><code class=\"bash language-bash\">docker run --platform linux/arm64 &lt;image-name&gt;\n</code></pre>\n<p>or in the docker base image you can specify the platform.</p>\n<pre><code class=\"dockerfile language-dockerfile\">FROM --platform=linux/arm64 alpine\n</code></pre>\n<p><em>Note:</em> Likely the images won't be working as expected in a different architecture. So, it is not a good idea to run an image built for <code>amd64</code> architecture on a <code>arm64</code> machine.</p>",
            "url": "https://prasanna.dev/posts/docker-architecture-specific-images",
            "title": "Multi architecture builds of Docker images",
            "summary": "It is possible to build a docker image for multiple architectures using the same Dockerfile. In this article, will be explaining why you need to do this, how to do it and where to use it.",
            "date_modified": "2023-07-25T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/docker-raw-issue-on-mac",
            "content_html": "<p>When you run <a href=\"https://www.docker.com/products/docker-desktop/\">Docker desktop On Mac</a>, you will notice a single huge <code>docker.raw</code> file inside the data directory (<code>~/Library/Containers/com.docker.docker/Data/vms/0/data</code>) of Docker.</p>\n<p>The size of this file is defined in the user settings.</p>\n<p><code>Settings -&gt; Resources -&gt; Virtual disk limit</code></p>\n<p><strong>What is this docker.raw?</strong></p>\n<p>Recently, the <a href=\"https://docs.docker.com/desktop/faqs/macfaqs/#where-does-docker-desktop-store-linux-containers-and-images\">docker faq</a> section is updated with this explanation</p>\n<blockquote>\n  <p><em>Docker Desktop stores Linux containers and images in a single, large “disk image” file in the Mac filesystem.</em></p>\n</blockquote>\n<p><strong>What's strange about this?</strong></p>\n<p>When you do <code>ls -alh</code> inside this directory, you will see the size of the file to be this maximum\nsize that's defined in the docker desktop preferences. It will be a constant number, and doesn't depend on the number of local containers and images you have in there.</p>\n<pre><code class=\"shell language-shell\">32G&amp;nbsp; Docker.raw\n</code></pre>\n<p>Even a fresh installation of Docker without any images or containers will also have the same effect and running <code>docker system prune</code> will have no impact on the size you see here.</p>\n<p>However, if you check the disk usage,&nbsp; it will be much lesser.</p>\n<p><code>du -h .</code></p>\n<p>In my case, it was</p>\n<pre><code class=\"shell language-shell\">1.7G Docker.raw\n</code></pre>\n<p>This is the actual space used by the local images and containers. If this number is equal or almost close to the max size, you can run the <code>docker system prune</code> to reclaim the space.</p>\n<p><strong>What's the issue here?</strong></p>\n<p>Docker team claims that, this is perfectly normal and blame the tools that reports incorrectly in their docs.</p>\n<p>However, Mac will use the maximum allotted space for the disk space calculation and warns you that you are low on storage when you hit the limits.</p>\n<p>I'm not sure who is to blame here, docker team or the Mac Osx. Since it happens only with docker, I guess docker is doing something wrong here.</p>\n<p>Here is the whole <a href=\"https://github.com/docker/for-mac/issues/2297\">mega Github thread</a> on this issue</p>\n<p><strong>What can I do about this?</strong></p>\n<p>Docker FAQs suggest that you can reduce the size in the settings page or move this file to an external mounted storage.</p>\n<p>Keeping this number to a small means, there will be lesser containers cached in your local system.</p>",
            "url": "https://prasanna.dev/posts/docker-raw-issue-on-mac",
            "title": "Huge docker raw file on mac",
            "summary": "Docker desktop on Mac creates a huge file (`docker.raw`) in it's data folder. I spent some time in unravelling the mystery, so documenting for future me.",
            "date_modified": "2022-11-07T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/git-config-hacks",
            "content_html": "<p>There are zillions of <a href=\"https://git-scm.com/docs/git-config\">git-configs</a> present, and certainly, all of us have our favorites. Here are some of the configs that I use currently. I use GitHub (private repo) and manually synchronize these git-configs (and other configs) across devices.</p>\n<h3 id=\"folderlevelconfiguration\">Folder level configuration</h3>\n<p>This is one of the handy features that was introduced in git 2.13.\nI usually have a root level folder for <code>personal</code> and <code>work</code> separately. Within work, there could be multiple <code>clients</code> directory. Each of these directories will have its <code>.gitconfig</code>-{personal/work/client}` file within it. These directory level gitconfig can contain any valid git-config and will be used to override the global ones at the directory level.</p>\n<p>A sample from my work level git-config</p>\n<pre><code class=\"git language-git\">[user]\n    name = Prasanna\n    email = work-email\n    signingkey = gpg-key-for-work-email\n</code></pre>\n<p>For the overrides to work, I need to add the following</p>\n<pre><code class=\"git language-git\">[includeIf \"gitdir:~/projects/work/\"]\n    path = ~/projects/work/.gitconfig-work\n</code></pre>\n<p><strong>Pro tips in managing the folder level configs</strong></p>\n<ol>\n<li><strong>End <code>gitdir</code> with a <code>/</code>:</strong> The <code>gitdir</code> should end with a <code>/</code> otherwise the config won't be picked up inside the directory.</li>\n<li><strong>Config will work only within a git repo:</strong>\nIn my case <code>~/projects/work</code> isn't a git repo. so if i check for <code>git config user.email</code> it will be the one defined globally. However, i have git repos as subfolder <code>~/projects/work/proj-a</code> inside these git repos the email i configured in <code>.gitconfig-work</code> will take effect.</li>\n</ol>\n<h3 id=\"alias\">alias</h3>\n<p><code>empty</code> - shortcut to create git empty commits.\n<code>delete-local-merged</code> - I like to keep my local branches clean. this will delete the branches that are merged with <code>main</code></p>\n<pre><code class=\"git language-git\">[alias]\n    empty = \"git commit --allow-empty\"\n    delete-local-merged = \"!git fetch &amp;&amp; git branch --merged | egrep -v 'main' | xargs git branch -d\"\n</code></pre>\n<h3 id=\"editor\">editor</h3>\n<p>setup the git's default editor to <code>vscode</code>.</p>\n<pre><code class=\"git language-git\">[core]\n    editor = code --wait\n</code></pre>\n<h3 id=\"pushstyle\">push style</h3>\n<p><code>push.default</code> strategy to <code>current</code>. this push the current branch to update a branch with the same name on the receiving end.</p>\n<pre><code class=\"git language-git\">[push]\n    default = current\n</code></pre>\n<h3 id=\"prunetrue\">prune true</h3>\n<p>Git has a default disposition of keeping data unless it’s explicitly thrown away; this extends to holding onto local references to branches on remotes that have themselves deleted those branches.</p>\n<p>setting <code>fetch.prune</code> to true, it will remove the local branch if the remote is deleted.</p>\n<pre><code class=\"git language-git\">[fetch]\n    prune = true\n</code></pre>\n<h3 id=\"autocorrect\">autocorrect</h3>\n<p>autocorrects a single typo. ex: <code>stats</code> to <code>status</code></p>\n<pre><code class=\"git language-git\">[help]\n    autocorrect = 1\n</code></pre>\n<h3 id=\"signingcommits\">signing commits</h3>\n<p>I like to have the small green tick in github. We can get verify the commit using gpg keys. <a href=\"https://docs.github.com/en/authentication/managing-commit-signature-verification/about-commit-signature-verification\">Read more on github</a></p>\n<p>I maintain gpg keys separately for my work email and personal email. So the signing key gets overriden at directory level.</p>\n<pre><code class=\"git language-git\">[commit]\n    gpgsign = true\n</code></pre>\n<p>this will mandate the commits to be signed.</p>\n<pre><code class=\"git language-git\">[user]\n    name = Prasanna\n    email = mail.prasanna.v@gmail.com\n    signingkey = &lt;GPG Key Signature&gt;\n</code></pre>\n<p>These are on my list for a while now. And i keep experimenting with new configs as i read along.</p>",
            "url": "https://prasanna.dev/posts/git-config-hacks",
            "title": "My productivity hacks - gitconfig",
            "summary": "Summarizing some of the configuration that i like and follow in managing my git repositories.",
            "date_modified": "2022-08-09T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/reflections-on-building-a-POS",
            "content_html": "<h3 id=\"whybuildandnotbuy\">Why build and not buy?</h3>\n<p>There are big players like NCR and Diebold that manufacture POS hardware units and combine them with software. These are very useful to start with, mostly they are windows based and will have close integration with the hardware and peripherals. And also to top it, they have maintenance plan to take care of the hardware/software and peripherals.</p>\n<p>Now, it makes a lot of sense to buy them directly if you have to manage a small amount of POS (~100s)</p>\n<p>However, for a bigger organisations this might not be sustainable. Certainly cost will be a factor, but more than cost the interoperability with other system is a key thing. If you are an organisation that will be managing 1000s of POS, then certainly you have your own systems/platform to orchestrate your business. And also POS will be one of the main revenue channel for the business and hence it needs to be embedded with the other channel of delivery to offer a greater customer experience as-well make the system management lot easier.</p>\n<h3 id=\"drawingparallelswithposarchitecture\">Drawing parallels with POS architecture</h3>\n<p>Technical architecture for the point of sale systems will be a hybrid of mobile app architecture and of cloud services.</p>\n<p><strong>Mobile client vs POS</strong></p>\n<p>POS has an advantage over mobile architecture, in terms of storage and computation power. And also POS are usually single purpose machines unlike mobile phones.</p>\n<p>Where it varies is that is on the fact that they aren't personally owned/managed devices. A POS will have lots of end users using the system, and hence an increased sense of security is needed here and another desired aspect of POS is that, when stuck they needs to be restored (automatically) without a human intervention.</p>\n<p>One of the huge difference i felt is the need for our POS to support end users even when it's offline.</p>\n<p><strong>Cloud service vs POS</strong></p>\n<p>Very likely we will need to run a full blown server inside a POS system to get the required features running. It will have certainly lots of components, that need to co-ordinate with each other to make it possible. For ex, an automatic updater, hardware/peripheral integrators.</p>\n<p>Another significant challenge here is that you will have to run and manage N such servers (where N is the number of POS you support). this is certainly an operation nightmare with regards to monitoring.</p>\n<h3 id=\"keydecisionsthatwehadtotake\">Key decisions that we had to take</h3>\n<ul>\n<li><strong>How to deliver data</strong></li>\n<li>POS are built as offline first servers. It will need lots of amount of product/pricing and promotions data upfront. They can store them there will be enough storage, however to deliver them efficiently will be a key factor.</li>\n<li>Serve data that's needed for that POS and not the universal data</li>\n<li>Most of the e-commerce apps on mobile aren't offline first. We can demand the user to have good/decent internet. However in case of POS it's store network and the expectation we can't close the store just because it's not having a good network.</li>\n<li>Try to look out for \"Download once and share\" technique. Alternate could be torrent like mechanism (p2p downloads)</li>\n<li><strong>How to deliver software and upgrades</strong></li>\n<li>Decide on a mechanism to deliver the software, upgrades and the security patches.</li>\n<li>What's the difference? Need to make sure that the terminals are upgraded. Unlike mobile, these aren't personal devices and hence need to control them automatically.</li>\n<li>Delivery mechanism shouldn't physical (or manual). Remote upgrades only. convenient to apply patches</li>\n<li>again lookout for \"Download once and share\" techniques.</li>\n<li><strong>Securing server</strong></li>\n<li>You are exposing a server in front of the end user.</li>\n<li>These aren't personal devices and hence more concern about security.</li>\n<li>What if POS is shop lifted? can't compromise all the data inside it. can't allow it to transact.</li>\n<li><strong>Integration with hardware</strong></li>\n<li>huge concern, due to the availability of range of hardwares and</li>\n<li>options: build an unified hardware interface so you can mock it while testing the rest.</li>\n<li><strong>Monitoring Driven development</strong></li>\n<li>Monitoring is key, need to know how many systems are rolled out, and which versions they are running.</li>\n<li>Monitors for errors. Use metrics as smoke detector and logs on demand forwarding.</li>\n</ul>",
            "url": "https://prasanna.dev/posts/reflections-on-building-a-POS",
            "title": "Reflections on building an Enterprise POS",
            "summary": "I was part of a team that built and rolled out POS machines to around 1000 stores. Sharing some of my key take aways from that engagement.",
            "date_modified": "2022-06-02T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/strangler-fig-pattern-with-cdc",
            "content_html": "<h2 id=\"businesscontext\">Business Context</h2>\n<p>Client is an e-commerce platform shipping to 130+countries around the globe.\nTheir order fulfillment for all the countries was happening out of their local warehouse in western asia.\nSince most of their revenues come from the European Union (EU) region, they chose to set up a new warehouse within the EU.</p>\n<h2 id=\"problemsthatneedstobesolved\">Problems that needs to be solved</h2>\n<ul>\n<li>The new warehouse will be managed by a 3rd party logistics (3PL) company. The 3PL will likely have a view on stocks that are managed in their warehouse. They need a source of truth for their stock data across the warehouses.</li>\n<li>There is no dimension of warehouse in the current system. The stock entry is defaulted to their only warehouse. The current system also lacked the details for stock being non-sellable (like In Transit, Damaged, Missing etc).</li>\n<li>Current legacy system was hard to change. Hence it was decided to move the stock domain into a new system. It will be done gradually, meaning for a considerable time we will need both the systems to co-exist.</li>\n</ul>\n<h2 id=\"designdecision\">Design Decision</h2>\n<p>Focussing on the above-mentioned problems, we decided to go with a new Stock service that integrates with the 3PL company and becomes the source of truth for the stock data.</p>\n<p>But then the old system is hard to decommission in one go, so the decision was made to keep the Monolith along with the new service.\nThe current workflows involving their current warehouse will be done via the old Monolith and the newer warehouses will start using the new Stock service.</p>\n<blockquote>\n  <p>\"New stock service will co-exist with the monolith”</p>\n</blockquote>\n<h2 id=\"responsibilitiesofmonolith\">Responsibilities of Monolith</h2>\n<p>The monolith system with the Aurora database had the stock information for their current warehouse and whenever a buyer buys items from the current warehouse, it was handled by this system and stock count reflected in its database.</p>\n<h2 id=\"responsibilitiesofnewstockservice\">Responsibilities of New stock service</h2>\n<p>The new stock service will have couchbase as its database and become the source of truth with respect to all the stock data. The new service will integrate with the new third party logistics (3PL) to manage the stocks in the new warehouse.\nThe new service will have a more granular level of stock information of non-sellable stocks. Any business intelligence reporting tool related to stock will be connected only with the new stock service.</p>\n<h2 id=\"continuoussynchronizationofdataacrossdatabases\">Continuous synchronization of data across databases</h2>\n<p>This architecture opens up with a problem of continuous synchronization of data between the Aurora database and the Couchbase instance.\nSince the new service will be the source of truth, and the old one will be decommissioned eventually, it was decided that the synchronization will be one way between Aurora to Couchbase only.\nThis means, the old system will not have stock details for the new warehouse(s).</p>\n<p>In order to achieve the data synchronization we opted in for <em><a href=\"https://en.wikipedia.org/wiki/Change_data_capture\">Change data capture pattern</a></em>. We went with <a href=\"https://debezium.io/\">Debezium</a>, to implement the change data capture.</p>\n<h2 id=\"reasonstogowithdebezium\">Reasons to go with Debezium</h2>\n<ul>\n<li>It’s an open source platform.</li>\n<li>It can be easily used alongside Kafka. We had kafka already in our ecosystem. So the operational overhead was minimum</li>\n<li>There are a wide variety of connectors present for Debezium to connect to source and sink.</li>\n</ul>\n<p>We deployed Debezium as <a href=\"https://debezium.io/documentation/reference/stable/architecture.html\">Kafka connectors</a>. They generate Kafka events whenever the configured table(s) has any changes.\nThe events were in <a href=\"https://avro.apache.org/\">Apache Avro</a> format and were published to an underlying Kafka instance.\nWe will then have a sink connector which listens to configured events and then posts it back to Couchbase.</p>\n<p><img src=\"/assets/posts/images/strangler-with-cdc/img-stock-service.png\" alt=\"Stock service architecture {700xx602}\" title=\"Stock service architecture\" /></p>\n<p>Few more things that needed additional care for this change data capture setup.</p>\n<h2 id=\"transformingnormalizedtableintoadenormalizedstructure\">Transforming normalized table into a denormalized structure</h2>\n<p>One of the challenges that we need to address while setting up the CDC is the compatibility of source and destination data structures.\nAurora being a RDBMS database, the schemas are designed with normalization.\nIn couchbase we had these records in a flat structure. So, multiple records in MySQL will be mapped to a single record in the couchbase.</p>\n<p>Let me take an example here. Consider the following tables with the columns.</p>\n<p>For any change in the above tables, a CDC event will be created with a content of old data and the modified data.\nIn our couchbase, we had them as documents so we will need to perform a join operation with other tables to create a document.</p>\n<p>This join operation was the tricky part to solve. Initially our approach was to use a RDBMS data store, to store the denormalized structure.\nSo every CDC event will update this table and from there another CDC event will help us to update the couchbase instance.</p>\n<p><img src=\"/assets/posts/images/strangler-with-cdc/img-initial-setup.png\" alt=\"Initial setup with interim database {700xx350}\" title=\"Initial setup with interim database\" /></p>\n<p>This approach meant an overhead of maintaining an additional database, additional CDC setup.</p>\n<p>So, eventually we moved to Kafka streams to solve this problem of joins.\nWe did explore <a href=\"https://ksqldb.io/\">KsqlDB</a> that’s built on Kafka streams but then felt Kafka streams api was sufficient for us to get going.\nAnother alternative we started exploring and eventually dropped due to the learning curve was <a href=\"https://flink.apache.org/\">Apache Flink</a>.</p>\n<h2 id=\"scaleandmonitoringaspects\">Scale and monitoring aspects</h2>\n<p>We had ~1.5million records of product data, ~5.3 million records of product variants in the database that need to be synced.\nThere were a few more tables that we needed to consume data from. On an average we were having ~60-75 CDC events per second related to our tables coming into our queue.</p>\n<p>Monitoring CDC wasn’t very different from monitoring our kafka queues. For Kafka monitoring, we had <a href=\"https://grafana.com/grafana/dashboards/12460\">this dashboard</a> and extended it.\nAdditionally, to this we had a dashboard to monitor the health of the connectors.</p>\n<h2 id=\"firsttimeloadmigration\">First time load/migration</h2>\n<p>In our case we used the exact CDC setup for the initial load as well.\nDebezium sink connectors offered throttling ability, so the spiked load on our couchbase instances were avoided.\nThough it took time to clear all the messages in our kafka queue, it avoided an additional solution for the first time load.</p>\n<h2 id=\"whentogoforcdc\">When to go for CDC?</h2>\n<p>CDC is a useful technique when doing a microservices migration using a strangler fig pattern.\nIn cases where you need to maintain multiple databases with different structures.\nIn my opinion, Debezium seems to be the best open source solution available right now.</p>",
            "url": "https://prasanna.dev/posts/strangler-fig-pattern-with-cdc",
            "title": "Experiences with Strangler fig approach using Change Data Capture",
            "summary": "Summarizing my experiences in one of my re-platforming project. Keeping the monolith application in tact, we had to slice out a new service and keep the data in sync.",
            "date_modified": "2022-02-28T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/experience-with-aws-saa",
            "content_html": "<p>I recently passed the AWS SAA certification 🎉, penning my experience here.</p>\n<p>I have been working with AWS for the past ~3 years on and off based on the projects and client needs. One of the challenges for me is that I was always reactive. Usually the needs are EC2, ECS, S3, Cloudfront, Cloudwatch and RDS and that's it. I haven't tried lots of their services, and I wasn't even aware of the optimisations that can be done on both cost as well as in the architecture front.</p>\n<p>Decided to pick up this certification AWS SAA to proactively design and build secured, resilient and cost optimised architectures upfront and not to wait for the issues to pop up and then optimise. This is my first certification and certainly re-lived my college exam days while preparing for this exam.</p>\n<h2 id=\"preparation\">Preparation</h2>\n<p>Started this around ~3 months back, but was very much overwhelmed with the amount of materials (courses/ notes) that's on the internet. Eventually settled on the following things,</p>\n<ol>\n<li><a href=\"https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c02/\">Udemy course</a> by <strong><a href=\"https://www.udemy.com/user/stephane-maarek/\">Stephane Maarek</a> 🎥</strong>&nbsp;This is the longest course (~27hours) I have ever signed up for. This is an excellent start if you are looking to get this certification. This gives an intro and decent depth on all (almost) the AWS services. If you follow the hands-on approach along with Stephen, you will be equally confident in managing these services.</li>\n<li>This <a href=\"https://github.com/keenanromain/AWS-SAA-C02-Study-Guide\">Github notes</a> 📝 Though the videos are exhaustive, it's impossible to remember everything that's in there. That's where these Github notes helped me. Whenever I had some 15 mins, i goto a section in here and read through that.</li>\n<li><a href=\"https://apps.ankiweb.net/\">Anki Cards:</a> 📇&nbsp;Using flashcards to revise and memorise is my habit for a long time. Thanks to this <a href=\"https://www.notion.so/AWS-SAA-Certification-1c3d99ed38ab494ea8ea467cd27ca725\">reddit user</a>, there is an excellent summary of all the services. You can import them to the anki app and you are good to go. 20 cards a day and your certification isn't far away 😁</li>\n<li>Took the <a href=\"https://www.udemy.com/course/aws-certified-solutions-architect-associate-amazon-practice-exams-saa-c02/\">practice exams in Udemy</a>. ✍🏼&nbsp;This is an eye opener for me. I can understand and relate to most of the questions (thanks to the course and notes) but this is where we need to apply the knowledge we learnt and it wasn't easy. I decided to take all the tests in the course as open book types. After a couple of tests, I realised an open book takes a lot of time and decided to make some guesses and mark those guess questions for later review.</li>\n</ol>\n<h2 id=\"onthedayexperience\">On the day experience</h2>\n<ul>\n<li>I took the exam through Pearson Vue online. Make sure to keep the work desk (wherever the laptop/computer is placed) clean. They do check that 😅</li>\n<li>You can check in 30 mins before the scheduled time, and likely the exam will also start early.</li>\n<li>One day before the exam, go through the system check and keep things ready.</li>\n<li>You will have your video and mic turned on throughout the exam, you can't mute or turn off the video.</li>\n<li>My guess on the evaluation part. There are lots of questions where they offer partial marks. So the score isn't binary (right or wrong). so don't give up halfway, if you have done the preparations well you are likely to pass the exam.</li>\n</ul>\n<h2 id=\"sometipsifiwanttoredothis\">Some tips if i want to re-do this</h2>\n<ul>\n<li><strong>Book the exam dates upfront.</strong> There isn't enough motivation to run through the gazillion notes, so have a milestone. It isn't a big problem, since you can amend the dates twice.</li>\n<li><strong>Pair with a like minded person.</strong> Again not to drop the ball on the certification and have someone to talk to.</li>\n<li>You can't master concepts along with the details in one go. accept that. so <strong>don't spend too much time on one thing</strong>.</li>\n<li><strong>Spend more time on model exams/tests</strong>. From a time perspective, split it 50-50 roughly. Spend 50% time to go over the video courses, understanding the concepts and the remaining 50 to go over the model tests.</li>\n<li>For non native english speakers, there is a provision to <strong>get 30 mins extra time for the exam.</strong> <a href=\"https://aws.amazon.com/certification/policies/before-testing/\"></a> <a href=\"https://aws.amazon.com/certification/policies/before-testing/\">(ESL+30)</a>. It's good to take this time, i cut close to the finishing 140 mins.</li>\n</ul>\n<p>Happy preparing, and all the best !!</p>",
            "url": "https://prasanna.dev/posts/experience-with-aws-saa",
            "title": "What I did to become AWS certified 😎",
            "summary": "My experiences and learnings when i worked on to get my AWS certification. I recently passed the AWS SAA certification. Penned down my learnings and experiences along the way",
            "date_modified": "2021-12-03T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/microservices-pattern-sagas",
            "content_html": "<blockquote>\n  <p><em>Cross posted from <a href=\"https://dev.to/prasann/micro-services-patterns-saga-to-music-or-to-dance-4hio\">Dev.to blog</a></em></p>\n</blockquote>\n<h2 id=\"whatisasagapattern\">What is a Saga pattern</h2>\n<p>Saga design pattern is a way to manage a single business transaction that spans across various micro services. Saga pattern breaks a single business transaction into a sequence of local transactions that updates each service and publishes a message or event to trigger the next local transaction step. If any of these local transaction fails, saga will execute the subsequent flows to rollback and cleanup the transaction.</p>\n<h2 id=\"whydoweneedthis\">Why do we need this</h2>\n<p>In a micro-services architecture, it's hard to trace where the transaction has failed and what needs to be rolled back etc. Saga brings in a structure to deal with this problem and also allows the micro-services to act within the specified boundaries.</p>\n<h2 id=\"implementingsaga\">Implementing Saga</h2>\n<p>Saga is implemented by adding a <code>transactionId</code> to all the local transactions. A <code>transactionId</code> represent a single business transactions, so with that as an identifier it is possible to trace all the corresponding service interactions.</p>\n<p>This managing of local transactions can be done in 2 styles.</p>\n<ul>\n<li>Orchestration style</li>\n<li>Choreography style</li>\n</ul>\n<h2 id=\"orchestrationstylesagas\">Orchestration Style Sagas</h2>\n<p>Orchestration pattern mimics an Orchestra, where each person (system in this case) waits for the conductor (another system) to give instructions on what needs to be done.</p>\n<p><img src=\"/assets/posts/images/saga-patterns/orchestration.png\" alt=\"Orchestration style saga {800xx972}\" title=\"Orchestration style saga\" /></p>\n<p>I don't prefer this design usually for the following reasons,</p>\n<ul>\n<li>Tight coupling between the conductor and the other systems in the ecosystem.</li>\n<li>Conductor is a single point of failure.</li>\n</ul>\n<h2 id=\"choreographystylesagas\">Choreography Style Sagas</h2>\n<p>Choreography pattern mimics a dance performance where each dancer knows their role and can perform it independently. Hence, there is no need of centralised conductor role.</p>\n<p><img src=\"/assets/posts/images/saga-patterns/choreography.png\" alt=\"Choreography style saga {800xx972}\" title=\"Choreography style saga\" /></p>\n<p>Some benefits of this pattern are,</p>\n<ul>\n<li>Faster development. Teams can build independently, with defined contracts.</li>\n<li>Loose coupling, easier to change systems.</li>\n<li>Better fault tolerance. There is no single point of failure here</li>\n</ul>\n<h2 id=\"asamplescenario\">A Sample Scenario</h2>\n<p>Let us walk through a sample use case and see how we can go about solving it using choreography style sagas.</p>\n<p>Since we are talking about music and dance, let me take a use case of booking a movie ticket.</p>\n<ul>\n<li>A customer can purchase a movie ticket by paying online.</li>\n<li>Once the payment is successful, the movie ticket is confirmed to that customer.</li>\n<li>If there is a payment failure, no ticket will be issued, and the order stands cancelled.</li>\n<li>If the show is cancelled, the customer will be fully refunded.</li>\n<li>If the customer choses to cancel a ticket, then they will be refunded a partial amount only.</li>\n</ul>\n<h2 id=\"identifyingeventsandcommands\">Identifying Events and Commands</h2>\n<p>Now, to implement them independently by various services, we need to identify the boundaries of various services and also their resposibilities.</p>\n<p><a href=\"https://www.eventstorming.com/\">Event storming</a> is one activity that the team can do to identify these events aka boundaries of responsibilities.</p>\n<p><strong>Results of the event storming looks like this:</strong></p>\n<p>I will be using the following notion to illustrate various commands and events involved in the above use-case.</p>\n<p><img src=\"/assets/posts/images/saga-patterns/legend.png\" alt=\"legend {800xx272}\" title=\"legend\" /></p>\n<p><img src=\"/assets/posts/images/saga-patterns/booking-flow.png\" alt=\"booking-flow {800xx850}\" title=\"booking-flow\" /></p>\n<p><img src=\"/assets/posts/images/saga-patterns/cancel-flow.png\" alt=\"cancel-flow {800xx850}\" title=\"cancel-flow\" /></p>\n<p>Once these events and commands are identified, teams independently can go ahead and start implementing them. Rollbacks are just another events mapped to a different command.</p>\n<p>This gives a power to the team to act independently and reduces the bottleneck on a single service.</p>",
            "url": "https://prasanna.dev/posts/microservices-pattern-sagas",
            "title": "Micro-services Patterns. Saga - to music or to dance?",
            "summary": "Saga pattern is useful to trace a distributed transaction across various micro-services. This post summarizes the 2 patterns of saga and explains a event storming for a sample use-case",
            "date_modified": "2021-06-09T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/slicing-microservices",
            "content_html": "<blockquote>\n  <p><em>Cross posted from <a href=\"https://dev.to/prasann/slicing-microservices-1agj\">Dev.to blog</a></em></p>\n</blockquote>\n<p>Building applications using micro-services are becoming a default go-to architecture these days. I have been part of few teams that build and deploy micro-services in a large scale.</p>\n<blockquote>\n  <p>One pertinent question that often asked is \"<strong><em>Did we slice it right?</em></strong>\"</p>\n</blockquote>\n<h2 id=\"whatisslicingaservicemean\">What is slicing a service mean</h2>\n<p>Slicing a micro-service refers to defining the boundaries of the micro-service.</p>\n<ul>\n<li>What should the service be responsible for</li>\n<li>What kind of data should it hold</li>\n<li>When it should delegate it's responsibility to another micro-service.</li>\n</ul>\n<p>Below are some of my experiences, that i have seen working.</p>\n<h2 id=\"whentodoit\">When to do it</h2>\n<p>One of the common behaviour we did in the teams I worked so far, is to let micro-services <strong>evolve</strong> organically. We add feature/capabilities to the existing service and later trim down the service by spawning a new one.</p>\n<p><img src=\"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fz8fj6jt4y0z5rlp1irs.png\" alt=\"organic slicing of micro-service\" /></p>\n<p>Some of the benefits,</p>\n<ul>\n<li><strong>No need of upfront discussion</strong>. Most likely we will have lesser information about the feature, then likely that our design might reflect the incompetencies.</li>\n<li><strong>Spawning a new service will have it's own cost</strong>. How much ever you automate, it still adds up the infrastructure cost, maintenance cost. If we are wrong about the slicing then we had to spend some more time and effort to unify it with some other service.</li>\n<li>Once we have built a feature, most of the <strong>people in the team will understand the use-case and will appreciate the need of a separate service</strong>. It doesn't become a single person's decision or a group of architect's decision. But a decision that comes from ground up. There is a better chance for the service the retain it's shape when it grew this way.</li>\n</ul>\n<p>This approach does require a good discipline in having a constant check on the growth of a service. A highly coupled service is very hard to break down later. And if we are too late to cut down a service, it might become an expensive operation too.</p>\n<h2 id=\"howtodoit\">How to do it</h2>\n<p>Here are some of the themes i have come across. I will try to explain my thoughts using a bare minimum add-to-cart like domain problem.</p>\n<h3 id=\"entitybasedslicing\">Entity based slicing</h3>\n<p>Very common and obvious start for a new service.</p>\n<p>Example: <code>UserService</code> dealing with the CRUD of a <code>User</code> entity in the system.</p>\n<p>It's easy to conclude <strong>entities (domain models)</strong> as boundaries, since it's intuitive for people to see the separation. But whether it's right? is highly questionable and depends a lot, on the use case.</p>\n<blockquote>\n  <p>It's simple, easy and often end up in chaos</p>\n</blockquote>\n<p>One of the significant problems, i have seen is that these services will be very much in demand by other service. They entire network becomes very chatty. One can assume the <code>User</code> entity will be needed at each and every step of the application and will have lots of interaction. Worst, is when the services decided to retain their copy of the data to enhance the performance of the application.</p>\n<p>Orchestrator will become a monolith. Since there will be lots of entity services to do mundane operation, orchestrator will become the one service to hold the business logic.</p>\n<p><img src=\"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/si5fohmhiv4aqoa1h7xi.png\" alt=\"entity based slicing {336xx271}\" /></p>\n<h3 id=\"journeybasedslicing\">Journey based slicing</h3>\n<p>Slicing services based on <strong>user journeys and behaviour</strong>. This is quite popular, especially among the product people. Mostly, the product evolution happens on a feature/journey based. So whenever a new journey is identified it's time to build a new service.</p>\n<p>Ex: <code>RatingService</code> a service that allows you to rate an entity. It can be products, people or article etc. Behaviours can include to make sure you don't rate same article twice, compute average ratings etc.</p>\n<p>One of the advantage of this technique is that usually the teams i have worked in the past, they own the journey and hence it's clear for them what needs to be the part of this service</p>\n<p>Huge drawback I have seen with this approach, is that it forces the data being duplicated across services. In order to maintain the true flavour of Microservice, we will end up having independent databases and eventually having duplicate data</p>\n<h3 id=\"bestofbothworlds\">Best of both worlds</h3>\n<p>It makes a lot of sense to combine the above 2 approaches. Identify the core entities (domain models) of the system, and have them as either independent or logically grouped service. Apply journey based slicing on top of this entity services. So, the teams will own the journey service and the entity services can be maintained by group of teams.</p>\n<p><img src=\"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/degdriqsoudx0xhr4pgt.png\" alt=\"best of both worlds {336xx271}\" /></p>\n<p>Some of the conventions that i have seen/worked while slicing Micro-services</p>\n<p><strong>Entity services should be thin and lenient.</strong></p>\n<p>Entity services should be merely act as APIs to the database operations. Do not try bringing in business logic here.</p>\n<p>For example, making the email-address non updatable in the entity service level. It makes sense to not to allow the end-user to allow update the email addresses, but often that might be a need from a back office personnel (admin user). So restricting such operation in the entity level might not be worth it. <strong>Journeys should take care of validations.</strong></p>\n<blockquote>\n  <p>It's hard to predict the future requirements so keep EntityServices simple and open for extension.</p>\n</blockquote>\n<p><strong>Avoid journey service calling other journey services</strong></p>\n<p>Journey services, should be independent of others. Store data that are necessary for the journey and use entity services to collaborate with common data.</p>\n<p><strong>Build composite entities wherever needed</strong></p>\n<ul>\n<li>Now, to answer the immediate question that will raise due to the above constraint. How to manage the duplicate business logic. To be honest, <strong>DRY principle is overrated in my opinion.</strong> But in case if you are looking for such thing, then try adding another layer of composite entities.</li>\n<li>These composite entities, will encapsulate multiple entities and some business logic around this. One classic example from the app we built is a tax computation service. It involves the product, and the location of the buyer to calculate the tax.</li>\n</ul>\n<p>All these things are from my past experience, I'm sure I'm going to learn more and course correct myself in this journey. But one thing that i feel will always help in evolving micro-services is to constantly question the slicing decision to get it at a right state. And also a good knowledge on the <a href=\"https://martinfowler.com/bliki/DomainDrivenDesign.html\">Domain driven design</a> helps a lot to take these decisions.</p>",
            "url": "https://prasanna.dev/posts/slicing-microservices",
            "title": "Slicing Microservices",
            "summary": "Some ideas to go about in designing micro-services. How to slice them and some general conventions",
            "date_modified": "2021-04-01T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/scaling-applications-using-microfrontends",
            "content_html": "<blockquote>\n  <p><em>Cross posted from <a href=\"https://archimydes.dev/fourthact/blog/scaling-applications-using-micro-frontends\">Archimydes blog</a></em></p>\n</blockquote>\n<p>This blog post is a summary of a presentation that I made at the Archimydes Mastermind Event that happened on 17th Dec 2020.</p>\n<p>Modern web applications tend to have complex and feature-heavy Frontends when compared to backends.</p>\n<p>With so many choices for frameworks and programming paradigms, building a consistent Frontend to scale is a challenging problem to solve. We cover ways in which you can scale your Frontend application and your development teams by using a Micro-Frontends design pattern.</p>\n<p>I'll start by introducing the pattern of Micro-frontends first. Then we'll be looking into some of the key decisions that need to be taken while starting a Micro-frontend project. Finally, we will see the circumstances where this pattern will be effective.</p>\n<h2 id=\"1scalingapplications\"><strong>1. Scaling Applications</strong></h2>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/Twitter_post_-_1.png\" alt=\"Scaling applications\" title=\"Scaling applications\" /></p>\n<p>In general, scaling applications implies scaling backend applications to serve an increasing number of users. Usually, it's about how to:</p>\n<ul>\n<li>Increase Performance</li>\n<li>Reduce Latency</li>\n<li>Sustain load</li>\n<li>Manage compute costs</li>\n</ul>\n<p>All these parameters are typically applicable for the backend applications.</p>\n<p>For frontend applications, we typically stop with a good CDN to deliver static assets efficiently. However,</p>\n<blockquote>\n  <p>Scaling frontend apps is also about&nbsp;scaling development teams, both by size (growing a size of a single team) or count (multiple teams)</p>\n</blockquote>\n<p>Additionally, applications are getting more frontend heavy because:</p>\n<ul>\n<li>backends are getting easier to deploy and get off the ground</li>\n<li>end-user compute is getting cheaper and more powerful everyday</li>\n<li>more functionality is being pushed to end-user interfaces and devices</li>\n</ul>\n<p>As a result of this, product teams need to figure out an efficient way to build and deliver frontend applications with multiple development teams working at scale. Product teams need to execute this while reducing bottlenecks in the development process.</p>\n<h2 id=\"2monolithsmicroservicesandmicrofrontends\"><strong>2. Monoliths, Microservices and Micro-frontends</strong></h2>\n<blockquote>\n  <p>Monoliths are not a bad design choice</p>\n</blockquote>\n<p>It's always best to start any application as a monolith. Upfront slicing of module boundaries is very hard and tends to go wrong. As the application grows, it's better to identify module boundaries and split them up.</p>\n<p><strong>Microservices</strong></p>\n<p>From monoliths, the best choice to evolve the backend services as microservices. We can then guarantee:</p>\n<ul>\n<li>Strong module boundaries</li>\n<li>Independent deployment</li>\n<li>Polyglot development and tech diversity</li>\n</ul>\n<p>However, most of the microservices I have seen are as follows</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/Twitter_post_-_2.png\" alt=\"Microservices\" title=\"Microservices\" /></p>\n<blockquote>\n  <p>Independent deployments ! == Independent releases</p>\n</blockquote>\n<p>Teams are able to develop and deploy backends independently. However, they need to wait for the frontend to be developed and deployed.</p>\n<p><strong>Enter Micro-frontends</strong></p>\n<p>Micro-frontends are nothing but taking the concept of micro-services to the frontend. Slice the frontend of the application to respect the module boundaries of the backend, and create an end-end independent release path.</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/Twitter_post_-_3.png\" alt=\"Microfrontends\" title=\"Microfrontends\" /></p>\n<blockquote>\n  <p>All of Microservices' promises + Independent releases</p>\n</blockquote>\n<h2 id=\"gainswithmicrofrontends\"><strong>Gains with Micro-frontends</strong></h2>\n<ul>\n<li>Independent teams</li>\n<li>Independent releases</li>\n<li>Simple, decoupled codebases</li>\n<li>Incremental upgrades</li>\n</ul>\n<h3 id=\"problemsthatneedsolving\"><strong>Problems that need solving</strong></h3>\n<ul>\n<li>T<strong><em>o 'share, or not to share'?</em></strong>&nbsp;- Code reusability is one of the most overrated principles in software development. The problems of reusability are often ignored or not shared. In going the micro-frontend way, this needs to be discussed among the teams. Out of the gate, a duplicate first strategy works since it allows teams to execute faster initially.</li>\n<li><strong>Application loading performance</strong>&nbsp;- Micro-frontends can cause an impact on the loading performance of the application. There are ways to mitigate it, but the effort it takes has to be taken into consideration.</li>\n<li><strong>Design consistency across the application -</strong> Having a larger number of people working on an application will lead to inconsistencies. Again, there are ways to mitigate this, however, the effort involved in mitigation needs to be considered.</li>\n</ul>\n<h2 id=\"3keydecisionswhiledoingmicrofrontends\"><strong>3. Key decisions while doing Micro-frontends</strong></h2>\n<p>Let's go over some of the major decisions that we need to take during the early stages of a micro-frontend application. I will try to cover the solution(s) that we took while building an application with distributed teams across 3 regions for 2 years. The decisions can vary based on the project context but nevertheless these problems need to be solved.</p>\n<p>In order to explain the challenges and decision, I will take up the following use-case:</p>\n<p><strong>Building an application to allow user to configure and buy a laptop. Similar to that of Apple's.</strong></p>\n<p>A user can <strong><em>configure</em></strong> a laptop with various components, accessories, protection plans, etc. The user should be able to <strong><em>search</em></strong> for accessories, or maybe built-in models, and then finally should be able to <strong><em>order</em></strong> the product and get it fulfilled.</p>\n<p>Apart from the 3 services - configure, search, and order, I will have another service called \"Frame\" merely to hold the application together.</p>\n<ul>\n<li><strong>Frame</strong>: A business logic agnostic orchestrator service that knows how to download the rest of the services' frontend</li>\n</ul>\n<p><strong>A) Composing multiple front-ends into a single application</strong></p>\n<blockquote>\n  <p>End users don't care about the tech used. Their experience should not be affected due to tech.</p>\n</blockquote>\n<p>Composing multiple frontends into a single application is one of the first problems that needs solving when choosing micro-frontends.</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/Twitter_post_-_4.png\" alt=\"Composing frontends into single app\" title=\"Composing frontends into single app\" />\n<strong>Composing front-ends</strong></p>\n<p>We can achieve this composition in 2 ways, let's go over the pros and cons of these approaches.</p>\n<h2 id=\"buildtimecompositionvsruntimecomposition\"><strong>Build-time Composition vs Run-time Composition</strong></h2>\n<p><strong>Build-time composition</strong> is where multiple frontend applications are built as a single big application and served. This can be accomplished using&nbsp;<strong>npm</strong>&nbsp;or&nbsp;<strong>yarn</strong>&nbsp;packages.</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/build-time-composition.png\" alt=\"Build time composition {800xx235}\" title=\"Build time composition\" /></p>\n<p><strong>Pros:</strong></p>\n<ul>\n<li>Good dependency management, resulting in smaller bundles</li>\n<li>Independent cross team development</li>\n</ul>\n<p><strong>Cons:</strong></p>\n<ul>\n<li>A monolith built by different teams</li>\n<li>Non atomic deployments</li>\n</ul>\n<p><strong>A Run-time composition</strong> is where the frontends get integrated into the browser directly when the user requests a page. This may be done on the \"Server-Side\" or in the \"Client-Side\"</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/run_time_frontend_composition_f5076854e1.png\" alt=\"Run-time composition {800xx373}\" title=\"Run-time composition\" /></p>\n<p><strong>Pros:</strong></p>\n<ul>\n<li>Independent teams with independent deployments</li>\n<li>Atomic deployments, so no versioning issues</li>\n</ul>\n<p><strong>Cons:</strong></p>\n<ul>\n<li>Too many API requests from Client(?), with increased bundle size</li>\n</ul>\n<p><strong>Toolkit options for Runtime composition</strong></p>\n<p><strong>Server side:</strong></p>\n<ul>\n<li>SSI (Server Side Includes)</li>\n<li>Tailor (from Zalando)</li>\n</ul>\n<p><strong>Client Side:</strong></p>\n<ul>\n<li>JSPM</li>\n<li>SystemJS</li>\n<li>FrintJS</li>\n<li>Single-Spa</li>\n</ul>\n<p><strong><em>We chose Run-time composition for the project we worked on. Since our app was rendered on the client-side, it was simpler for us to achieve this.</em></strong></p>\n<h2 id=\"bcommunicationbetweenthefrontends\"><strong>B) Communication between the frontends</strong></h2>\n<p>Multiple frontends need to share data with each other. Though this needs to be minimal, it's unavoidable. A couple of options to achieve this is by:</p>\n<ul>\n<li><strong>State management tools</strong></li>\n</ul>\n<p>A global store in the application and all frontends using the same library to access the store.</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/state_management_tools_604f976fa9.png\" alt=\"State management tools {800xx153}\" title=\"State management tools\" /></p>\n<ul>\n<li><strong>Window events</strong></li>\n</ul>\n<p>Another approach could be to utilize the window (DOMs) eventing capability. Below is a sample event.</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/window_events_46783b22ad.png\" alt=\"Window events {800xx250}\" title=\"Window events\" /></p>\n<p><strong><em>We used to communicate through common redux store and redux events as all the apps in our micro-frontends were using Redux.</em></strong></p>\n<h2 id=\"cdesignconsistency\"><strong>C) Design Consistency</strong></h2>\n<p>One of the hardest problem to solve for is design consistency.</p>\n<p>In our team, we addressed this challenge by forming guilds. Consider that there are three teams, and each team has a designer assigned to it.</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/Twitter_post_-_5.png\" alt=\"Actual team structure\" title=\"Actual team structure\" /></p>\n<p>We formed a guild comprising of all designers and some interested developers. They encompass a virtual team. They take all the design decisions and make sure their respective teams are abiding by the central design tenets.</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/Twitter_post_-_6.png\" alt=\"Guild1\" title=\"Guild1\" /></p>\n<p>Initially, the guild created a style guide for the application. Mainly CSS and the application teams copy-pasted it from the style guide to build components.</p>\n<p>As we developed more features, we started pulling out Higher-order JS components and made them sharable. This is more of an evolution and works well once you have a stable design system in place.</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/Twitter_post_-_7.png\" alt=\"Styleguide {800xx400}\" title=\"Styleguide\" /></p>\n<p>And also, since the teams were using the same frontend framework (React) it was easier for us to build this component library.</p>\n<h2 id=\"dtestingstrategy\"><strong>D) Testing Strategy</strong></h2>\n<p>Deciding on \"How to test\" is important. Since it's a relatively newer paradigm and there are lots of moving parts in the application.</p>\n<p>Primarily we will be discussing the \"Integration tests\" and \"Functional tests\" from the testing strategy, as there won't be much difference in the way the \"Unit tests\" are done.</p>\n<ul>\n<li><strong>Integration tests</strong></li>\n</ul>\n<p>Having a lightweight \"Consumer Driven Contracts\" (CDC) helped us a lot.</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/testing_strategy_1776956c37.png\" alt=\"Integration tests\" title=\"Integration tests\" /></p>\n<p>A CDC is where the consumer services' give some tests to the provider service. A provider has to run all of its consumer services before publishing an artifact for deployment.</p>\n<p>This doesn't need to be very complex and can be done quickly using some lightweight options without using any big frameworks. But then, it's all case by case.</p>\n<p>In our scenario, Frame was the consumer of all the services and it shared a simple JSON contract and a small JS test with all of its providers. This ensured that the frame wasn't broken when a service deployed automatically.</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/sample_of_frames_contract_025a143c30.png\" alt=\"Frame test {800xx333}\" title=\"Frame test\" /></p>\n<ul>\n<li><strong>Functional tests</strong></li>\n</ul>\n<p>This is one of my least favorite testing methods, however, like everything else in tech, it does have some staunch supporters and followers. In our case, we only had a few critical and successful user journeys automated using Selenium for functional testing.</p>\n<p><img src=\"/assets/posts/images/scaling-microfrontends/functional_tests_6cdcf7c24a.png\" alt=\"Functional tests {800xx400}\" title=\"Functional tests\" /></p>\n<p>These journeys cut across multiple services and hence are harder to develop and maintain. Some of the FAQs I usually get on these tests are</p>\n<h2 id=\"faqs\"><strong>FAQs</strong></h2>\n<ul>\n<li><strong>Who owns functional tests?</strong></li>\n</ul>\n<p>Ans. The product team and business analysts. They define the scenarios for automation.</p>\n<ul>\n<li><strong>Who writes functional tests?</strong></li>\n</ul>\n<p>Ans. Guild containing QAs from all teams and a few developers.</p>\n<ul>\n<li><strong>Who fixes functional tests?</strong></li>\n</ul>\n<p>Ans. Team which breaks it.</p>\n<h2 id=\"whenshouldyouoptformicrofrontends\"><strong>When should you opt for Micro-frontends?</strong></h2>\n<p>Micro frontends are not for everyone. It adds significant overhead with development and maintenance.</p>\n<ul>\n<li><strong>A. Distributed self-contained teams, with a need for parallelization</strong></li>\n</ul>\n<p>If your development teams aren't co-located, and there is a decent amount of parallelization that needs to be done, this could be a reason to implement micro-frontends.</p>\n<ul>\n<li><strong>B. Collaborate with different frameworks in the frontend</strong></li>\n</ul>\n<p>Imagine you are inheriting a legacy application but want to build a new feature with modern design elements, then micro-frontends gives you a good head start.</p>\n<ul>\n<li><strong>C. Teams that have experience building Microservices application, and are willing to take it to the next step</strong></li>\n</ul>\n<p>Most of the points mentioned here are forward-thinking practices. Micro-frontends needs a good solid understanding of the domain and good discipline to contain things within one's boundary.</p>\n<p>Finally, it's worth remembering that:</p>\n<blockquote>\n  <p>It's not a sprint. It's a marathon.</p>\n</blockquote>\n<p>Micro-frontends adds significant overhead to the overall application. This isn't desired for smaller applications or for the application that will be built and managed by a single team. The above mentioned challenges are worth solving, only if you are up for a longer run with multiple teams.</p>",
            "url": "https://prasanna.dev/posts/scaling-applications-using-microfrontends",
            "title": "Scaling Applications Using Micro-Frontends",
            "summary": "When starting a project with Micro-Frontends, here are some typical problem that require solving and some possible solutions.",
            "date_modified": "2021-01-21T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/ace-with-react",
            "content_html": "<h2 id=\"atlassianconnectexpressace\">Atlassian Connect Express (ACE)</h2>\n<p>ACE is a toolkit in node.js to build atlassian connect apps. Like a JIRA, Confluence plugins.</p>\n<p>Best way to start with ACE is to bootstrap from <code>atlas-connect</code> Y<a href=\"https://bitbucket.org/atlassian/atlassian-connect-express/src/master/\">ou can follow the documentation here to get a sample app up and running in few minutes.</a></p>\n<h2 id=\"aceandhandlebars\">ACE and handlebars</h2>\n<p>ACE is a wrapper on top of the <a href=\"https://expressjs.com/\">expressjs</a> and hence the application is pretty much an express application. The bootstrapped application will have both the frontend and the backend part. For the frontend, ACE by default uses <a href=\"https://handlebarsjs.com/\">handlebars</a> as templating option.</p>\n<p>When you make a API call, the server converts the handlebar templates into HTML and returns.</p>\n<h2 id=\"replacehandlebarsviewswithaspa\">Replace Handlebars views with a SPA</h2>\n<p>As you can see, the above mechanism isn't tailored to use any single page application. Luckily, ACE doesn't change lots of things from express and hence we can use the same technique to achieve the goal.</p>\n<p><strong><em>In this example I'm choosing <a href=\"https://reactjs.org/\">React</a>, however the technique is fairly the same for any SPA framework.</em></strong> Since the idea is fairly simple. Using the framework build the app and deliver it to the browser on the init api call. From then on, the app will be controlled by the browser.</p>\n<h2 id=\"upandrunningwithreact\">Up and running with React.</h2>\n<p><strong>1. Add a route to serve static files.</strong></p>\n<p>To the already bootstrapped ACE project, i added a folder called client, and inside that I used <a href=\"https://github.com/facebook/create-react-app\">CRA</a> to bootstrap my React project. This will be my SPA. Make sure to keep the built artifact within this folder. In my case it was <code>{ProjectRoot}/client/build/*</code></p>\n<p>Now in the express application's index.js need to modify the route to serve this page. In my case, i used <a href=\"https://developer.atlassian.com/cloud/jira/software/modules/page/\">generalPages module</a> of ACE. In <code>atlassian-connect.json</code> I have mentioned <code>url</code> to be <code>/init</code></p>\n<p>Here is the modified <code>/init</code> to serve the react app.</p>\n<pre><code class=\"jsx language-jsx\">app.get(\"/init\", addon.authenticate(), (req, res) =&gt; {\n  res.sendFile(path.join(__dirname, \"/../client/build/\", \"index.html\"));\n});\n</code></pre>\n<p>you can import the path like this <code>import path from 'path';</code></p>\n<p>2<strong>. Configure static directories for express server.</strong></p>\n<p>Next is to set the path variable for the express. You need to define the static directory for the express server to fetch the files from.</p>\n<p>in <code>app.js</code> you can configure this.</p>\n<pre><code class=\"jsx language-jsx\">const app = express();\nconst addon = ace(app);\n\n/* more config */\n\nconst staticDir = path.join(__dirname, \"public\");\n//*** This line is important **********//\n\napp.use(express.static(path.join(__dirname, \"client\", \"build\")));\n\n//*** This line is important **********//\napp.use(express.static(staticDir));\n</code></pre>\n<p>Now, we have configured express to look into 2 different directories for static files.</p>\n<p><strong>3. Include atlassian JS API as part of the SPA</strong></p>\n<p>Lastly, we need to make a change in the client app. Jira/Confluence when they load their plugin, they expect the plugin to have <code>all.js</code> . This is the client side logic of ace. So, we need to include this as part of our React's <code>index.html</code> without this, <strong><em>the loader in Jira will never disappear</em></strong></p>\n<p>In <code>client/public/index.html</code> add the script tag just below the body.</p>\n<pre><code class=\"jsx language-jsx\">&lt;script\n  src=\"https://connect-cdn.atl-paas.net/all.js\"\n  type=\"text/javascript\"\n  data-options=\"sizeToParent:true;resize:false\"\n&gt;&lt;/script&gt;\n</code></pre>\n<p>You can read about more <code>data-options</code> <a href=\"https://developer.atlassian.com/cloud/jira/software/about-the-javascript-api/\">here</a>.</p>\n<p>That's it, you will now see the React application in the connect app.</p>\n<p><strong>*Note:</strong> In development mode, there isn't any hot reload in this case. If you make any changes to the react app, need to build the app manually. Ofcourse, you can modify the <code>package.json</code> to automate this, but <code>webpack-dev-server</code> isn't much helpful.*</p>\n<h2 id=\"authenticationwithintheconnectapp\">Authentication within the connect app</h2>\n<p>If you are planning to bundle few APIs along with the react app, then one of the harder thing to crack is authentication. This isn't documented very clearly anywhere.</p>\n<p>ACE uses a JWT to authenticate api's and unfortunately the token isn't accessible for React application since it is running inside a iframe by default. If you are using the default Handlebars, ACE provides helper methods to access the JWT.</p>\n<h3 id=\"workaround\">Workaround</h3>\n<p>The first call from JIRA/Confluence call will carry the JWT. In the above code snippet <code>addon.authenticate()</code> will validate the JWT executes the callback. In this place, we can create a JWT and set it as cookie header. Post that, in all the API calls made from React app, we can validate the JWT against our secret, and it will sort out the issue of authentication.</p>\n<p>Here is a sample code for a Hello world with ACE and React. <a href=\"https://github.com/prasann/ace-with-react\">Github Source.</a></p>",
            "url": "https://prasanna.dev/posts/ace-with-react",
            "title": "ACE with React",
            "summary": "Atlassian connect express comes by default with handlebars, this post describes how to make it work with a SPA.",
            "date_modified": "2020-07-31T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/centralized-error-handling-express",
            "content_html": "<p><em><strong>Note: All my examples are in typescript and there are million other ways to achieve similar result, this is just my way of doing things.</strong></em></p>\n<p>If you are looking to start a new application in express, go over to the express site, use <a href=\"https://expressjs.com/en/starter/generator.html\">express-generator</a> to create an application.</p>\n<h2 id=\"errorhandling\">Error handling</h2>\n<p>By default, any errors that are thrown within your application, will be sent as a 500 response code, along with the error stack trace in the body. This was inconvenient for me since,</p>\n<ul>\n<li>I don't want the end-user to know the system stack trace.</li>\n<li>I want to use exceptions for errors like <code>BadRequest</code> <code>AuthExceptions</code> etc.</li>\n</ul>\n<p>So, i decided to tweak the default behaviour and take control of error handling. If your use case is similar, then proceed with further steps.</p>\n<h3 id=\"customerrorhandler\">Custom error handler</h3>\n<p>Create your own custom ErrorHandler class (<code>error_handler.ts</code>), this will extend the node's <a href=\"https://nodejs.org/api/errors.html#errors_class_error\">Error</a> class.</p>\n<pre><code class=\"typescript language-typescript\">//error_handler.ts\nexport class AppError extends Error {\n  statusCode: number;\n  message: string;\n\n  constructor(statusCode, message) {\n    super(message);\n    this.statusCode = statusCode;\n    this.message = message;\n  }\n}\n</code></pre>\n<p>Now in your application you can invoke this custom error handler by calling,</p>\n<pre><code class=\"typescript language-typescript\">new AppError(404, \"Unable to find the resource\");\n//or\nnew AppError(403, \"You are not authorized to perform this action\");\n</code></pre>\n<h3 id=\"wiringerrorinterceptorintoexpressapplication\">Wiring error interceptor into express application</h3>\n<p>Once you start throwing exceptions within your application, next step is to convert those errors into a meaningful response for the end-user. Express app provides a way to hook up a custom error\nhandler into your application. A middleware that takes in 4 parameters is your way to add your custom error handler.</p>\n<p>Let's add the custom error handler function in the same <code>error_handler.ts</code> class and export. This generic function will parse the thrown error and constructs appropriate response.</p>\n<pre><code class=\"typescript language-typescript\">//error_handler.ts\nexport const customErrorHandler = (err, res) =&gt; {\n  const { statusCode, message } = err;\n  res.status(statusCode).json({ error: { message } });\n};\n</code></pre>\n<pre><code class=\"typescript language-typescript\">import express from \"express\";\nimport customErrorHandler from \"error_handler\";\nconst app = express();\n\n// Other middlewares, routes...\n\n// Adding your custom error handler.\napp.use((err, req, res, next) =&gt; {\n  customErrorHandler(err, res);\n});\n</code></pre>\n<p>Now, whenever any error that is thrown in the application will be caught by this error handler. This will in turn respond back with appropriate status code.</p>\n<h3 id=\"dealingwithunknownerrors\">Dealing with unknown errors</h3>\n<p>As you can see, the <code>customErrorHandler</code> has a limitation of handling only the errors that are of type <code>AppError</code> since it expects <code>statusCode</code> to be present in the error.\nHowever, there will be <code>RuntimeExceptions</code> that will occur in the application. It's kind of hard to catch all these sort of errors in the application and re-throw them as custom errors.</p>\n<p>So, we will improve our <code>customErrorHandler</code> to handle such <code>RuntimeExceptions</code>.</p>\n<pre><code class=\"typescript language-typescript\">//error_handler.ts\nconst handleKnownExceptions = (err, res) =&gt; {\n    //log it\n    const { statusCode, message } = err;\n    res.status(statusCode).json({error: {message});\n};\n\nconst handleUnknownExceptions = (err, res) =&gt; {\n    //log it\n    res.status(500).json({ error: {message: 'Something went wrong.' }});\n};\n\nexport const customErrorHandler = (err, res) =&gt; {\n    err instanceof AppError ? handleKnownExceptions(err, res) : handleUnknownExceptions(err, res);\n};\n</code></pre>\n<p>Now, we introduced one more way of handling errors. If the caught error is not that of ours (<code>AppError</code>) then we respond back with a 500 response.\nWe don't want our end user to know about the system internals and hence respond with a static message.</p>\n<h3 id=\"dealingwithasynchronousroutes\">Dealing with asynchronous routes</h3>\n<p>This centralized error handling will not work for the errors that are thrown in the <code>await</code> methods i.e, any error that are thrown in an async block will not reach our <code>customErrorHandler</code>.\nThis is a limitation with respect to express 4.x.</p>\n<p>As a workaround, you have to make the routes to be synchronous. Instead of changing all the routes to synchronous blocks i used this\n<a href=\"https://github.com/Abazhenov/express-async-handler\">middleware</a> to achieve a similar effect. Post wrapping my routes with this middleware, all the errors in async block will then reach our <code>customErrorHandler</code></p>\n<p>Here is the <a href=\"https://gist.github.com/prasann/b6ad07b3962b6ea2953fef027df5d10b\">gist</a> to the final <code>error_handler.ts</code></p>",
            "url": "https://prasanna.dev/posts/centralized-error-handling-express",
            "title": "Centralized error handling in Express applications.",
            "summary": "Handling exceptions in an express application, responding back with standard error response.",
            "date_modified": "2020-04-09T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/logging-in-golang-projects",
            "content_html": "<p>One of the common requirement in any project is to have some additional context while logging. And most of us aren't consuming the logs directly these days.\nWe use either ELK stack or some other proprietary tools to consume the logged information.\nIn these cases, it's important to know where those logs specifically came from and also important to log it in a format that's easy to parse and index.</p>\n<p>In our case, we were using splunk and we have built a lots of dashboards based on Splunk logs. So our convention is to log in JSON format and also to log machine information and some more environment based information.</p>\n<p>Here is the post describing how we achieved it using go-lang.</p>\n<h2 id=\"settingupanabstractionforlogging\">Setting up an abstraction for logging</h2>\n<p>We decided to go with logrus as our logging library. Instead of using and importing logrus in all the places, we wrote a layer of abstraction.</p>\n<p>This layer, then exposes various public functions to be consumed by the actual callers. So the logrus usage, is hidden and can be later changed too</p>\n<p>In this layer, we can then inject common variables that needs to be logged as part of all log statements.</p>\n<h2 id=\"loggingthecaller\">Logging the caller</h2>\n<p>The moment we introduce the abstraction, we have introduced a problem of losing the actual log position. logrus will log the abstraction layer as the log position for all log statement.</p>\n<p>So, here we are logging the caller as \"ContextLogTag\". The caller will be then identified using the go runtime. We can navigate through the stack in the go runtime to log the caller.</p>\n<p>Here is the code for does that</p>\n<pre><code class=\"go language-go\">func getCallerInfo() string {\n    _, filePath, lineNo, isOk := runtime.Caller(2)\n    if isOk {\n        pathArray := strings.Split(filePath, \"/\")\n        fileName := pathArray[len(pathArray)-1]\n        return fmt.Sprintf(\"%s#%d\", fileName, lineNo)\n    } else {\n        return \"\"\n    }\n}\n</code></pre>\n<p>Here is our abstraction layer.</p>\n<pre><code class=\"go language-go\">package logger\n\nimport (\n    \"fmt\"\n    \"github.com/sirupsen/logrus\"\n    \"log\"\n    \"os\"\n    \"runtime\"\n    \"strings\"\n)\n\nvar logger *logrus.Logger\n\ntype Fields map[string]interface{}\n\nconst (\n    contextLogTag     string = \"ContextLogTag\"\n    errorLogTag       string = \"ErrorLogTag\"\n    deviceLogTag      string = \"Device ID\"\n)\n\nvar logEntry *logrus.Entry\n\nfunc Setup() {\n    level, err := logrus.ParseLevel(\"&lt;&lt;loglevel from env&gt;&gt;\")\n    if err != nil {\n        log.Fatalf(err.Error())\n    }\n\n    logger = &amp;logrus.Logger{\n        Out:   os.Stdout,\n        Level: level,\n    }\n    logger.Formatter = &amp;logrus.JSONFormatter{}\n\n    logEntry = logger.WithFields(logrus.Fields{\n        deviceLogTag:      \"&lt;&lt;deviceId from env&gt;&gt;\",\n    })\n}\n\nfunc Error(errMessage string, err error, fields map[string]interface{}) {\n\n    if fields != nil {\n        for key, val := range fields {\n            logEntry = logEntry.WithField(key, val)\n        }\n    }\n    logEntry.\n        WithField(contextLogTag, getCallerInfo()).\n        WithField(errorLogTag, err).\n        Error(errMessage)\n}\n\nfunc Fatal(errMessage string, err error, fields map[string]interface{}) {\n    if fields != nil {\n        for key, val := range fields {\n            logEntry = logEntry.WithField(key, val)\n        }\n    }\n    logEntry.\n        WithField(contextLogTag, getCallerInfo()).\n        WithField(errorLogTag, err).\n        Fatal(errMessage)\n}\n\nfunc Info(msg string, fields map[string]interface{}) {\n    if fields != nil {\n        for key, val := range fields {\n            logEntry = logEntry.WithField(key, val)\n        }\n    }\n    logEntry.WithField(contextLogTag, getCallerInfo()).Info(msg)\n}\n\nfunc Warn(fields map[string]interface{}, args ...interface{}) {\n    if fields != nil {\n        for key, val := range fields {\n            logEntry = logEntry.WithField(key, val)\n        }\n    }\n    logEntry.Warn(args...)\n}\n\nfunc getCallerInfo() string {\n    _, filePath, lineNo, isOk := runtime.Caller(2)\n    if isOk {\n        pathArray := strings.Split(filePath, \"/\")\n        fileName := pathArray[len(pathArray)-1]\n        return fmt.Sprintf(\"%s#%d\", fileName, lineNo)\n    } else {\n        return \"\"\n    }\n}\n</code></pre>",
            "url": "https://prasanna.dev/posts/logging-in-golang-projects",
            "title": "Logging in Golang projects",
            "summary": "A log abstraction in go-lang projects, that can then be used to log common information. This also hides the log library inclusion, making it easier to swap out the library for a different one.",
            "date_modified": "2019-10-17T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/add-redux-middleware-dynamically",
            "content_html": "<p><a href=\"https://redux.js.org/advanced/middleware\">Redux middlewares</a> can be used for a variety of things. You can basically tap into a redux event and perform some action with it. Logging and analytics are very common use cases for Redux middleware.</p>\n<p>In my case, I have a middleware component, that needs to be injected while initializing the Redux store. The middleware component will be served dynamically when the app loads.</p>\n<h3 id=\"exportingthemiddlewarecomponent\">Exporting the middleware component</h3>\n<p>This middleware detects a specific redux action and persist an information to the local storage. It's a custom middleware with a minimal change. This custom function takes in <code>middlewareAPI</code> as the parameter instead of having the state.</p>\n<pre><code class=\"js language-js\">const persistInfo = (middlewareAPI) =&gt; (next) =&gt; (action) =&gt; {\n  if (action.type === \"SOME_ACTION\") {\n    const result = next(action);\n    const state = JSON.stringify(middlewareAPI.getState().listen.value);\n    window.localStorage.setItem(\"PERSIST_THIS_INFO\", state);\n    return result;\n  }\n  return next(action);\n};\n\nexport default persistInfo;\n</code></pre>\n<h3 id=\"loadingthecustommiddleware\">Loading the custom middleware</h3>\n<p>Here is a small utility function that can take in a custom middleware and initialize the store.</p>\n<pre><code class=\"js language-js\">import { createStore, compose } from \"redux\";\nimport reducers from \"./reducers\";\n\nclass Store {\n  constructor() {\n    const composeEnhancers =\n      typeof window === \"object\" &amp;&amp; window.__REDUX_DEVTOOLS_EXTENSION_COMPOSE__\n        ? window.__REDUX_DEVTOOLS_EXTENSION_COMPOSE__({})\n        : compose;\n\n    this.store = createStore(reducers);\n  }\n\n  instance() {\n    return this.store;\n  }\n\n  addMiddleware(middleware) {\n    const middlewareAPI = {\n      getState: this.store.getState,\n      dispatch: (action) =&gt; this.store.dispatch(action),\n    };\n    this.store.dispatch = compose(middleware(middlewareAPI))(\n      this.store.dispatch\n    );\n  }\n}\nexport default new Store();\n</code></pre>\n<p>This is my store class with the store initialization happens in the constructor. Simply, importing this store class and calling the <code>addMiddleware</code> function it's possible to inject the custom middleware component to your redux store.</p>",
            "url": "https://prasanna.dev/posts/add-redux-middleware-dynamically",
            "title": "Add Redux custom middleware dynamically",
            "summary": "Redux provides options to add behaviour through middlewares. Here is an example of dynamically adding middleware to the store.",
            "date_modified": "2018-04-11T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/start-nginx-when-upstream-unavailable",
            "content_html": "<h3 id=\"upstreamsinnginx\">Upstreams in Nginx</h3>\n<p><code>upstream</code> is an nginx directive to define groups of servers. Servers can listen on differnt ports, and it is possible to mix and match the UNIX-domain sockets and TCP connections. You can read about it <a href=\"http://nginx.org/en/docs/http/ngx_http_upstream_module.html\">here.</a></p>\n<h3 id=\"issuewithupstream\">Issue with upstream</h3>\n<p>If you are using proxy_pass with upstream definitions in nginx config, then nginx checks for the server availability during the startup phase. A sample nginx.conf with upstream is here, lots of the .conf file is redacted to focus on the point in discussion.</p>\n<pre><code class=\"nginx language-nginx\">    http {\n        ...\n        upstream service-a {\n            server service-a-ip-or-name:3000;\n        }\n\n        server {\n            ...\n            location /service-a/ {\n                proxy_pass http://service-a/;\n            }\n        }\n    }\n</code></pre>\n<p>In the above mentioned scenario, nginx server will check for service-a while start-up phase. If service-a is down, you will see an error like host not found in upstream service-a</p>\n<h3 id=\"theworkaround\">The Workaround</h3>\n<p>This workaround is for services running in local setup in different docker containers. So, instead of using <code>upstream</code> directive you can directly point your service-discoverable-name in the proxy pass. The only thing while running docker containers, you need to add an additional nginx directive <code>resolver</code> and make it point to docker's internal DNS resolver. 127.0.0.11 The above mentioned config can be re-written as mentioned.</p>\n<pre><code class=\"nginx language-nginx\">    http {\n        ...\n        resolver 127.0.0.11;\n\n        server {\n            ...\n            location /service-a/ {\n                proxy_pass http://service-a-ip-or-name:3000/;\n            }\n        }\n    }\n</code></pre>\n<p><em>Note: nginx approach is very valid in production like setups. However, in developer boxes it may not be possible to have all the services running while nginx starts. The workaround mentioned here should be mostly used in local or in dev setup and not advisable to use in prodcution like setup.</em></p>",
            "url": "https://prasanna.dev/posts/start-nginx-when-upstream-unavailable",
            "title": "Start nginx when upstream is unavailable",
            "summary": "nginx will not start if one of the defined upstreams is not available. Here is a workaround to get through with those situations.",
            "date_modified": "2018-03-21T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/soap-call-in-clojure-compojure-with-saaj",
            "content_html": "<h3 id=\"simpleobjectaccessprotocolsoap\">Simple Object Access Protocol (SOAP)</h3>\n<p>SOAP brings its own protocol and focuses on exposing pieces of application logic (not data) as services. SOAP is focused on accessing named operations, each implements some business logic through different interfaces. This image below expresses the difference between a SOAP and normal REST/JSON endpoint very well.</p>\n<p><img src=\"/assets/posts/images/soap-primer.png\" alt=\"SOAP explanation\" title=\"Soap Primer\" /></p>\n<p>Source: <a href=\"https://stackoverflow.com/a/44713574/419448\">Stack overflow</a></p>\n<h3 id=\"soapwithattachmentapiforjavasaaj\">Soap With Attachment API for Java (SAAJ)</h3>\n<p><a href=\"https://docs.oracle.com/javaee/5/tutorial/doc/bnbhg.html\">SAAJ</a> is a lower level API in Java that express SOAP messages. Java developers rarely use SAAJ since the JAX WS and Spring WS provides better abstraction over SAAJ.</p>\n<h3 id=\"soapinclojure\">SOAP in Clojure</h3>\n<h4 id=\"1prerequisite\">1. Prerequisite</h4>\n<p>As a one-time step, convert the WSDL into Java objects. This can be done using `wsimport` or `xjc`</p>\n<pre><code class=\"bash language-bash\">xjc -wsdl wsdl-file-name\n</code></pre>\n<p>or</p>\n<pre><code class=\"bash language-bash\">wsimport wsdl-file-name\n</code></pre>\n<h4 id=\"2buildsoapmessage\">2. Build SOAP Message</h4>\n<p>First step is to build a soap message with header and body. The root element of the SOAP body is one of the Java object created in the first step. Construct the Java object with the necessary data. Finally convert the SOAP Message into string.</p>\n<h4 id=\"3performpost\">3. Perform POST</h4>\n<p>A simple HTTP POST need to be performed with <code>Content-Type</code> header set to <code>text/xml</code>. This can be done using normal <code>clj-http</code> methods.Authentication should be covered ideally in the SOAP header.</p>\n<h4 id=\"4parseresponseintojavaobject\">4. Parse response into Java Object</h4>\n<p>Finally the response string has to be converted into a SOAP Message again. This is required to parse the SOAP Response Body into one of the generated object.</p>\n<h3 id=\"codeinaction\">Code in action</h3>\n<p>Here is my <a href=\"https://github.com/prasann/soap-clj\">Github repository</a> with a small working application.</p>",
            "url": "https://prasanna.dev/posts/soap-call-in-clojure-compojure-with-saaj",
            "title": "Dealing with SOAP in clojure",
            "summary": "Dealing with SOAP in clojure is not very straight-forward due to the lack of framework support. This post explains how to perform SOAP call using basic Java libraries.",
            "date_modified": "2018-02-15T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/using-network-call-in-react",
            "content_html": "<p>All network calls that are necessary to load data needed by the component should go inside <code>componentDidMount()</code></p>\n<blockquote>\n  <h5 id=\"fromreactdocshttpsfacebookgithubioreactdocsreactcomponenthtmlcomponentdidmount\"><a href=\"https://facebook.github.io/react/docs/react-component.html#componentdidmount\">From React docs</a></h5>\n  <p>componentDidMount() is invoked immediately after a component is mounted. Initialization that requires DOM nodes should go here. If you need to load data from a remote endpoint, this is a good place to instantiate the network request. Setting state in this method will trigger a re-rendering.</p>\n</blockquote>\n<h4 id=\"whynotinsideconstructor\">Why not inside <code>constructor()</code>?</h4>\n<ul>\n<li>If you make a fetch for a component in constructor, and the user navigates away from the page containing that component before the request completes, it will still try to setState on that component despite being unmounted, and React will throw an error.</li>\n<li>If your component fails to load, still you will end up making an unnecessary server-request.</li>\n</ul>\n<h4 id=\"whynotincomponenentwillmount\">Why not in <code>componenentWillMount()</code>?</h4>\n<p>This function is invoked immediately before mounting occurs. So, obviously this appears to be a best place to place the call to load data. However that's not the case.</p>\n<ul>\n<li>Even if you add the network call in componentWillMount, your request will almost certainly not finish before the component is rendered. There is no way to pause the rendering till the request returns. So you will end up re-rendering the component anyways.</li>\n<li>This is the only lifecycle hook called on server rendering. So, if you are serving from the backend, this will be executed twice.</li>\n</ul>",
            "url": "https://prasanna.dev/posts/using-network-call-in-react",
            "title": "Asynchronous calls in React component",
            "summary": "React documentation suggests to use componentDidMount for async calls. Here is the explanation of why you shouldn't do in constructor or in componentWillMount.",
            "date_modified": "2017-09-10T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/flyway-migrations-in-lein-clojure",
            "content_html": "<p><a href=\"https://leiningen.org/\">Leiningen</a> is the easiest way to start with clojure project automation. The project under discussion is a webservices written in clojure with <a href=\"https://github.com/metosin/compojure-api\">compojure-api</a> and <a href=\"https://github.com/ring-clojure/ring\">ring</a> middleware.</p>\n<p>When it came to Database migrations, I didn't find anything straightforward amongst the lein plugins. So, decided to use <a href=\"https://flywaydb.org/\">flyway</a>. I have worked with flyway in the past with Java applications. But, this is the first time with clojure, leiningen combination.</p>\n<h3 id=\"migrationutilityinclojure\">Migration utility in clojure</h3>\n<p>Here is the small migration helper written in Clojure</p>\n<pre><code class=\"clojure language-clojure\">(ns app.migration\n  (:require [environ.core :refer [env]])\n  (:import org.flywaydb.core.Flyway\n           org.flywaydb.core.internal.info.MigrationInfoDumper))\n\n;; Build DB String from the Environment Variables\n(def db-url (str \"jdbc:postgresql://\"\n                 (env :pg-db-host) \":\"\n                 (env :pg-db-port) \"/\" (env :pg-db-name)))\n\n;; Initialize Flyway object\n(def flyway\n  (let [locations (into-array String [\"classpath:db/migration\"])]\n    (doto (new Flyway)\n      (.setDataSource db-url (env :pg-db-user) (env :pg-db-password) (into-array String []))\n      (.setLocations locations))))\n\n(defn migrate [] (.migrate flyway))\n\n(defn clean [] (.clean flyway))\n\n(defn reset [] (clean) (migrate))\n\n(defn info []\n  (println (MigrationInfoDumper/dumpToAsciiTable (.all (.info flyway)))))\n</code></pre>\n<h3 id=\"runningmigrationduringdeployment\">Running migration during deployment</h3>\n<p>I'm using <a href=\"https://github.com/weavejester/lein-ring\">lein-ring</a> plugin, this provided an option to execute function before the handler starts. So, I wired app.migrate to the init block of the handler.</p>\n<p>This helps to run migration everytime before the application deploys. Ofcourse, flyway will take care of what migrations need to run based on the migration version.</p>\n<h3 id=\"runningmigrationsforlocaldevelopment\">Running migrations for local development</h3>\n<p>The above method works perfectly for the application deployment scenarios. However, in local it will be better to execute​ ​migration and clean databases as and when required, rather than re-deploying the application. lein-exec plugin offers​ ​a way to create and execute clojure code from project.clj files. With the above-mentioned migration present, all ​I​​ ha​ve to do is to create some aliases as shown below.</p>\n<pre><code class=\"clojure language-clojure\">:aliases {\n    \"db-clean\"   [\"exec\" \"-ep\" \"(use 'deal-picker.migration) (clean)\"]\n    \"db-migrate\" [\"exec\" \"-ep\" \"(use 'deal-picker.migration) (migrate)\"]\n    \"db-info\"    [\"exec\" \"-ep\" \"(use 'deal-picker.migration) (info)\"]\n    \"db-reset\"   [\"exec\" \"-ep\" \"(use 'deal-picker.migration) (reset)\"]\n}\n</code></pre>",
            "url": "https://prasanna.dev/posts/flyway-migrations-in-lein-clojure",
            "title": "Flyway migrations in lein clojure",
            "summary": "Integrating flyway migrations to compojure apps. Flyway is a popular Java based database migration tool. This post describes about integrating flyway seamlessly with lein compojure ring stack in clojure.",
            "date_modified": "2017-07-15T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/card-slider-using-css3",
            "content_html": "<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/%40keyframes\">CSS Keyframes</a> is a powerful feature to create animations in CSS.</p>\n<p>Below is a small snippet I created for slider like animation.</p>\n<pre><code class=\"html language-html\">&lt;div id=\"card\"&gt;Click me to animate&lt;/div&gt;\n</code></pre>\n<pre><code class=\"css language-css\">.animate {\n  opacity: 1;\n  animation: slider 1s linear;\n}\n\n@keyframes slider {\n  0% {\n    margin-left: 0;\n    opacity: 1;\n  }\n  25% {\n    margin-left: -200px;\n    opacity: 0;\n  }\n  50% {\n    margin-left: 200px;\n    opacity: 0;\n  }\n  100% {\n    margin-left: 0;\n    opacity: 1;\n  }\n}\n\n#card {\n  background: #1f1f1f;\n  margin: 10px;\n  display: block;\n  border: 1px dashed white;\n  height: 200px;\n  width: 200px;\n  color: white;\n  font-weight: bold;\n  padding: 10px;\n  text-align: center;\n  cursor: pointer;\n}\n</code></pre>\n<pre><code class=\"js language-js\">$(\"#card\").on(\"click\", () =&gt; {\n  $(\"#card\").addClass(\"animate\");\n  setTimeout(() =&gt; $(\"#card\").removeClass(\"animate\"), 1000);\n});\n</code></pre>\n<p>Here is the <a href=\"https://codepen.io/prasann/pen/ppNLNL\">link to the codepen</a></p>\n<p>Most of the browsers do support keyframes now. <a href=\"https://caniuse.com/#feat=css-animation\">Here</a> is the \"Can I Use\" page for keyframes.</p>",
            "url": "https://prasanna.dev/posts/card-slider-using-css3",
            "title": "Card slider using CSS Keyframes",
            "summary": "Slider animation using css keyframes",
            "date_modified": "2017-05-25T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/store-function-inside-redux-store",
            "content_html": "<p><a href=\"https://redux.js.org/\">Redux</a> is a predictable state container for Javascript. Redux state has to be serializable all the time.</p>\n<p>Object serialization is the process of converting an object's state to a string from which it can later be restored.</p>\n<p>So, if you are trying to store a inside the Redux state, you need to serialize them before persisting.</p>\n<blockquote>\n  <p>Storing functions inside redux state is not a best practice in general. So try to avoid it.</p>\n</blockquote>\n<p>Javascript functions can be serialized quite easily, the challenge is in retrieving them from the store to execute.</p>\n<p>Below are the helper functions for persisting functions inside Redux state.</p>\n<pre><code class=\"js language-js\">//Returns a string\nexport const serializeFunction = (func) =&gt; func.toString();\n//serializeFunction(()=&gt;console.log('Hello!!'))\n// Output ==&gt; \"()=&gt;console.log('Hello!!')\"\n</code></pre>\n<p>The function to be stored in the state should be converted into string using serializeFunction.</p>\n<pre><code class=\"js language-js\">  //Returns a function\n  export const deserializeFunction = (funcString) =&gt; (new Function(\\`return ${funcString}\\`)());\n</code></pre>\n<p>Convert the string from the redux store into a function using deserializeFunction</p>",
            "url": "https://prasanna.dev/posts/store-function-inside-redux-store",
            "title": "Storing a function in the Redux store",
            "summary": "Redux state can be very useful to share data across the application. This post is about storing a function inside the redux store.",
            "date_modified": "2017-05-17T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/jest-test-toLocaleString-javasscript",
            "content_html": "<p>We had to use <code>toLocaleString</code> with a specific country-code. <code>toLocaleString('de')</code>. This works perfectly in all the browsers. However, not in jest tests.</p>\n<p>Our Jest tests were running with <code>--env=jsdom</code> I got to know that jsdom and phantomJS aren't supporting multiple locale implementations.</p>\n<p><a href=\"https://github.com/ariya/phantomjs/issues/12327\">PhantomJS support locale-specific.</a></p>\n<p>So, the only solution I found is to mock these methods and test rest of the logic. Here is a sample mock behaviour.</p>\n<pre><code class=\"js language-js\">import * as helpers from \"../src/helpers\";\ndescribe(\"formatDate\", () =&gt; {\n  it(\"should invoke localString implementation to format date \", () =&gt; {\n    const localStringMock = jest.fn();\n    const mockDate = { toLocaleString: localStringMock };\n    helpers.formatDate(mockDate);\n    expect(localStringMock).toHaveBeenCalledWith(\"de-DE\", {\n      year: \"numeric\",\n      month: \"2-digit\",\n      day: \"2-digit\",\n      hour: \"2-digit\",\n      minute: \"2-digit\",\n    });\n  });\n});\n</code></pre>\n<p><strong>Note:</strong> This behaviour is applicable for toLocaleDateString() toLocaleTimeString()</p>",
            "url": "https://prasanna.dev/posts/jest-test-toLocaleString-javasscript",
            "title": "Mock toLocaleString in Jest",
            "summary": "Found an issue while testing toLocaleString and other related JS prototype function. Described here is the way to mock them.",
            "date_modified": "2017-01-31T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/expire-session-after-timeout-spring",
            "content_html": "<p>Using Spring security we were building an application which has 2 types of users Internal and External. Our requirement was</p>\n<ol>\n<li>Internal and External users have different idle timeouts.</li>\n<li>External user's session should be invalidated after 30 mins. Irrespective of whether the user is active or not.</li>\n</ol>\n<h4 id=\"settingupidletimeoutinspringsecurity\">Setting up Idle timeout in Spring security</h4>\n<p>Spring provides out of box option to configure an idle timeout value. This invalidation is done by Spring security and happens while making a request after specified amount of time.</p>\n<p>We were able to achieve this by setting up setMaxInactiveIntervalInSeconds on the session object while creation.</p>\n<h4 id=\"settingupmaxtimeoutinspringsecurity\">Setting up Max timeout in Spring security</h4>\n<p>The above technique can be used only for setting the idle time. But our second scenario is to invalidate the session irrespective of whether the user is active or not.</p>\n<p>We ended up writing a custom filter which to invalidate the session manually whenever the session age is greater than the specified value.</p>\n<p>This filter will invalidate the session when the maximum time has reached for that session.</p>",
            "url": "https://prasanna.dev/posts/expire-session-after-timeout-spring",
            "title": "Spring security session timeouts",
            "summary": "Setup session timeouts in spring security. This will explain how to setup the idle timeout and also the max timeout for separate sessions.",
            "date_modified": "2016-09-24T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/angular-resource-and-interceptors",
            "content_html": "<p>Once you set up your project with angular and <a href=\"https://docs.angularjs.org/api/ngResource\">ngResource</a> you will be able to access $resource object.</p>\n<p>$resource will serve as a factory which creates a resource object that lets you interact with RESTful services. You can call HTTP methods directly on this resource object.</p>\n<p>In our application we will have a custom wrapper on top of the angular resource. This wrapper will provide ability for us to transform the object differently on success and error response.</p>\n<p>All adapters will use the wrapper and all end points will overwrite the transform logic on success and error response.</p>\n<pre><code class=\"js language-js\">'profile': {\n    method: 'GET',\n        params: {accountId: '@accountId'},\n    transformRequest: (data) =&gt; {\n        const moreParams = {newParams: data};\n        return angular.toJson(moreParams);\n    },\n        successTransformResponse: (data, headers, status) =&gt; {\n        // Handle parsing for HTTP status 200.\n    },\n        errorTransformResponse: (data, headers, status) =&gt; {\n        // Depending on the status code handle transform logic.\n    }\n}\n</code></pre>\n<h4 id=\"handlinggenericerrorcodes\">Handling generic error codes</h4>\n<p>So, next we have to handle generic error responses across the application. Error codes like 401 (Unauthorized), 503 (Service Unavailable) needs to be redirected to different pages.</p>\n<p>The interceptors are service factories that are registered with the $httpProvider by adding them to the $httpProvider.interceptors array. The factory is called and injected with dependencies (if specified) and returns the interceptor.</p>\n<pre><code class=\"js language-js\">$provide.factory('myHttpInterceptor', function($q, dep1, dep2) {\n    return {\n        'request': function(config) {\n            // do something on success\n            return config;\n        },\n        'responseError': function(rejection) {\n            // do something on error\n            if (canRecover(rejection)) {\n                return responseOrNewPromise\n            }\n            return $q.reject(rejection);\n        }\n    }\n}\n</code></pre>\n<p>In the responseError method block, we used to handle all the generic error response code across the application.</p>\n<h4 id=\"observation\">Observation</h4>\n<p>I was expecting the code in HttpInterceptor to be executed before my transform logic in the resource wrapper. But i was wrong. Only after the resource transformation http interceptors are called. (Refer this <a href=\"https://github.com/angular/angular.js/issues/7594\">issue.</a>)</p>\n<p>So, whenever a service responds with 500 error, Http interceptor will redirect the user to a different page. However, this will not happen if there is an error in transformation logic. In order to circumvent this problem, we started writing our error transform response specifically for the error codes. This means that, our transformation logic will not be executed for our generic error codes and eventually it reaches http interceptor.</p>\n<pre><code class=\"bash language-bash\">{{ site.data.comments }}\n</code></pre>",
            "url": "https://prasanna.dev/posts/angular-resource-and-interceptors",
            "title": "Angular resource and http interceptor",
            "summary": "This post describe about the use of angular resource library along with http interceptor.",
            "date_modified": "2016-07-17T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/post-errors-to-an-endpoint-angular",
            "content_html": "<p>We were looking for an efficient way of capturing all the Javascript errors from browsers in our backend so it appears in our Kibana dashboard along with the server logs</p>\n<p>We had a Angular 1.5.8 application in front of multiple micro-services endpoint. Any error in the angular application will appear in the browser console and we planned to push these logs back to the server.</p>\n<h4 id=\"angulars_dexceptionhandler_\">Angular's <em>$exceptionHandler</em></h4>\n<p>In order to catch all the exceptions, we have to override the $exceptionHandler component provided by angular. Only catch here is that, since we are overriding angular component we may not be able to inject $http or any other angular component in our overrides and doing so will throw a cyclic dependency issue.</p>\n<h4 id=\"initialsolution\">Initial solution</h4>\n<p>We came up with an idea of injecting $injector and fetching $http using the same.</p>\n<pre><code class=\"js language-js\">factory('$exceptionHandler', \\['$log', '$window', '$injector',\n    ($log, $window, $injector)=&gt; {\n        return (exception, cause) =&gt; {\n            $log.error(exception, cause);\n            try {\n                const $http = $injector.get('$http');\n                const logMessage = \\[{\n                    level: 'error',\n                    message: exception.toString(),\n                    url: $window.location.href,\n                    stackTrace: exception.stack,\n                    currentTimestamp: Date.now()\n                }\\];\n                $http.post('/log/message', logMessage);\n            } catch (loggingError) {\n                $log.log(loggingError);\n            }\n        );\n</code></pre>\n<p>The above piece of code will work perfectly and will be able to post all the errors generated to an exposed endpoint.</p>\n<p>But the problem is, if the $http.post throws any exception then it causes unrecoverable recursion and browser will hung.</p>\n<p>In order to come out of that issue, we re wrote our http post logic using native JS syntax.</p>\n<h4 id=\"finalsolution\">Final solution</h4>\n<p>Same code re written using native JS functions.</p>\n<pre><code class=\"js language-js\">factory('$exceptionHandler', \\['$log', '$window', '$injector', ($log, $window, $injector)=&gt; {\n    return (exception, cause) =&gt; {\n        $log.error(exception, cause);\n        try {\n            let commonHeaders = $injector.get('$http').defaults.headers.common;\n            const logMessage = \\[{\n                level: 'error',\n                message: exception.toString(),\n                url: $window.location.href,\n                stackTrace: exception.stack,\n                currentTimestamp: Date.now()\n            }\\];\n            let xmlhttp = new XMLHttpRequest();\n            xmlhttp.open('POST', '/log/message');\n            xmlhttp.setRequestHeader('Content-Type', 'application/json;charset=UTF-8');\n            for (let header in commonHeaders) {\n                if (commonHeaders.hasOwnProperty(header)) {\n                    let headerValue = commonHeaders\\[header\\];\n                    if (angular.isFunction(headerValue)) {\n                        headerValue = headerValue();\n                    }\n                    xmlhttp.setRequestHeader(header, headerValue);\n                }\n            }\n            xmlhttp.send(angular.toJson(logMessage));\n        } catch (loggingError) {\n            $log.log(loggingError);\n        }\n    };\n});\n</code></pre>",
            "url": "https://prasanna.dev/posts/post-errors-to-an-endpoint-angular",
            "title": "Post browser logs to server in an Angular app",
            "summary": "This post describes about posting all the browser errors in an angular application to an endpoint. This will be helpful to analyse or debug issues.",
            "date_modified": "2016-06-24T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/expanding-amazon-ebs-volumes",
            "content_html": "<p>I had an AWS image which was created using an EC2 instance of size 8 GB. Whenever i try to launch an instance i usually change the storage size to something say 20 GB. But once the system is launched when i do a</p>\n<pre><code class=\"bash language-bash\">df -h\n</code></pre>\n<p>i still see 8 GB and not 20 GB.</p>\n<p>On further reading i understood i need to resize the disk size. So i did the same using</p>\n<pre><code class=\"bash language-bash\">sudo resize2fs /dev/xvde1\n</code></pre>\n<p>But i was getting the following error:</p>\n<p>The filesystem is already *** blocks long. Nothing to do!</p>\n<p>Then to reolve this issue i have to perform the following steps.</p>\n<ul>\n<li>SSH to the machine.</li>\n</ul>\n<pre><code class=\"bash language-bash\">fdisk /dev/xvde\n</code></pre>\n<ul>\n<li>You should be seeing this message.</li>\n</ul>\n<p>WARNING: DOS-compatible mode is deprecated. It's strongly recommended to switch off the mode (command 'c') and change display units to sectors (command 'u')</p>\n<ul>\n<li>Enter 'u' to change display units</li>\n<li>Enter 'p' to view the current paritions.</li>\n<li>Enter 'd' to delete current partitions.</li>\n<li>Enter 'n' to create a new partition.</li>\n<li>Enter 'p' to set it as primary partitions.</li>\n<li>Enter '1' to set it as primary partitions.</li>\n<li>Set the desired space. If nothing is given the entire space is allotted.</li>\n<li>Enter 'a' to make it bootable.</li>\n<li>Enter '1' and 'w' to write and save the changes.</li>\n<li>Reboot the instance from AWS console.</li>\n<li>Now if you resize the parition it worked all fine.</li>\n</ul>\n<pre><code class=\"bash language-bash\">sudo resize2fs /dev/xvde1\n</code></pre>\n<p>Check the partition size, it should be all set with more space.</p>",
            "url": "https://prasanna.dev/posts/expanding-amazon-ebs-volumes",
            "title": "Expanding Amazon EBS Volume in a EC2 instance.",
            "summary": "Even after increasing the size of the EBS volume in the AWS console, the actual size of the EBS wasn't increased. Have to follow the following steps to grow the EBS size.",
            "date_modified": "2016-02-03T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/capistrano-set-deployed-revision",
            "content_html": "<p>We use Capistrano to deploy our Rails application. Recently i upgraded our capistrano version from 2 to 3</p>\n<p>Capistrano 3 has a complete DSL changeover. Apart from this one other major change I figured out was the way a Git repository is been deployed.</p>\n<p>Previously a Git repository is cloned in the deploy location. Now in Cap 3 a Git archive is been downloaded to the deploy location. This means the deploy directory is no more a Git repository. During Cap 2 times, we used to run a 'git log' command in the deployed driectory to find the deployed revision. Now after upgrade I am unable to do this.</p>\n<p>Cap 3 has got a REVISION file, which contain the SHA of the deployed commit. This wasn't useful in our case, as we show this message in our web application.</p>\n<p>So i ended up writing a Cap task using a similar logic to create a REVISION file with our custom formatted Git message.</p>\n<pre><code class=\"ruby language-ruby\">    namespace :deploy do\n      task :add\\_revision\\_file do\n        on roles(:app) do\n          within repo\\_path do\n            execute(:git, :'log', :\"--pretty=format:'%h | %ai | %d %s'\", :'-1',\n            :\"#{fetch(:branch)}\", \"&gt;#{release\\_path}/REVISION\")\n          end\n        end\n      end\n    end\n\n\n    after 'deploy:updating', 'deploy:add\\_revision\\_file'\n</code></pre>\n<p>This will overwrite the REVISION file created by Cap with our custom message. Which will be consumed by our application.</p>",
            "url": "https://prasanna.dev/posts/capistrano-set-deployed-revision",
            "title": "Set deployed Git revision using Capistrano 3",
            "summary": "While deploying Rails application using Capistrano 3, recording the current deployed git revision to be used by Rails applicaiton.",
            "date_modified": "2016-01-02T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/url-generation-error-after-upgrading-rails",
            "content_html": "<p>On upgrading my rails app from 4.0 to 4.2.5 i steeped onto a wierd issue where my form_for tag breaks and starts throwing exception.</p>\n<p>A REST model on new action raised an UrlGenerationError exception because of the form_for tag.</p>\n<p>For ex: if User is a model my form_for looked like this</p>\n<pre><code class=\"ruby language-ruby\">    form\\_for(@user, url: user\\_path(@user)) do |f|\n</code></pre>\n<p>Raised exception was</p>\n<pre><code class=\"bash language-bash\">    No route matches {:action=&gt;\"show\", :controller=&gt;\"users\", :id=&gt;nil} missing required keys: \\[:id\\]\n</code></pre>\n<p>The @user object’s id is nil since it’s not yet saved in the database. Previously if it was nil that is been skipped by the the url generation. All these occurrences started throwing errors.</p>\n<p>I have to change the form_for tag to</p>\n<pre><code class=\"ruby language-ruby\">    form\\_for(@user) do |f|\n</code></pre>\n<p>This posts the form to default users_path.</p>\n<h3 id=\"nestedobjects\">Nested objects:</h3>\n<pre><code class=\"ruby language-ruby\">    form\\_for(@user, url: user\\_address\\_path(@user, @address)) do |f|\n</code></pre>\n<p>was changed to</p>\n<pre><code class=\"ruby language-ruby\">    form\\_for(\\[@user, @address\\]) do |f|\n</code></pre>",
            "url": "https://prasanna.dev/posts/url-generation-error-after-upgrading-rails",
            "title": "UrlGenerationError after upgrading to Rails 4.2",
            "summary": "On a REST model new action, form_for tag breaks and raises UrlGenerationError after upgrading to Rails 4.2",
            "date_modified": "2015-12-08T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/add-ckeditor-to-rails-pipeline",
            "content_html": "<p>We are using <a href=\"http://ckeditor.com/\">ckeditor</a> in our rails application (Rails 4.2).</p>\n<p>Number of network calls made by the ckeditor and its plugins are quite alot and we were facing difficulty in integrating them with the Rails asset pipeline.</p>\n<p>My initial approach is to use a <a href=\"https://github.com/tsechingho/ckeditor-rails\">ckeditor rails gem</a>. However getting it to work was complicated. On top of it we had some custom plugins written for ckeditor and making it to work with ckeditor rails gem was almost impossible.</p>\n<p>Taking some pointers from this <a href=\"https://github.com/galetahub/ckeditor/issues/307\">issue</a> finally could get into some working solution.</p>\n<ol>\n<li><p>Move all the CKEditor files into vendor/assets/javascript/ckeditor</p></li>\n<li><p>In application.js add</p>\n<p>//= require ckeditor/ckeditor</p></li>\n<li><p>ckeditor.js looks up for other ckeditor relative to CKEDITOR_BASEPATH location. So before loading ckeditor in JS add a line to set that environment variable.</p>\n<p>window.CKEDITOR_BASEPATH = '/assets/ckeditor/';</p></li>\n<li><p>Add</p>\n<p>config.assets.precompile &lt;&lt; ['ckeditor/*']</p>\n<p>to your application.rb file.</p></li>\n<li><p>Finally add a file called precompile<em>hook.rake This rake task will help in compiling the ckeditor files and add it to the assets folder. The content of the rake task is here. <a href=\"https://gist.github.com/prasann/c8978041777cb443fb77\">precompile</em>hook.rake</a></p></li>\n</ol>\n<p>Here is the screenshot of the network calls before and after adding ckeditor to asset pipeline.</p>\n<p><a href=\"/assets/images/posts/add_ckeditor_to_rails/full/before.png\" title=\"Before adding to asset pipeline\"><img src=\"/assets/images/posts/add_ckeditor_to_rails/thumbs/before.png\" alt=\"Before adding to asset pipeline\" /></a> <a href=\"/assets/images/posts/add_ckeditor_to_rails/full/after.png\" title=\"After adding to asset pipeline\"><img src=\"/assets/images/posts/add_ckeditor_to_rails/thumbs/after.png\" alt=\"After adding to asset pipeline\" /></a></p>\n<p>Even after adding ckeditor to asset pipeline the it did not effectively reduce all calls into one. Still the ckeditor's plugin calls are been fired separately.</p>",
            "url": "https://prasanna.dev/posts/add-ckeditor-to-rails-pipeline",
            "title": "Integrating CKEditor with Rails asset pipeline.",
            "summary": "Integrating CKEditor plugin into rails asset pipeline.",
            "date_modified": "2015-05-26T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/deploying-jekyll-blog-through-rake-script",
            "content_html": "<p>This blog is powered by Jekyll and I use Github pages as web server.</p>\n<h4 id=\"branchstructureingithub\">Branch structure in Github</h4>\n<p>Github by default publish the contents of the master branch as a github page. So i have created two branches in the repository.</p>\n<p><strong>source:</strong> contains Jekyll based folder structure. _drafts, _posts, _site etc. All the new posts are added in the drafts folder first and then once its written fully it is then moved to _posts folder and are then ready to be published.</p>\n<p><strong>master:</strong> is simply a generated content from the rake script. This branch has all the HTML files that are generated using Jekyll gem.</p>\n<h4 id=\"folderstructureindevbox\">Folder structure in Dev box</h4>\n<p>I have both the branches checked out in different folders. Both these folders are present in the same level (will be useful while generating output)</p>\n<h4 id=\"rakescript\">Rake script</h4>\n<h5 id=\"togeneratehtml\">To generate HTML</h5>\n<p>I have the Rakefile in the root level of my source branch. The rake task mentioned below will create HTML equivalent inside the _site folder.</p>\n<pre><code class=\"ruby language-ruby\">task :generate do\nJekyll::Site.new(Jekyll.configuration({\n    \"source\"      =&gt; \".\",\n    \"destination\" =&gt; \"\\_site\"\n    })).process\nend\n</code></pre>\n<h5 id=\"topublishingithub\">To publish in Github</h5>\n<p>This task copies the entire _site folder into the master branch (locally). This is why i need to checkout both master and source branches separately and keep them in the same level.</p>\n<p>After copying the contents, simply it switches to the master branch and does a git push.</p>\n<p>Once the changes are pushed into github's master branch the changes are then reflected in your site immediately.</p>\n<pre><code class=\"ruby language-ruby\">task :publish =&gt; \\[:generate\\] do\n    cp\\_r \"\\_site/.\", LOCAL\\_DIR\\_NAME\n    cp \".travis.yml\", LOCAL\\_DIR\\_NAME\n    pwd = Dir.pwd\n    Dir.chdir LOCAL\\_DIR\\_NAME\n    system \"git add --all\"\n    message = \"Site updated at #{Time.now.utc}\"\n    system \"git commit -m #{message.inspect}\"\n    system \"git push origin master:refs/heads/master\"\n    Dir.chdir pwd\nend\n</code></pre>",
            "url": "https://prasanna.dev/posts/deploying-jekyll-blog-through-rake-script",
            "title": "Deploying Jekyll site for Github pages through rake script",
            "summary": "Deploying jekyll blog or site for Github pages using rake script.",
            "date_modified": "2014-08-30T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/integrating-netflix-hystrix-to-a-spring-application",
            "content_html": "<p>After reading Martin Fowler's <a href=\"http://martinfowler.com/bliki/CircuitBreaker.html\">Circuit breaker post</a> i thought of implementing the same to my application. While looking for possible approaches to this solution i stepped onto Hystrix.</p>\n<p><a href=\"https://github.com/Netflix/Hystrix\">Hystrix</a> is a latency and fault tolerance library designed to isolate points of access to remote systems, services and 3rd party libraries, stop cascading failure. Using this implementing CircuitBreakers are quite straight forward.</p>\n<p>To start with i wanted to measure the latency of the 3rd party calls that goes through my application. Mine was Java Spring applcation running in Tomcat. As per Hystrix documentation all the third party calls that need to be monitored are to be wrapped within a command. This command will be executed in a separate thread. Since all the 3rd party calls go through this layer it is easy to monitor those calls. It is also possible to define a fallback approach when a particular service call fails. And there by isolating these scenarios from the application code. More of how this works is explained in detail on <a href=\"https://github.com/Netflix/Hystrix/wiki\">Hystrix wiki</a></p>\n<p>The problem i had was i already have my application up and running. And all i need to do is just monitoring the 3rd party calls (as of now) Now integrating Hystrix meant i need to re design the 3rd party calls to introduce a middle layer to wrap them up.</p>\n<p>I was thinking of writing Aspect based solution to wrap these calls throughout my application with an annotation.</p>\n<p>To do this, make sure you have included Spring AOP in your application. Declare and define an annotation as shown below.</p>\n<pre><code class=\"java language-java\">@Aspect\n@Component\npublic class CircuitBreakerAspect {\n    @Around(\"@annotation(com.example.Monitor)\")\n    public Object monitoringAround(final ProceedingJoinPoint aJoinPoint) throws Throwable {\n        String theShortName = aJoinPoint.getSignature().toShortString();\n        HystrixCommand.Setter theSetter =\n                HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(theShortName));\n        theSetter = theSetter.andCommandKey(HystrixCommandKey.Factory.asKey(theShortName));\n        HystrixCommand theCommand = new HystrixCommand(theSetter) {\n            @Override\n            protected Object run() throws Exception {\n                try {\n                    return aJoinPoint.proceed();\n                } catch (Exception e) {\n                    throw e;\n                } catch (Throwable e) {\n                    throw new Exception(e);\n                }\n            }\n        };\n        return theCommand.execute();\n    }\n}\n</code></pre>\n<p>Using Hystrix dashboard you can able to wire the views and could able to monitor the application. More about this is written <a href=\"http://www.mirkosertic.de/doku.php/architecturedesign/springhystrix\">here</a></p>\n<p>However this annotation approach can be used only for monitoring purposes or when you need to do similar actions for all the services. When i moved onto writing circuit breakers this cannor be done since all the service calls need its own fallback approaches. So in that case it was better to implement them as separate commands so all the code will fit within.</p>",
            "url": "https://prasanna.dev/posts/integrating-netflix-hystrix-to-a-spring-application",
            "title": "Integrating Netflix Hystrix to a Spring Application",
            "summary": "Hystrix is a latency and fault-tolerance library from Netflix. This post describes how to integrate it with Spring Aspects to make the implementation simpler.",
            "date_modified": "2014-07-14T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/testing-apiary-using-github-travis",
            "content_html": "<p><a href=\"http://apiblueprint.org/\">API Blueprint</a> is a documentation-oriented API description language. A couple of semantical assumptions over the plain Markdown.</p>\n<p><a href=\"https://github.com/apiaryio/dredd\">Dredd</a> is a command-line tool for testing API documentation written in API Blueprint format against its backend implementation.</p>\n<p>I could able to setup dredd quite easily on my Mac by installing Node and npm. However its not quite straight forward in Windows. I faced lots of difficulties while installing node, npm and dredd.</p>\n<p>So i decided to use <a href=\"http://travis-ci.org/\">Travis</a> to setup the testing pipeline for my jobs. All i needed to do is to have a .travis.yml file to install node_js and install dredd using npm.</p>\n<p>Added a simple script file to run the dredd tool inside the job. And that's it. As part of the code i also checked in the API markdown files which will run againt the APIs</p>",
            "url": "https://prasanna.dev/posts/testing-apiary-using-github-travis",
            "title": "Testing APIary using Dredd.",
            "summary": "Test API blueprint mardown files by simply hosting them on GitHub and setting up a pipeline in Travis CI.",
            "date_modified": "2014-07-01T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/making-https-call-using-apache-httpclient",
            "content_html": "<p>This post details about making Secure HTTP(HTTPs) call from a server using Apache HTTPClient library.</p>\n<p>The simplest will be to ignore the ssl certificates and to trust any connection. This approach is not acceptable for production code as it defeat the purpose of using HTTPS. However in some use cases if you want to try out something quickly you can go with this route.</p>\n<h4 id=\"trustanycertificateapproachsimplenotrecommendedforproductioncode\">Trust any certificate approach (Simple, not recommended for production code.)</h4>\n<pre><code class=\"java language-java\">import javax.net.ssl.SSLContext;\nimport javax.net.ssl.X509TrustManager;\n\nimport org.apache.http.client.HttpClient;\nimport org.apache.http.conn.ssl.SSLConnectionSocketFactory;\nimport org.apache.http.conn.ssl.SSLContexts;\n\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\n\nimport java.security.SecureRandom;\n\npublic class HttpClientFactory {\n\n    private static CloseableHttpClient client;\n\n    public static HttpClient getHttpsClient() throws Exception {\n\n        if (client != null) {\n            return client;\n        }\n        SSLContext sslcontext = SSLContexts.custom().useSSL().build();\n        sslcontext.init(null, new X509TrustManager\\[\\]{new HttpsTrustManager()}, new SecureRandom());\n        SSLConnectionSocketFactory factory = new SSLConnectionSocketFactory(sslcontext,\n                SSLConnectionSocketFactory.BROWSER\\_COMPATIBLE\\_HOSTNAME\\_VERIFIER);\n        client = HttpClients.custom().setSSLSocketFactory(factory).build();\n\n        return client;\n    }\n\n    public static void releaseInstance() {\n        client = null;\n    }\n}\n</code></pre>\n<p>The above method will return httpClient object which can be used to make any HTTPS calls. Performing HTTPS call is no different from making HTTP call from now on. So you can have a factory with two methods, one for secure and one for non-secure.</p>\n<p>Here we have used HttpsTrustManager, which will do nothing more than trusing all clients. This is done by simply implementing X509TrustManager and auto generating all the methods.</p>\n<pre><code class=\"java language-java\">import java.security.cert.CertificateException;\nimport java.security.cert.X509Certificate;\n\nimport javax.net.ssl.X509TrustManager;\n\npublic class HttpsTrustManager implements X509TrustManager {\n\n    @Override\n    public void checkClientTrusted(X509Certificate\\[\\] arg0, String arg1)\n            throws CertificateException {\n        // TODO Auto-generated method stub\n\n    }\n\n    @Override\n    public void checkServerTrusted(X509Certificate\\[\\] arg0, String arg1)\n            throws CertificateException {\n        // TODO Auto-generated method stub\n\n    }\n\n    @Override\n    public X509Certificate\\[\\] getAcceptedIssuers() {\n        return new X509Certificate\\[\\]{};\n    }\n\n}\n</code></pre>\n<h4 id=\"importingakeystorerecommended\">Importing a keystore (Recommended)</h4>\n<p>If you are writing produciton quality code, then you should be looking at this approach. Have a all the keys in your application and create a SSLContext using those keystores. The created SSLContext can then be injected to SSLConnectionSocketFactory and remaining steps will be the same.</p>\n<pre><code class=\"java language-java\">import javax.net.ssl.SSLContext;\n\nimport org.apache.http.client.HttpClient;\nimport org.apache.http.conn.ssl.SSLConnectionSocketFactory;\nimport org.apache.http.conn.ssl.SSLContexts;\n\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.security.KeyStore;\nimport java.security.KeyStoreException;\nimport java.security.NoSuchAlgorithmException;\nimport java.security.cert.CertificateException;\nimport java.security.KeyManagementException;\n\npublic class HttpClientFactory {\n\n    private static CloseableHttpClient client;\n\n    public static HttpClient getHttpsClient() throws Exception {\n\n        if (client != null) {\n            return client;\n        }\n        SSLContext sslcontext = getSSLContext();\n        SSLConnectionSocketFactory factory = new SSLConnectionSocketFactory(sslcontext,\n                SSLConnectionSocketFactory.BROWSER\\_COMPATIBLE\\_HOSTNAME\\_VERIFIER);\n        client = HttpClients.custom().setSSLSocketFactory(factory).build();\n\n        return client;\n    }\n\n    public static void releaseInstance() {\n        client = null;\n    }\n\n    private SSLContext getSSLContext() throws KeyStoreException,\n    NoSuchAlgorithmException, CertificateException, IOException, KeyManagementException {\n        KeyStore trustStore  = KeyStore.getInstance(KeyStore.getDefaultType());\n        FileInputStream instream = new FileInputStream(new File(\"my.keystore\"));\n        try {\n            trustStore.load(instream, \"nopassword\".toCharArray());\n        } finally {\n            instream.close();\n        }\n        return SSLContexts.custom()\n                .loadTrustMaterial(trustStore)\n                .build();\n    }\n}\n</code></pre>\n<p>The only difference between the two approaches are the way the SSLContext been created.</p>",
            "url": "https://prasanna.dev/posts/making-https-call-using-apache-httpclient",
            "title": "Making HTTPS call using Apache HttpClient.",
            "summary": "Perform Https calls from server using Apache HttpClient library.",
            "date_modified": "2014-06-26T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/skeleton-gradle-spring-mvc-app",
            "content_html": "<p>Will try to list down few of my learnings to set up a Spring MVC app with Gradle. And also this time i tried using Servlet 3.0 spec which means no .xml files for Spring configuration.</p>\n<p>Refer to the <a href=\"https://github.com/prasann/GradleSpringApp\">GitHub repo</a> for the source code. I will be giving some notes on the code.</p>\n<h4 id=\"createbuildgradlefile\">Create build.gradle file</h4>\n<p>build.gradle can be pretty much simple to start with. I started with adding java and war plugin followed by adding dependencies to the Spring artifacts. I have to define</p>\n<p>runtime 'javax.servlet:jstl:1.2'</p>\n<p>to make sure it doesn't get packaged as part of the war.</p>\n<p>Then thought it will be awesome to start the application in one command instead of building the war and deploying it in local instance. After some initial searching landed onto this <a href=\"https://github.com/bmuschko/gradle-cargo-plugin\">Cargo plugin</a> This lets you to configure the server of your choice and get it working. So after doing some basic configuration got this working.</p>\n<p>Now</p>\n<p>gradle war cargoRunLocal</p>\n<p>since the task name is not so user friendly, just added an alias to it.</p>\n<p>task serve(dependsOn: cargoRunLocal) &lt;&lt; {\n}</p>\n<h4 id=\"setupspring\">Setup Spring</h4>\n<p>I decided to play around with Servlet 3.0 style of Spring configuration. This means that i do not need to create web.xml or applicationContext.xml files. Instead i can go with complete Java style configuration.</p>\n<p>Application containers (tomcat 7+ in my case) will look for implementation of WebApplicationInitializer and will load that class on the startup. Initializer.java in my src will be equivalent for web.xml</p>\n<p>MvcConfig.java will be equivalent to applicationContext.xml file. This contains all the bean initialization, property place holders and more.</p>\n<p>As you can see most of the configurations are handled by annotations.</p>\n<h4 id=\"setupunittests\">Setup Unit Tests</h4>\n<p>Setting up Unit tests are no different to Gradle. As i mentioned i have used Java style configuration for my Spring classes. So the style of testing my controllers will also be different.</p>\n<p>InitControllerTest.java will be my controller test. I have initialized a mock web application in the</p>\n<p>@Before</p>\n<p>method and the rest of the stuff are handled by annotations.</p>\n<h4 id=\"setuplogging\">Setup Logging</h4>\n<p>Setting up slf4j is quite straight forward. You have to add slf4j-log4j, log4j jars and add a log4j.properties to the</p>\n<p>src/main/resources</p>\n<p>In the log4j.properties you can define the way your appenders should work.</p>",
            "url": "https://prasanna.dev/posts/skeleton-gradle-spring-mvc-app",
            "title": "Gradle, Spring MVC App.",
            "summary": "A skeleton sample app demostrating gradle set up with Spring MVC along with basic logging and deployment in tomcat environment.",
            "date_modified": "2014-06-14T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/cross-site-http-requests",
            "content_html": "<p>Browsers have a default security mechanism to prevent http(s) request from one domain to other. Since there are tons of possibilities to misuse them.</p>\n<p>The same origin policy prevents a document or script loaded from one origin from getting or setting properties of a document from another origin. This policy dates all the way back to Netscape Navigator 2.0.</p>\n<p>However, there are lots of genuine use cases for this scenario to occur which got to be handled by the application and the browsers.</p>\n<p>I had to face one such genuine case, and got to deal with one of the miserable browser of my time IE :(</p>\n<p>Just documenting few techniques to overcome this problem. I am not elaborating the techniques since that can be figured out, all i wanted is to document the possible solutions and when to use them.</p>\n<h4 id=\"1corscrossoriginresourcesharing\">1. CORS - Cross Origin Resource sharing</h4>\n<p>A detailed description of how to implement is on this <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS\">Mozilla docs</a></p>\n<p>This is a very common and an elegant solution that you can find on the net for this problem. All you will need is to add few headers on the application server. This will allow application to allow requests from other application (domain). And also all the response from the parent application will include a header</p>\n<p>Access-Control-Allow-Origin:</p>\n<p>Which will tell the browser to allow those specified domains to talk to the application.</p>\n<p>This solution works well with most of the modern browsers. Check out for the CORS browser support <a href=\"http://caniuse.com/cors\">here.</a></p>\n<p><strong>Limitations with IE:</strong></p>\n<p>As you can see the browser compatibility chart, IE8 and IE9 has a partial implementation to CORS.</p>\n<p>Modern browsers will be able to support CORS for XMLHttpRequest. However IE8 and IE9 supports CORS using XDomainRequest object. What this means is that they have few limitations of their own.</p>\n<p>Some of the most important constraints are,</p>\n<ul>\n<li>Your requests should be only GET, POST HTTP methods and not PUT, DELETE etc.</li>\n<li>Both the domain (the calling and the caller) uses the same protocol. Either HTTP or HTTPS.</li>\n<li>Your request should not have any custom headers.</li>\n</ul>\n<p>The exhaustive list is been detailed out in this <a href=\"http://blogs.msdn.com/b/ieinternals/archive/2010/05/13/xdomainrequest-restrictions-limitations-and-workarounds.aspx\">MSDN blog</a>.</p>\n<p><strong>Workaround for IE</strong></p>\n<p>If you think you can live with the constraints mentioned above, then the workaround is quite simple. You got to change all the XHR to XDR to make it work. Luckily if you are using jQuery you don't need to go through changing all the requests. Instead you can use this <a href=\"https://github.com/MoonScript/jQuery-ajaxTransport-XDomainRequest\">jQuery plugin</a> . I guess there are more of these available just check out before breaking your brains.</p>\n<h4 id=\"2jsonpsolution\">2. JSONP Solution</h4>\n<p>Using <a href=\"http://json-p.org/\">JSONP</a> response instead of JSON response.</p>\n<p><strong>Advantage:</strong> No need of any specific workaround for IE8.</p>\n<p><strong>Limitation:</strong> Works only for HTTP(S) GET request. If you are planning to use POST/PUT/DELETE this solution is not for you.</p>\n<h4 id=\"3iframehack\">3. iFrame Hack.</h4>\n<p>This is a creepy hack. Lets say, if you want to make a call from appA to appB. In appA's landing page load a hidden iFrame with some URL of appB. Then perform all the requests to appB from that iFrame. Since iFrame's domain is appB browsers' will not complain.</p>\n<p><strong>Limitation:</strong> Here the challenge is to consume the response. Your landing page should wait for an even in the iFrame and should consume the iFrame content. Don't even think of this solution if you want to make more than 1 cross site request in a page.</p>\n<h4 id=\"4reverseproxysolution\">4. Reverse Proxy solution</h4>\n<p>If you want your appA to make a call to appB. Set up a simple reverse proxy to the appA. And use relative paths for the Ajax requests, while the server would be acting as a proxy to any remote location.</p>\n<p>So in appA the relative path of the request will be <em>/cors-ajax</em>. The browser will not complain since this is not pointing to a different domain. And the reverse proxy rule will redirect anything of <em>cors-ajax</em> to appB.</p>\n<p>More reference to this implementation:</p>\n<ul>\n<li><a href=\"http://www.askapache.com/htaccess/reverse-proxy-apache.html#Configuring_Proxy\">Configuring the Proxy</a></li>\n<li><a href=\"http://stackoverflow.com/questions/7807600/apache-mod-proxy-configuring-proxypass-proxypassreverse-for-cross-domain-ajax\">Configuring Mod Proxy - SO</a></li>\n</ul>\n<p><strong>Limitation:</strong> The server config are quite hard (at least for me) to understand and perform.</p>\n<h4 id=\"5appbasedsolution\">5. App based solution</h4>\n<p>This solution is very similar to that of the reverse proxy but you don't need to make any server config changes. The initial CORS approach sounds reasonable, but few limitations like same protocol might stop us from using it. Applications like <a href=\"http://anyorigin.com/\">AnyOrigin</a>, <a href=\"http://whateverorigin.org/\">WhateverOrigin</a> does that for you. They support http and https so you can use the protocol of the main window and consume the response. If you feel unsafe of using a different domain, you can deploy it in your own infrastructure.</p>\n<p><strong>Limitation:</strong> One more app to maintain :(</p>\n<h4 id=\"6addagenericcontrollerservletinyourparentdomain\">6. Add a generic controller/servlet in your parent domain.</h4>\n<p>Have a controller/servlet in your app which actually does the external domain call. Have only one GET, POST method. Keep posting all your requests to the same end-point with an additional header containing the actual end-point. Inside the method extract the header, make a call and go around about it. This means that browser doesn't know its an external domain call as your app will serve as a wrapper to that external domain call.</p>\n<p><strong>Limitation:</strong> Multiple HTTP calls for single request/response.</p>\n<p><strong>More on:</strong> <a href=\"http://stackoverflow.com/questions/3076414/ways-to-circumvent-the-same-origin-policy\">How to circumvent same origin policy?</a></p>",
            "url": "https://prasanna.dev/posts/cross-site-http-requests",
            "title": "Cross Site HTTP(S) Requests - CORS Issue",
            "summary": "Some tried out solutions for the cross site request issue. Should be a good place to look out for which solution to be used under a circumstance.",
            "date_modified": "2014-06-01T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/re-running-testng-failed-tests",
            "content_html": "<p>We were running our Selenium functional tests using <a href=\"http://testng.org/doc/index.html\">testNG</a> runner in <a href=\"http://jenkins-ci.org/\">Jenkins</a>. However the problem was we were having too many failures on our initial run, and lot of these failures were classified as random or not reproducable. Mainly these are test script issues. Some times the testers tend to put some static wait conditions which might work on their machine but not in the Jenkins agent. Sometimes the environment against which our tests run might be a bit slow which pushes our pass percentage well behind. The ideal appropriate fix will be is to go through the failed test cases and figure out the randomness and fix it. But we thought of adding a re run mechanism to our test job to identify these random failures. As the no. of test cases grew, we ended up in re running the failed tests multiple times, something like this.</p>\n<p>Name Total Failed</p>\n<p>InitialRun 100 30</p>\n<p>ReRun1 30 20</p>\n<p>ReRun2 20 10</p>\n<p>In this case we will interpret the last 10 failures as a legitimate failures and rest as intermittent ones.</p>\n<p>Please note that this is not a correct approach to reduce the failure count. This make the testers more lazy since they always analyse only the failed tests in the final run. And this increases the running time of the job inadvertently since the sure fail cases runs for n times and failing always.</p>\n<h3 id=\"rerunningtestngfailedtestsusingant\">Re Running testNG failed tests using ANT</h3>\n<p>After every test run, testng will create a file called testng-failed.xml in the report directory which will contain the failed tests of that run. There was an issue with this file however. If you define multiple suites in your initial test suite file then this outer testng-failed.xml will contain the failed tests of the first run alone. The remaining suite's failed tests will be in the inner directories under their corresponding suite names.</p>\n<p>This testng-failed.xml will also inherit all the properties from the original test suite file. For example if we have defined thread-count value then the same value will be retained.</p>\n<p>So what I did was copied this file to a location and fed this to the testng task to run again. This can be achieved by any means, since we were using ant as our build tool i configured this in our build.xml file itself.</p>\n<pre><code class=\"xml language-xml\">    &lt;target name=\"runTests\" depends=\"compile\" description=\"Running tests\"&gt;\n        &lt;echo&gt;Running Tests...&lt;/echo&gt;\n        &lt;taskdef resource=\"testngtasks\" classpath=\"lib/testng-6.8.jar\" /&gt;\n        &lt;testng outputDir=\"${report.dir}\" useDefaultListeners=\"true\" classpathref=\"build.classpath\"\n            listeners=\"org.uncommons.reportng.HTMLReporter,org.uncommons.reportng.JUnitXMLReporter\"&gt;\n            &lt;classpath location=\"${class.dir}\" /&gt;\n            &lt;xmlfileset dir=\".\" includes=\"testng-suite.xml\" /&gt;\n            &lt;sysproperty key=\"org.uncommons.reportng.title\" value=\" Test report\" /&gt;\n            &lt;sysproperty key=\"properties\" value=\"${properties}\" /&gt;\n        &lt;/testng&gt;\n        &lt;copy file=\"${report.dir}/testng-failed.xml\" todir=\"${basedir}/test-output-rerun/0\"/&gt;\n\n        &lt;antcall target=\"multiReRun\"/&gt;\n    &lt;/target&gt;\n    &lt;target name=\"multiReRun\" description=\"Multiple rerun tests\"&gt;\n        &lt;antcall target=\"runFailedTests\"&gt;\n            &lt;param name=\"rerun.report.dir\" value=\"${rerun.base.dir}/1\"/&gt;\n            &lt;param name=\"src.rerun.dir\" value=\"${rerun.base.dir}/0\"/&gt;\n        &lt;/antcall&gt;\n        &lt;antcall target=\"runFailedTests\"&gt;\n            &lt;param name=\"rerun.report.dir\" value=\"${rerun.base.dir}/2\"/&gt;\n            &lt;param name=\"src.rerun.dir\" value=\"${rerun.base.dir}/1\"/&gt;\n        &lt;/antcall&gt;\n    &lt;/target&gt;\n</code></pre>\n<p>Here output folder of the init run will be test-output. And am copying testng-failed.xml from test-output to test-output-rerun/0 . This is just to make my multiReRun more convenient. Now i have repeated the block of code for two times. This is due to the fact that ant doesn't support the regular for..counter loop.</p>\n<p>When i went through some ant docs figured out that ant script supports JavaScript!! May be i can use that to constuct a string like \"1,2,3,4,5\" and pass it onto the for loop of ant.</p>",
            "url": "https://prasanna.dev/posts/re-running-testng-failed-tests",
            "title": "Re running testNG failed times n times.",
            "summary": "Steps to set up re-runn of testNG failed tests for n number of times, using ant task.",
            "date_modified": "2014-04-30T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/wiring-jasmine2-with-phantom",
            "content_html": "<p>Briefed out the steps that i did to run my Jasmine test suite in my CI.</p>\n<ol>\n<li>Download phantomjs.exe (Our CI server was running in a Windows server). <a href=\"http://phantomjs.org/download.html\">Download link</a></li>\n<li>Use <a href=\"https://gist.github.com/prasann/9972777\">run-jasmine.js</a>. This runner code is taken from phantomJS example and modified to run Jasmine 2.0 and to format the output as we needed.</li>\n<li>Assuming phantomjs.exe, run-jasmine.js and SpecRunner.html (Specrunner file) are in the same level in a directory, execute this command</li>\n</ol>\n<pre><code class=\"bash language-bash\"> phantomjs.exe run-jasmine.js SpecRunner.html \\[--debug\\]\n</code></pre>\n<p>The --debug is optional. If run on the debug mode it will print the stack trace of the failed specs and also prints all the specs that are been executed.</p>\n<p>SpecRunner.html is very similar to the one that comes along with Jasmine 2.0 samples. The SpecRunner.html that comes with Jasmine 1.3 will not work, as the way of booting Jasmine is changed in the latest version. The only changes I made to the SpecRunner.html is to modify my src and spec file locations.</p>",
            "url": "https://prasanna.dev/posts/wiring-jasmine2-with-phantom",
            "title": "Wiring Jasmine 2.0 with Phantom JS",
            "summary": "This post describes the steps that are necessary for to wire Jasmine 2.0 test suites with phantomJS.",
            "date_modified": "2014-04-28T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/utc-time-android-device-ntp-server-sync",
            "content_html": "<p>I had a requirement to persist the current UTC time of a request in Android device for future reference.</p>\n<p>Getting the time from the Android device and converting it to UTC will not be efficient since, user might have set wrong time in the device and it might mislead the data.<br />\nSo we decided to sync the device with NTP server before converting the time to UTC.</p>\n<p><strong>Step 1 :</strong> Copy this <a href=\"https://gist.github.com/prasann/9003350\" title=\"SntpClient.java\">SntpClient.java</a> into your source.<br />\n<strong>Step 2 :</strong> The SntpService.java to compute the current UTC is here below.</p>\n<pre><code class=\"java language-java\">public String getUTCTime(){\n        long nowAsPerDeviceTimeZone = 0;\n        SntpClient sntpClient = new SntpClient();\n        if (sntpClient.requestTime(\"0.africa.pool.ntp.org\", 30000)) {\n            nowAsPerDeviceTimeZone = sntpClient.getNtpTime();\n            Calendar cal = Calendar.getInstance();\n            TimeZone timeZoneInDevice = cal.getTimeZone();\n            int differentialOfTimeZones = timeZoneInDevice.getOffset(System.currentTimeMillis());\n            nowAsPerDeviceTimeZone -= differentialOfTimeZones;\n        }\n        return DateUtils.getFormattedDateTime(new Date(nowAsPerDeviceTimeZone));\n    }\n</code></pre>\n<p>Some more details on SntpService code:</p>\n<p>Connect to any of the prominent ntp servers. There were lots of recommendation to place this in config file, however i thought it doesn't make sense for Android since i have to repackage this anyways.</p>\n<pre><code>sntpClient.getNtpTime()\n</code></pre>\n<p>gives you the current NTP time as per the device time zone.</p>\n<p>Then identify the device's time zone,</p>\n<pre><code>cal.getTimeZone()\n</code></pre>\n<p>and calculate the offset difference between UTC and the current device time.</p>\n<pre><code>DateUtils.getFormattedDateTime(date)\n</code></pre>\n<p>is our custom method to format date into String.</p>",
            "url": "https://prasanna.dev/posts/utc-time-android-device-ntp-server-sync",
            "title": "UTC time in Android device. With NTP server sync.",
            "summary": "Using NTP time in the anroid application. This involves calling the SNTP server and also converting the time to UTC format.",
            "date_modified": "2014-02-13T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/whats-new-in-apple-passbook-ios7",
            "content_html": "<p>I'm currently working in a project that integrated with iOS Passbook application to deliver digital passes to their users. Spent some time in investigating the changes to passbook application in iOS7 (beta 4). Here is a quick summary of it.</p>\n<p>Apple listened to its user's feedback and have come with some features which increases the usability of the application.</p>\n<p><strong>Add multiple passes to Passbook.</strong></p>\n<p>Yes, you can able to add multiple passes to the Passbook application in one go. So, if you are issuing multiple passes to your users, probably have a page with all the passes and you can have download all link, which downloads all the passes in one shot.</p>\n<p><strong>Delivery through Barcode.</strong></p>\n<p>Apple have added one more delivery mechanism to the passbook. Currently you can deliver a pass through Safari browser (with vnd.apple.pkpass as header)&nbsp;or Through email attachment or to stream from your native iOS application. Now in the new passbook app they have added a barcode scanner. So the content of the pass can be crisped into a barcode and can be delivered to the Passbook application.</p>\n<p><strong>Anchor tags at the back of the pass.</strong></p>\n<p>Currently you can have links, but you cannot have link text. For example if you want to link <a href=\"http://google.com\">http://www.google.com</a> to 'Click here' it is not possible. But it will be possible from iOS7</p>\n<p><strong>Expiration date for Passes.</strong></p>\n<p>Now passes can have its own expiration date. It's a meta data that you can set and after that the passes will be destroyed from the Apple passbook automatically.</p>\n<p><strong>Usage restriction by Geo location.</strong></p>\n<p>You can restrict the usage of passes, within a specific geo location. For example if you are issuing a coupon, then you can make sure that your users could able to access the coupons within a specific geo co-ordinates.</p>\n<p>All these are nice features that are provided in iOS7. So far i was happy with all these news, until i read that, (<a href=\"https://devforums.apple.com/thread/190987?tstart=0\">Dev forum link</a>)</p>\n<p><strong>The rendering algorithms are significantly different from the previous versions.</strong></p>\n<p>This seems to be a major issue to me, since i have to now test the appearance of my pass with new version. And have to think about optimising the design across all the versions.</p>",
            "url": "https://prasanna.dev/posts/whats-new-in-apple-passbook-ios7",
            "title": "What's new in Apple Passbook iOS7",
            "summary": "New features in passbook application in the iOS7.",
            "date_modified": "2013-08-06T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/setting-up-cucumber-jvm",
            "content_html": "<p><a href=\"https://github.com/cucumber/cucumber-jvm\" title=\"Cucumber JVM\">Cucumber JVM</a> is Java implementation of Cucumber BDD.</p>\n<h3 id=\"integratingintotheproject\"><strong>Integrating into the Project</strong></h3>\n<p>The installation using maven is super simple, just add the dependency and you are ready to go. Make sure you add both command line interface (cucumber-core) and the IDE interface (cucumber-junit)</p>\n<p>I was using Intellij and add Intellij Cucumber plugin, to make the navigations easier.</p>\n<p>One thing i liked very much is the ability to add custom annotations to the feature. You can add a custom annotation and can create Before and After hook for them.</p>\n<p>In <code>.feature</code> file</p>\n<pre><code>@Email\nFeature:\n</code></pre>\n<p>In the step definitions file.</p>\n<pre><code>@Before({\"@Email\"})\n@After({\"@Email\"})\n</code></pre>\n<h3 id=\"integratingwithspring\"><strong>Integrating with Spring</strong></h3>\n<p>For Spring integration you need to add one more component of the cucumber-jvm&nbsp;<em>(cucumber-spring)</em></p>\n<p>It is advisable to have a test runner class which can run all the feature files in one go especially when you are runnning in the CI.</p>\n<p>The structure of the test runner class will be :</p>\n<pre><code class=\"java language-java\">@RunWith(Cucumber.class)\npublic class CucumberAdapterTest {\n}\n</code></pre>\n<p>Make sure to place all the feature files in the same package as of this Runner class. Or you can specify the path using the cucumber options, like this.</p>\n<pre><code class=\"java language-java\">@RunWith(Cucumber.class)\n@Cucumber.Options(features = \"classpath:\\*\\*/\\*.feature\")\npublic class CucumberAdapterTest {\n}\n</code></pre>\n<p>If you are placing all the step definition in other package you can add that to the annotation using glue attribute.</p>\n<pre><code class=\"java language-java\">@RunWith(Cucumber.class)\n@Cucumber.Options(features = \"classpath:\\*\\*/\\*\", glue = {\"path of the step definitions\"})\npublic class CucumberAdapterTest {\n}\n</code></pre>\n<p>This will look up for cucumber.xml file in the classpath. This xml file can hold all the bean definitions. My cucumber.xml was super simple.</p>\n<pre><code class=\"xml language-xml\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;beans xmlns=\"http://www.springframework.org/schema/beans\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxmlns:context=\"http://www.springframework.org/schema/context\"\nxsi:schemaLocation=\"http://www.springframework.org/schema/beans\nhttp://www.springframework.org/schema/beans/spring-beans-3.0.xsd\nhttp://www.springframework.org/schema/context\nhttp://www.springframework.org/schema/context/spring-context-3.0.xsd\"&gt;\n\n    &lt;import resource=\"classpath\\*:/application-context.xml\"/&gt;\n\n    &lt;context:component-scan base-package=\"path of the step definition\"/&gt;\n    &lt;context:annotation-config/&gt;\n&lt;/beans&gt;\n</code></pre>\n<p>The step defnitions can lie in a different package and make sure you use glue attribute to wire them in the Runner class.</p>\n<pre><code class=\"java language-java\">public class StepDefinitions {\n@Autowired\nEntityRepository entityRepository;\n\n    @Given(\"^Register a user$\")\n    public void registerUser() throws Throwable {\n\n    }\n}\n</code></pre>\n<h3 id=\"integratingwithspringtransactions\"><strong>Integrating with Spring Transactions</strong></h3>\n<p>One last thing that i wanted to do is to hook up Spring transactions. So all the data created by the features have to be removed after the test completes. So you can write independent tests without bothering about the data.</p>\n<p>You can use '<em>txn</em>' annotation that comes with Cucumber-JVM. All you need to do is to wire up that package along with your adapter class.</p>\n<pre><code class=\"java language-java\">@RunWith(Cucumber.class)\n@Cucumber.Options(glue = {\"cucumber.api.spring\"})\npublic class CucumberAdapterTest {\n}\n</code></pre>\n<p>and</p>\n<pre><code>@txn\nScenario: Some scenario to test\n</code></pre>",
            "url": "https://prasanna.dev/posts/setting-up-cucumber-jvm",
            "title": "Setting up Cucumber-jvm",
            "summary": "Setting up cucumber BDD framework in your Java project.",
            "date_modified": "2013-07-09T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/link-your-sublime-text-2-instances-with-dropbox",
            "content_html": "<p>After started using <strong>Sublime Text 2 (ST2)</strong>, was completely head over heels for it. After using it for some time, figured out that ST2 uses a settings file to remember the plugins, open tabs etc. I was using my laptop (Mac) and sometimes uses my PC (Windows) do type blog post or some other stuff. So thought of giving a try in syncing these two using Dropbox folder.</p>\n<p>The idea is to have the settings file in the Dropbox folder and to have a symlink in the OS to point to. This means whatever changes i did to a ST2 instance will reflect in my other instance too.</p>\n<p><strong>Prerequisites:</strong></p>\n<ul>\n<li>Install ST2 in both the machines.</li>\n<li>Have a DropBox account and install the software in both the machines.</li>\n</ul>\n<h3 id=\"inmac\"><strong>In Mac:</strong></h3>\n<p>Move the entire Sublime Text folder into the Dropbox folder.</p>\n<pre><code class=\"bash language-bash\">mv '~/Library/Application\\\\ Support/Sublime\\\\ Text\\\\ 2/' '~/Dropbox/Sublime\\\\ Text\\\\ 2'\n</code></pre>\n<p>Next step is to create a symlink in the original location so that it the folder in the Dropbox will be used to store the settings.</p>\n<pre><code class=\"bash language-bash\">ln -s '~/Library/Application\\\\ Support/Sublime\\\\ Text\\\\ 2' '~/Dropbox/Sublime\\\\ Text\\\\ 2'\n</code></pre>\n<h3 id=\"inwindows\"><strong>In Windows:</strong></h3>\n<p>Windows don't have symlink concept. So we have to settle with <a href=\"http://en.wikipedia.org/wiki/NTFS_symbolic_link\" title=\"NTFS Junction Point\">NTFS symbolic link</a></p>\n<pre><code class=\"bash language-bash\">mklink /J 'C:/Users/user\\_name/Dropbox/Sublime Text 2' 'C:/User/user\\_name/Applications/Sublime Text 2'\n</code></pre>\n<p>That's it, now you don't need to worry about the sync between these two instances.</p>",
            "url": "https://prasanna.dev/posts/link-your-sublime-text-2-instances-with-dropbox",
            "title": "Link your Sublime Text 2 instances with Dropbox",
            "summary": "Sublime text has a key mapping file where it stores all the shortcut. Here is a way to share your preferences and key maps across two machines using a dropbox account.",
            "date_modified": "2013-01-09T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/scribblings-socket-io",
            "content_html": "<p>I was trying my hands on socket.io. On my first glance it looked extremely simple to get going.&nbsp;The app i was working was on node.js, so i had no trouble in including socket.io into my project.</p>\n<p>My app had client and server component. For the server component i could able to do the npm install and got the socket.io working. Whereas for the client component i couldn't able to find the stand alone js available for download. Basically the js comes in along with the npm which means you got to take it out separately if you want to use it. Then i used the js file from their <a href=\"https://github.com/learnboost/socket.io\">Github repo</a>.</p>\n<h2 id=\"bydefaultsocketiodoesntperformbroadcast\">By default Socket.io doesn't perform broadcast</h2>\n<p>This is my first learning. Though it seems to be obvious after taking a good look onto the API, it wasn't very clear for me in the beginning.</p>\n<p><em>For example:</em></p>\n<p><strong>Server</strong></p>\n<pre><code class=\"js language-js\">var app = require(\"express\")(),\n  server = require(\"http\").createServer(app),\n  io = require(\"socket.io\").listen(server);\n\nserver.listen(80);\n\nio.sockets.on(\"connection\", function (socket) {\n  socket.on(\"first_msg\", function (data) {\n    socket.emit(\"reply\", {\n      hello: \"world\",\n    });\n  });\n});\n</code></pre>\n<p><strong>Client 1:</strong></p>\n<pre><code class=\"js language-js\">&lt;script src = \"/socket.io/socket.io.js\" &gt; &lt;/script&gt;\n&lt;script&gt;\n    var socket = io.connect('http:/ / localhost ');\n    socket.emit('first\\_msg ', { my: 'data1 ' });\n&lt;/script&gt;\n</code></pre>\n<p><strong>Client 2:</strong></p>\n<pre><code class=\"js language-js\">&lt;script src = \"/socket.io/socket.io.js\" &gt; &lt;/script&gt;\n&lt;script&gt;\n    var socket = io.connect('http:/ / localhost ');\n    socket.on('reply ', function (data){\n        console.log(\"Client1 had pinged server.\");\n    }\n&lt;/script&gt;\n</code></pre>\n<p>In this case i was expecting my <em>Client2</em> console.log to execute but that never happened. Reason being whenever <em>Client1</em> emits '<em>first</em>msg_' it was _Client1_ who was receiving the reply too (obvious i know !!).</p>\n<p>So in these cases socket.io provides an API to broadcast messages.Hence instead of</p>\n<pre><code class=\"js language-js\">socket.emit(\"reply\", { hello: \"world\" });\n</code></pre>\n<p>it should have been</p>\n<p>socket.broadcast.emit('reply', { hello: 'world' });</p>\n<h2 id=\"exposedeventsinsocketioarejustdefinedforsocketonmethods\">Exposed events in socketIO are just defined for socket.on methods</h2>\n<p>I was trying to emit a custom message from my client. I need to perform some actions on its success and failure. Now i need to attach success and error callbacks. For this i found this <a href=\"https://github.com/LearnBoost/socket.io/wiki/Exposed-events\" title=\"Exposed Events\">Exposed events</a> doc. The funda is that all these exposed events are defined only for socket.on which means while emitting a message i cannot bind any callbacks to it.</p>\n<p>For error callback it is straight forward. We have</p>\n<pre><code class=\"js language-js\">socket.on('error', () -&gt; console.log(\"Error Occured\"))\n</code></pre>\n<p>which can be bound on the socket so whenever an error is been thrown on the socket the defined behaviour gets executed.</p>\n<p><strong>Client</strong> emits the custom message and sends JSON data to the socket via socket.emit, also he gets an update function that handles the success callback</p>\n<pre><code class=\"js language-js\">socket.emit(\"message\", { hello: \"world\" });\nsocket.on(\"messageSuccess\", function (data) {\n  //do something\n});\n</code></pre>\n<p><strong>Server</strong>-side Gets a call from the message emit from the client and emits the messageSuccess back to the client</p>\n<pre><code class=\"js language-js\">socket.on(\"message\", function (data) {\n  io.sockets.emit(\"messageSuccess\", data);\n});\n</code></pre>",
            "url": "https://prasanna.dev/posts/scribblings-socket-io",
            "title": "Scribblings on Socket.io",
            "summary": "When i tried out socket.io for the first time, it was quite an interesting learning of few new paradigms and techniques.",
            "date_modified": "2012-12-03T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/hamcrest-conflict-junit",
            "content_html": "<p>Using jUnit 4.8 i got into an issue when testing the few <a href=\"http://code.google.com/p/lambdaj/\" title=\"LambdaJ\">lambda</a> expressions. I was constantly getting this error while testing code involving Hamcrest matchers.</p>\n<p>java.lang.NoSuchMethodError: org.hamcrest.core.AllOf.allOf(Lorg/hamcrest/Matcher;Lorg/hamcrest/Matcher;)Lorg/hamcrest/Matcher;</p>\n<p>However when i ran the application it was all fine. So i could sense the issue with the jUnit.</p>\n<p>The problem is due to hamcrest versioning issue with jUnit. jUnit uses an old version while other libraries (in my case LambdaJ) was using the latest version. The fix will be to download the junit-dep-4.*.jar from the jUnit <a href=\"https://github.com/KentBeck/junit/downloads\" title=\"gitHub kent beck\">download</a> page. Since you app already have Hamcrest class the test will run smoothly.</p>",
            "url": "https://prasanna.dev/posts/hamcrest-conflict-junit",
            "title": "Hamcrest conflict in jUnit.",
            "summary": "Hamcrest matchers are used in jUnit for assertions. There is a weird problem with the version conflict between Hamcrest and jUnit. The solution is been discussed here.",
            "date_modified": "2012-06-26T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/handle-maxuploadsizeexceededexception-spring",
            "content_html": "<p>I was doing a AJAX file upload using jQuery and Spring 3. Spring provides a way to limit the file being uploaded and this can be configured while creating the multipart bean by specifying the maxUploadSize parameter.</p>\n<p>So whenever an user tries to upload a file with size size greater than that of the specified one then Spring will throw <em>'MaxUploadSizeExceededException'</em> exception and returns back. The problem for me is that i was doing the file upload using AJAX so i wanted a custom error to be thrown rather than the Spring's default error.</p>\n<p>And also because of this exception the control will not even reach your specified controller so there is no chance to catch it in your Controller.&nbsp;After some lookup i found this simple fix for it.</p>\n<p>FileUploadController: Controller which will handle the file upload request.</p>\n<p><em><strong>Make this FileUploadController to implement HandlerExceptionResolver. This will force you to define resolveException() method.</strong></em></p>\n<pre><code class=\"java language-java\">@ResponseBody\npublic ModelAndView resolveException(HttpServletRequest httpServletRequest,\n        HttpServletResponse httpServletResponse, Object o, Exception e) {\n    if (e instanceof MaxUploadSizeExceededException) {\n        ModelAndView modelAndView = new ModelAndView(\"inline-error\");\n        modelAndView.addObject(\"error\",\n        \"Error: Your file size is too large to upload. Please upload a file of size &lt; 5 MB and  continue. \");\n    return modelAndView;\n    }\n    e.printStackTrace();\n    return new ModelAndView(\"500\");\n}\n</code></pre>\n<p><strong>&nbsp;How to show the error on the same page:</strong></p>\n<p>The call to the controller is from a jQuery ajax method. But the problem here is that even with this approach your jQuery POST method is going to receive a HTTP_OK message from the controller. Hence if you are waiting at the error callback then you have no chance of catching this error.</p>\n<p>So what i have done here is to return inline-error view back as the response. On the success callback of the jQuery i check for the presence of the error_div in the response and display the field in the page. Else show the success message.</p>\n<p><em>inline-error.jsp</em></p>\n<pre><code class=\"html language-html\">&lt;div class=\"error\" id=\"error\\_div\"&gt;${error}&lt;/div&gt;\n</code></pre>\n<p><em>PS: This is definitely not the cleanest approach, but this solved my problem :)</em></p>",
            "url": "https://prasanna.dev/posts/handle-maxuploadsizeexceededexception-spring",
            "title": "Handle MaxUpload SizeExceededException in Spring",
            "summary": "Handling MaxUploadExceedException in Ajax call with Spring controllers. This exception occurs when the file size greate than what is expected is been uploaded by the user.",
            "date_modified": "2012-06-23T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/make-ntfs-write-mac-lion",
            "content_html": "<p>Just now i got my new Mac book pro, pre loaded with Lion OSX and one of the surprises i stepped onto is NTFS write issue on the Mac.</p>\n<p>Based on a few blog posts and comments I managed to find a way that worked for me, so I thought I’d put it all here in one place for others.</p>\n<p><strong>Pre requisites:</strong></p>\n<p>Get <a href=\"http://mxcl.github.com/homebrew/\" title=\"homebrew\">HomeBrew</a> installed in your machine. And of course this needs XCode tools to be installed.</p>\n<p><strong>Steps:</strong></p>\n<ol>\n<li>Install latest Fuse4X (a fork of MacFUSE) and NTFS-3G packages:</li>\n</ol>\n<pre><code class=\"bash language-bash\">brew install fuse4x\nbrew install ntfs-3g\n</code></pre>\n<ol start=\"2\">\n<li>Type brew info fuse4x-kext in the terminal. You will be shown a message similar to this:</li>\n</ol>\n<p>In order for FUSE-based filesystems to work, the fuse4x kernel extension\nmust be installed by the root user:</p>\n<pre><code class=\"bash language-bash\">sudo cp -rfX /usr/local/Cellar/fuse4x-kext/0.8.13/Library/Extensions/fuse4x.kext /System/Library/Extensions\nsudo chmod +s /System/Library/Extensions/fuse4x.kext/Support/load\\_fuse4x\n</code></pre>\n<p>Perform both the operation. 3) And after this i simply followed this <a href=\"http://fernandoff.posterous.com/ntfs-write-support-on-osx-lion-with-ntfs-3g-f\">blog post entry</a>. Since you have already installed Fuse4x and ntfs-3g you can directly jump to</p>\n<blockquote>\n  <p>\"Ok, at this point you should have a functional fuse4x and ntfs-3g install.\"</p>\n</blockquote>\n<p>and create an alternative</p>\n<pre><code class=\"bash language-bash\">/sbin/ntfs\\_mount\n</code></pre>\n<p>script as described there.<br />\nAnd at last you got make one change to get things working.<br />\nThe script in the bog post is for MacPort users. For HomveBrew users you got to make this change.<br />\nreplace</p>\n<pre><code class=\"bash language-bash\">/opt/local/bin/ntfs-3g\n</code></pre>\n<p>with</p>\n<pre><code class=\"bash language-bash\">/usr/local/bin/ntfs-3g\n</code></pre>\n<p>And that's it. Just try mounting a NTFS drive and you should have write permissions to your drive. If you face any issues check out the log @</p>\n<pre><code class=\"bash language-bash\">/var/log/ntfsmnt.log\n</code></pre>\n<p>or try re-booting the machine in the worst case.</p>",
            "url": "https://prasanna.dev/posts/make-ntfs-write-mac-lion",
            "title": "Make your NTFS drive writable under Mac Lion",
            "summary": "Make your NTFS drive writabe under mac lion.",
            "date_modified": "2011-12-26T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/slot-machine-effect-jquery",
            "content_html": "<p><strong>Slot Machine Effect:</strong></p>\n<p><strong>Requisite:</strong></p>\n<ul>\n<li>jQuery 1.5+</li>\n</ul>\n<p><strong>Idea:</strong></p>\n<ul>\n<li>Create an element to display the animation.</li>\n<li>Next create an empty element say <div></div> and set its position: 'fixed'</li>\n<li>Set the position of the empty element to the Start Value of the slot.</li>\n<li>Now use jQuery animate to move the empty element from Start value to the specified End Value in a given duration.</li>\n<li>jQuery animate has a step() method which gives you the current position of the div for every unit of time.</li>\n<li>Now inside this step() method set the display element's text to the current position value of the empty element.</li>\n<li>Since empty element moves from start value to end value, you will see the numbers changing from start value to end value in the display area.</li>\n</ul>\n<p><strong>Javascript Code:</strong></p>\n<pre><code class=\"js language-js\">$('#animate\\_btn').click(function() {\n    cashFlow($('.value'), $('#startVal').val(), $('#endVal').val(),\n        $('#duration').val() \\* 1000, $('#decimal').val());\n});\n\ncashFlow = function(elem, from, to, duration, decimal) {\n    var magicObject;\n    if (typeof magicObject === 'undefined') {\n        magicObject = $('&lt;div&gt;&lt;/div&gt;').appendTo('body');\n    }\n    magicObject.css({\n        position: \"fixed\",\n        left: from\n    }).animate({\n        left: to\n    }, {\n        duration: duration,\n        step: function(currentLeft) {\n            elem.html(Number(currentLeft).toFixed(decimal));\n        },\n        complete: function() {\n            magicObject.remove();\n        }\n    });\n};\n</code></pre>",
            "url": "https://prasanna.dev/posts/slot-machine-effect-jquery",
            "title": "Slot machine effect using jQuery",
            "summary": "A cool widget that looks like a slot machine. Small piece of code and a nice trick to animate the numbers to achieve a slot machine effect.",
            "date_modified": "2011-09-24T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/writing-custom-tags-for-jstls",
            "content_html": "<p>First start with writing a tag library descriptor(TLD). A TLD is a XML document that contains information about a library as a whole and about each tag contained in the library.<br />\nThe structure of the TLD file is pretty readalbe.</p>\n<p>Below is an implementation of tag which takes in a section name(value) of a web page and checks whether the logged-in user has rights to view the section.</p>\n<p><strong>Step 1:</strong> custom.tld</p>\n<pre><code class=\"xml language-xml\">&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\" ?&gt;\n&lt;!DOCTYPE taglib PUBLIC \"-//Sun Microsystems, Inc.//DTD JSP Tag Library 1.1//EN\"\n        \"http://java.sun.com/j2ee/dtds/web-jsptaglibrary\\_1\\_1.dtd\"&gt;\n&lt;taglib xmlns=\"http://java.sun.com/j2ee/dtds/web-jsptaglibrary\\_1\\_1.dtd\"&gt;\n    &lt;tlibversion&gt;1.0&lt;/tlibversion&gt;\n    &lt;jspversion&gt;1.1&lt;/jspversion&gt;\n    &lt;shortname&gt;custom&lt;/shortname&gt;\n    &lt;info&gt;Custom tag library&lt;/info&gt;\n    &lt;tag&gt;\n        &lt;name&gt;permission&lt;/name&gt;\n        &lt;tagclass&gt;com.prasans.PermissionTag&lt;/tagclass&gt;\n        &lt;info&gt;\n            Checks the User Permission to access the content.\n        &lt;/info&gt;\n        &lt;attribute&gt;\n            &lt;name&gt;value&lt;/name&gt;\n            &lt;required&gt;true&lt;/required&gt;\n        &lt;/attribute&gt;\n        &lt;attribute&gt;\n            &lt;name&gt;invertCondition&lt;/name&gt;\n            &lt;required&gt;false&lt;/required&gt;\n        &lt;/attribute&gt;\n    &lt;/tag&gt;\n&lt;/taglib&gt;\n</code></pre>\n<p>Here we have implemented a tag called permission within the 'custom' tag library.</p>\n<p>Usage: <em><custom:permission value=\"\">{section}</custom:permission></em><br />\nSimilarly you can add more tags to your library by adding more <tag> nodes.</p>\n<p>After done with defining TLD, next step is to implement the conditional logic. Below is a piece of Java code that does the implementation of the TLD.</p>\n<p><strong>Step 2:</strong> PermissionTag.java</p>\n<pre><code class=\"java language-java\">package com.prasans;\n\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.jsp.jstl.core.ConditionalTagSupport;\n\npublic class PermissionTag extends ConditionalTagSupport {\nprivate String value = null;\nprivate boolean invertCondition;\n\n    public void setValue(String value) {\n        this.value = value;\n    }\n\n    public String getValue() {\n        return value;\n    }\n\n    public boolean isInvertCondition() {\n        return invertCondition;\n    }\n\n    public void setInvertCondition(boolean invertCondition) {\n        this.invertCondition = invertCondition;\n    }\n\n    @Override\n    protected boolean condition() {\n        // If needed you can access Request Object like this.\n        HttpServletRequest request = (HttpServletRequest) pageContext.getRequest();\n        boolean permission = checkForThePermission(value);\n        return invertCondition ? !permission : permission;\n    }\n}\n</code></pre>\n<p><strong>Explanation:</strong></p>\n<p>* Since the expectation of this tag is to return true or false, we are extending the&nbsp;<em>ConditionalTagSupport</em> class. Based on the need you can choose upon your class implementation.</p>\n<p>*Note that all tag attributes are member variables of the class and all of them should have getters and setters.</p>\n<p>*Here we have overridden the condition() from ConditionalTagSupport to return the needed boolean result.</p>\n<p>* InvertCondition is an attribute that helps us in simulating negative scenarios.</p>\n<p>For ex: \"Show the section <em>If User A do not have 'X' permission</em>\"</p>\n<p>After building the TLD and its corresponding logic, the next step is to integrate with your application.<br />\nAdd the custome tag library to your web.xml to integrate with your web app.</p>\n<p><strong>Step 3:</strong> web.xml</p>\n<pre><code class=\"xml language-xml\">&lt;jsp-config&gt;\n    &lt;taglib&gt;\n        &lt;taglib-uri&gt;/custom&lt;/taglib-uri&gt;\n        &lt;taglib-location&gt;/WEB-INF/tags/custom.tld&lt;/taglib-location&gt;\n    &lt;/taglib&gt;\n&lt;/jsp-config&gt;\n</code></pre>\n<p>The taglib-uri is the <em><shortname></em> defined in the TLD file. And <em><taglib-location></em> is the location of the tld. Make sure that you are bundling the TLD along with your WAR.</p>\n<p>Thats it. You can start using your custom tags in your JSPs now.</p>",
            "url": "https://prasanna.dev/posts/writing-custom-tags-for-jstls",
            "title": "Writing Custom Tags for JSTLs",
            "summary": "Writing a custom JSTL tag and integrating with the application. A sample code to do the same.",
            "date_modified": "2011-09-10T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/reloading-an-activity-in-android",
            "content_html": "<p>More often i wanted to reload an activity to refresh the contents of an page. I have seen many ways to do this in Android world and i always puzzled about the best approach.</p>\n<p>However these are some of the approaches that i took . I found the following two approaches a lot cleaner as they kill the existing intent and restart.</p>\n<p><strong>Approach 1:</strong></p>\n<pre><code class=\"java language-java\">Intent intent = getIntent();\nfinish();\nstartActivity(intent);\n</code></pre>\n<p><strong>Approach 2:</strong></p>\n<pre><code class=\"java language-java\">Intent intent = getIntent();\noverridePendingTransition(0, 0);\nintent.addFlags(Intent.FLAG\\_ACTIVITY\\_NO\\_ANIMATION);\nfinish();\noverridePendingTransition(0, 0);\nstartActivity(intent);\n</code></pre>\n<p><em><strong>Note:</strong> The second approach works only from API 5+</em></p>",
            "url": "https://prasanna.dev/posts/reloading-an-activity-in-android",
            "title": "Reloading an activity in Android",
            "summary": "Refreshing or reloading activity in Android application. This might be important in the case of refreshing data in the activity.",
            "date_modified": "2011-07-12T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/configuring-context-name-for-an-applicationtomcat-war",
            "content_html": "<p>Many times i have faced this necessity of &nbsp;having a different Context name than that of the WAR name. Especially when i use Maven. And i also found that it is quite simple in Tomcat. While looking for it i found two approached for doing it. One using Context.xml and the other using Server.xml</p>\n<p><strong>Problem Statement:</strong> I have a WAR file myapp-build123.war and the app can be referred using <em>http://localhost:8080/myapp-build123</em> but i need to refer my application as <em>http://localhost:8080/myapp</em> So here is what im gonna do.</p>\n<p><strong>Approach 1: <em>Context.xml</em> (Recommended way)</strong></p>\n<ul>\n<li>Create a Context file in ${TOMCAT_HOME}/conf/Catalina/localhost directory. Name the file as myapp.xml (The file name should be the same as desired context name) The content of the file is given below.</li>\n</ul>\n<pre><code class=\"bash language-bash\">&lt;Context path=\"/somepath\" docBase=\"/home/myapp-build123\"/&gt;\n</code></pre>\n<p><em>Basically the path attribute is been ignored by Tomcat so if you want you can ignore it too.</em></p>\n<p><em>The docBase will contain the path of the WAR. Here i have placed my WAR file in the home directory.</em></p>\n<ul>\n<li><p>An important thing to note here is that the WAR file cannot be placed inside the ${TOMCAT<em>HOME}/webapps folder. If you place the WAR in the webapps folder then the war will get exploded with the same name as the WAR file and after that it is not possible to configure the Context name. So place the WAR anywhere in the system apart from ${TOMCAT</em>HOME}/webapps folder.</p></li>\n<li><p>Now its time to start the server and you will see a folder named myapp in the ${TOMCAT_HOME}/webapps folder, containing the application files.</p></li>\n</ul>\n<p><strong>Approach 2: Server.xml ( <a href=\"http://tomcat.apache.org/tomcat-6.0-doc/config/context.html\">Not Recommended</a> after Tomcat 4.x )</strong></p>\n<ul>\n<li>Open the Server.xml file from ${TOMCAT_HOME}/conf folder. Search for the Host tag and place the Context tag inside it.</li>\n</ul>\n<pre><code class=\"xml language-xml\">&lt;Host name=\"localhost\"  appBase=\"webapps\"\n    unpackWARs=\"true\" autoDeploy=\"true\"\n    xmlValidation=\"false\" xmlNamespaceAware=\"false\"&gt;\n    &lt;Context path=\"/myapp\" docBase=\"/myapp-build123\"/&gt;\n&lt;/Host&gt;\n</code></pre>\n<ul>\n<li>In this scenario you can place the WAR file in the ${TOMCAT_HOME}/webapps folder itself. And also it is possible to access the application by both the URLs,&nbsp;http://localhost:8080/myapp-build123 and http://localhost:8080/myapp.</li>\n</ul>",
            "url": "https://prasanna.dev/posts/configuring-context-name-for-an-applicationtomcat-war",
            "title": "Configuring Context name for an application",
            "summary": "Setting up a different context name than the WAR name for the Java application deployed in Tomcat server.",
            "date_modified": "2011-07-08T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/primary-key-onetoone-mapping-in-hibernate",
            "content_html": "<p>Today was trying to join two tables using its Primary keys using Hibernate. Here is what I tried to do:</p>\n<pre><code class=\"java language-java\">@Table(name = \"customer\")\n@Entity\npublic class Customer {\n    @Id\n    @OneToOne\n    @JoinColumn(name = \"customer\\_id\", updatable = false)\n    private Credentials credentials;\n}\n</code></pre>\n<p>I was constantly getting an error stating invalid column name. Later then I realized that its not possible have Join in the Primary-Key and bind to a custom object. This forced me to have a Auto-generated Id as a key to the table and named it as the primary key. This is how my code looked after modification.</p>\n<pre><code class=\"java language-java\">@Table(name = \"customer\")\n@Entity\npublic class Customer {\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private long id;\n\n    @OneToOne\n    @JoinColumn(name = \"customer\\_id\",updatable = false)\n    private Credentials credentials;\n}\n</code></pre>",
            "url": "https://prasanna.dev/posts/primary-key-onetoone-mapping-in-hibernate",
            "title": "Primary-key @OneToOne mapping in Hibernate",
            "summary": "Joining two tables using Primary keys in Hibernate.",
            "date_modified": "2011-05-17T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/play-age-of-empires-aoe-in-windows-7",
            "content_html": "<p>Due to an issue in the Windows 7 Aero theme , the map of the Age Of Empires (AOE) actually renders poorly. The playing area will be laid in patches and the small map in the bottom right won't show the correct player's color.</p>\n<p>The fix for this issue is pretty simple.</p>\n<ul>\n<li><p>Start the AOE game.</p></li>\n<li><p>Get back to your desktop without closing the game,(by pressing the Windows Key (or) Show Desktop key).</p></li>\n<li><p>Start your task manager . Select the Processes tab. Look for process explorer.exe and kill it.</p></li>\n<li><p>This will hide your desktop icons, taskbar and all other opened windows.</p></li>\n<li><p>Using Alt +Tab get back to the Game. Now the map will be rendered nicely. Enjoy Playing :)</p></li>\n<li><p>After finishing the game , Start Task Manager again. Choose File -&gt; New Task and type explorer.exe in the dialog box.</p></li>\n</ul>\n<p>*Everything is back to normal.</p>",
            "url": "https://prasanna.dev/posts/play-age-of-empires-aoe-in-windows-7",
            "title": "Age Of Empires (AOE) Fix in Windows 7",
            "summary": "Age of Empires (a popular strategy game) has got some problems with color rendering while played in Window 7. A quick harmless solution for that is here.",
            "date_modified": "2011-04-27T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/add-edittexts-dynamically-and-retrieve-values-android",
            "content_html": "<p>Adding EditText to your Android application is no different from adding any other form elements except for one thing. Retrieving values from them is slightly different and of course nothing impossible. Just a little more bit of coding and thats it.</p>\n<p><img src=\"/assets/posts/images/android-edittext.png\" alt=\"Android-EditText {307xx578}\" title=\"Android-EditText\" /></p>\n<p>Following code snippet creates a series of EditTexts and also let you to access its values.</p>\n<pre><code class=\"java language-java\">import android.app.Activity;\nimport android.os.Bundle;\nimport android.view.View;\nimport android.view.ViewGroup;\nimport android.widget.Button;\nimport android.widget.EditText;\nimport android.widget.LinearLayout;\nimport android.widget.TableLayout;\nimport android.widget.TableRow;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport static android.view.ViewGroup.LayoutParams.FILL\\_PARENT;\nimport static android.view.ViewGroup.LayoutParams.WRAP\\_CONTENT;\nimport static android.widget.LinearLayout.VERTICAL;\n\npublic class Sample extends Activity {\n    private List&lt;EditText&gt; editTextList = new ArrayList&lt;EditText&gt;();\n\n    @Override\n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n\n        LinearLayout linearLayout = new LinearLayout(this);\n        ViewGroup.LayoutParams params = new ViewGroup.LayoutParams(FILL\\_PARENT, WRAP\\_CONTENT);\n        linearLayout.setLayoutParams(params);\n        linearLayout.setOrientation(VERTICAL);\n\n        int count = 10;\n        linearLayout.addView(tableLayout(count));\n        linearLayout.addView(submitButton());\n        setContentView(linearLayout);\n    }\n\n    private Button submitButton() {\n        Button button = new Button(this);\n        button.setHeight(WRAP\\_CONTENT);\n        button.setText(\"Submit\");\n        button.setOnClickListener(submitListener);\n        return button;\n    }\n\n    // Access the value of the EditText\n\n    private View.OnClickListener submitListener = new View.OnClickListener() {\n        public void onClick(View view) {\n            StringBuilder stringBuilder = new StringBuilder();\n            for (EditText editText : editTextList) {\n                stringBuilder.append(editText.getText().toString());\n            }\n        }\n    };\n\n    // Using a TableLayout as it provides you with a neat ordering structure\n\n    private TableLayout tableLayout(int count) {\n        TableLayout tableLayout = new TableLayout(this);\n        tableLayout.setStretchAllColumns(true);\n        int noOfRows = count / 5;\n        for (int i = 0; i &lt; noOfRows; i++) {\n            int rowId = 5 \\* i;\n            tableLayout.addView(createOneFullRow(rowId));\n        }\n        int individualCells = count % 5;\n        tableLayout.addView(createLeftOverCells(individualCells, count));\n        return tableLayout;\n    }\n\n    private TableRow createLeftOverCells(int individualCells, int count) {\n        TableRow tableRow = new TableRow(this);\n        tableRow.setPadding(0, 10, 0, 0);\n        int rowId = count - individualCells;\n        for (int i = 1; i &lt;= individualCells; i++) {\n            tableRow.addView(editText(String.valueOf(rowId + i)));\n        }\n        return tableRow;\n    }\n\n    private TableRow createOneFullRow(int rowId) {\n        TableRow tableRow = new TableRow(this);\n        tableRow.setPadding(0, 10, 0, 0);\n        for (int i = 1; i &lt;= 5; i++) {\n            tableRow.addView(editText(String.valueOf(rowId + i)));\n        }\n        return tableRow;\n    }\n\n    private EditText editText(String hint) {\n        EditText editText = new EditText(this);\n        editText.setId(Integer.valueOf(hint));\n        editText.setHint(hint);\n        editTextList.add(editText);\n        return editText;\n    }\n}\n</code></pre>",
            "url": "https://prasanna.dev/posts/add-edittexts-dynamically-and-retrieve-values-android",
            "title": "Add EditText(s) dynamically and retrieve values - Android",
            "summary": "Adding multiple edit text boxes to the android application dynamically through code and controlling them.",
            "date_modified": "2011-03-21T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/test-your-controllers-modelattribute-methods",
            "content_html": "<p>I was about to write some unit tests around my Spring’s controller classes and also i wanted to write the test using MockHttpRequest and MockHttpResponse.</p>\n<p>My controller had a method to which i was using ModelAttribute as one of the parameter. I just want to simulate the same scenario in my Unit Tests.</p>\n<p>Unfortunately i could not see any methods in MockHttpRequest to help me with this. So i had to take a simple different approach as an workaround for this.</p>\n<p>My Controller code looks similar to this:</p>\n<pre><code class=\"java language-java\">@Controller\n@RequestMapping(value = \"/register\")\npublic class MyController {\n    @RequestMapping(value = \"/save\", method = RequestMethod.POST)\n    public ModelAndView save(@ModelAttribute User user) {\n        //Code to save the User object\n        return new ModelAndView();\n    }\n}\n</code></pre>\n<p>My Unit Tests:</p>\n<pre><code class=\"java language-java\">public class MyControllerTest {\nMockHttpServletResponse response;\nMockHttpServletRequest request;\nAnnotationMethodHandlerAdapter handler;\n\n    @Before\n    public final void init() {\n        response = new MockHttpServletResponse();\n        request = new MockHttpServletRequest();\n        handler = new AnnotationMethodHandlerAdapter();\n    }\n\n    @Test\n    public void shouldTestSaveUser() {\n        final User mockUser = new UserTestBuilder().withName(\"John\").build();\n        request.setMethod(\"POST\");\n        request.setRequestURI(\"/register/save\");\n\n        MyController myController = new MyController() {\n            @ModelAttribute\n            public User mockModel() {\n                return user;\n            }\n        }\n        ModelAndView model = handler.handle(request, response, myController);\n    }\n}\n</code></pre>\n<p><em><strong>Explanation:</strong></em></p>\n<p>Whenever a method in a controller is annotated with @ModelAttribute , it will be invoked for every request made to that controller. So while creating the mycontroller object i am overriding a sample method which has this annotation and returns a User object as a ModelAttribute.</p>",
            "url": "https://prasanna.dev/posts/test-your-controllers-modelattribute-methods",
            "title": "Test your Controller's ModelAttribute methods.",
            "summary": "Injecting a ModelAttribute to the controller's method in Unit tests using Spring and jUnit.",
            "date_modified": "2011-01-19T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/hadoop-version-in-aws-map-reduce",
            "content_html": "<p>Creating job flows using AWS MapReduce's GUI is pretty simple and very straight forward. But i wanted to use Java SDK to create/run jobs in MapReduce. I could successfully able set up the job and configured all the parameters except for a weird error.</p>\n<pre><code class=\"java language-java\">java.lang.NoSuchMethodError:\norg.apache.hadoop.mapred.JobConf.\nsetBooleanIfUnset(Ljava/lang/String;Z)\n</code></pre>\n<p>I was constantly getting this error while running the job. Initially i had no idea why this error occurs and none of the forum talks about it either. Then i figured out that the default Hadoop version used by the Ec2 instances was 0.18 and i was expecting 0.20. Interestingly i didn't face this issue when i did it through GUI.</p>\n<p>As a solution i need to explicitly set the version number as 0.20 to the Instances object so that it will use the same while running the job.</p>\n<pre><code class=\"java language-java\">JobFlowInstancesConfig instances = new JobFlowInstancesConfig();\ninstances.setHadoopVersion(\"0.20\");\n</code></pre>",
            "url": "https://prasanna.dev/posts/hadoop-version-in-aws-map-reduce",
            "title": "Hadoop Version in AWS Map Reduce",
            "summary": "Performing Map Reduce operation using Amazon AWS interface.",
            "date_modified": "2010-11-14T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/sax-parser-characters-method",
            "content_html": "<p>Was playing around SAX parsing some Gigs of XML file 😅 Here are few learnings from the game.</p>\n<p>My intention was to read values between a corresponding tag. I initially went after using characters() in SAX parser which actually worked fine for initial feeds. But as i keep increasing the size of the XMLs and if the size of the tagContent was large the problem arises. characters() function not always gives back the entire value in a single shot. It may return the value in multiple chunks. So need to be careful in assigning and using the values from characters() method.</p>\n<p>So the better way to use characters() method is to keep appending all the values to a buffer and use the value in the corresponding end tag section. Also need to make sure that the buffer has to be flushed in the corresponding start element.</p>\n<p><strong>Sample Xml:</strong></p>\n<pre><code class=\"xml language-xml\">&lt;Sample&gt;\n    &lt;StudentCollection&gt;\n        &lt;Student&gt;\n            &lt;Name&gt;Jack&lt;/Name&gt;\n            &lt;Age&gt;12&lt;/Age&gt;\n        &lt;/Student&gt;\n        &lt;Student&gt;\n            &lt;Name&gt;Jill&lt;/Name&gt;\n            &lt;Age&gt;13&lt;/Age&gt;\n        &lt;/Student&gt;\n        &lt;Student&gt;\n            &lt;Name&gt;Rose&lt;/Name&gt;\n            &lt;Age&gt;14&lt;/Age&gt;\n        &lt;/Student&gt;\n    &lt;/StudentCollection&gt;\n&lt;/Sample&gt;\n</code></pre>\n<p><strong>Sample SAX handler code to print the Names:</strong></p>\n<p><strong>Initial Code: (Works fine for small values & small files)</strong></p>\n<pre><code class=\"java language-java\">public void startElement(String uri, String tag, String qName, Attributes attributes) {\n}\n\npublic void characters(char ch[], int start, int length) {\n  System.out.println(\"Name of a student: \" + new String(ch, start, length));\n}\n\npublic void endElements(String uri, String tag, String qName) {\n}\n</code></pre>\n<p><strong>Final Code:</strong></p>\n<pre><code class=\"java language-java\">public void startElement(String uri, String tag, String qName, Attributes attributes) {\n  if (\"Name\".equals(tag)) {\n    tagContentBuffer = new StringBuffer();\n  }\n}\n\npublic void characters(char ch[], int start, int length) {\n  tagContentBuffer.append(new String(ch, start, length));\n}\n\npublic void endElements(String uri, String tag, String qName) {\n  if (\"Name\".equals(tag)) {\n    System.out.println(\"Name of a student: \" +\n       tagContentBuffer.toString());\n  }\n}\n</code></pre>",
            "url": "https://prasanna.dev/posts/sax-parser-characters-method",
            "title": "SAX parser characters() method.",
            "summary": "Implementing `characters()` method for SAX parsing huge files. With a sample code in Java",
            "date_modified": "2010-10-06T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/ftp-client-ubuntu",
            "content_html": "<p>Today was about to download multiple files from an authenticated FTP server. The native ftp client of Ubuntu didn't help me as expected. I was trying to log into the FTP server and was constantly getting disconnected when trying to perform some operation. When browsed for some alternative found this ncftp client. This actually worked instantly and was pretty easy in installing.</p>\n<p><strong><em>Install NCFTP:</em></strong><br />\n<code>sudo apt-get install ncftp</code></p>\n<p><strong><em>Log into FTP server:</em>&nbsp;</strong><br />\n<code>ncftp -u username hostname -p</code></p>\n<p>it will prompt for password enter it. Type '?' in the terminal for the list of commands and there you go :)</p>\n<p>Check out the NCFTP client for other platforms.</p>",
            "url": "https://prasanna.dev/posts/ftp-client-ubuntu",
            "title": "FTP Client (Ubuntu)",
            "summary": "Download multiple files from authenticated FTP server in Ubuntu.",
            "date_modified": "2010-08-30T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/stop-halting-at-assertions",
            "content_html": "<p>When I was writing functional tests i stepped into a scenario where i have to continue with the test even after an assertion failure. The idea here is that you find an assertion failing, still you may need to go ahead and check out all the assertions as these tests generally take long time to complete. At the end of the test it will be good to have the summary of all the failures along with the stack trace. I was looking for some testing framework to help out with this functionality but unfortunately cant find any.</p>\n<p>Finally using TestNg I implemented this feature. All you need is to write a listener that listens whenever a test fails and simply logs the stack trace without failing the test. There is a neat step by step tutorial given <a href=\"http://seleniumexamples.com/blog/guide/using-soft-assertions-in-testng\">here&nbsp;</a></p>\n<p>Though this example has some specific information for Selenium based tests, it works fine with normal functional tests too.</p>\n<p>Happy Testing :)</p>",
            "url": "https://prasanna.dev/posts/stop-halting-at-assertions",
            "title": "Stop halting at Assertions",
            "summary": "Generally assertions cause the test to halt. But sometimes we need to continue further and evaluate all the asserts and expect a comprehensive report of all the asserts.",
            "date_modified": "2010-03-04T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/printing-multiple-divs-in-a-page",
            "content_html": "<p>I stumbled upon a situation where i have to print multiple sections of a page into a single document. I actually tried out my own javascript to render only the required divs into an iframe and then the idea is to print that iframe calling <em>window.print()</em> in that iframe.</p>\n<p>Having limited Javascript knowledge the above mentioned task was a bit tedious to me , then i sat upon googling to get a ready made plugin that implements this feature and my search ended upon <a href=\"http://plugins.jquery.com/project/jqPrint\">Jqprint</a>. Jqprint is a plugin written in Jquery and it does the same that i mentioned above. Using this jqprint was really very easy. what all you need to do is that just mark all the divs that you want to print with a specific class name or with id. If your classname is printdiv then all you need to do is just call &nbsp;<em>$(\".printdiv\").jqprint();</em></p>",
            "url": "https://prasanna.dev/posts/printing-multiple-divs-in-a-page",
            "title": "Printing multiple divs in a page.",
            "summary": "When an user tries to print a HTML page, allow multiple divs from the page to appear in the print and not the entire page. Not using media CSS query",
            "date_modified": "2010-02-10T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/install-inconsolata-ttf-in-ubuntujaunty",
            "content_html": "<p>As developers tend to spend most of the time in front of IDEs it makes sense to pick up the best suited font for development. And i have seen most of the developers prefer to use monochrome fonts as it yields better feel while looking at the code. It is been now widely accepted by many developers to use Inconsolata as their development font. So better start using it and prove yourself geeky ;)<br />\nWhen i tried to use Inconsolata with my IntellijIDEA i couldn't find the ttf type inconsolata. And Intellij supports only ttf types. After a long search i downloaded thr ODf type and converted it to ttf using a converter and then i had the issue of installing it to my Jaunty. And i took help of my dev friends out here to resolve stuffs. So thought of consolidating the steps together as it may reduce someone else's pain.</p>\n<p><strong>Steps to install Inconsolata.ttf in Ubuntu(Jaunty).</strong></p>\n<p><strong>Step 1:</strong> Start with downloading inconsolata font. <a href=\"http://www.4shared.com/file/xnMYNL0w/Inconsolata.html\">Inconsolata.ttf</a></p>\n<p><strong>Step 2:</strong></p>\n<p>mkdir /usr/share/fonts/truetype</p>\n<p>cd /usr/share/fonts/truetype</p>\n<p>sudo mkdir ttf-inconsolata</p>\n<p><strong>Step 3:</strong> Copy the Inconsolata.ttf into the directory.</p>\n<p>sudo cp ~/Desktop/Inconsolata.ttf ttf-inconsolata</p>\n<p><strong>Step 4:</strong> Now modify the permissions to allow it to be accessed by IDE's</p>\n<p>cd ttf-inconsolata</p>\n<p>sudo chmod 777 Inconsolata.ttf</p>\n<p><strong>Step 5:</strong>Its not over yet. Finally before going to the IDE you need to cache the font to make it accessible.</p>\n<p>sudo fc-cache -f -v</p>\n<p>This will show a list of fonts cached recently. Check whether Inconsolata.ttf is been cached.</p>\n<p>Now you can keep staring at your code for a long time as it feels a lot better :)</p>",
            "url": "https://prasanna.dev/posts/install-inconsolata-ttf-in-ubuntujaunty",
            "title": "Install Inconsolata.ttf in Ubuntu(Jaunty).",
            "summary": "Inconsolata is dev friendly font used by devs for their code. This post is about installing Inconsolata tru type in Ubuntu - Jaunty.",
            "date_modified": "2009-12-10T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/test-smtp-server",
            "content_html": "<p>I came across testing a feature that ends up sending a mail. And to my misfortune i do not have access to the smtp server from my test environment. So i was looking out for a mock smtp server kind of stuff and ended up happily with test smtp server.</p>\n<p><a href=\"http://www.aboutmyip.com/AboutMyXApp/DevNullSmtp.jsp\">Devnull SMTP server</a> is a dummy SMTP server that can be used for testing purposes. It helps you see all communication between a client and the server and is very useful if you are trying to find problems with your email server or a client that you wrote. And the best part is it is very simple to use and it is free of cost too. So if you have any such necessity do try this out.</p>",
            "url": "https://prasanna.dev/posts/test-smtp-server",
            "title": "Test SMTP server",
            "summary": "How can you set up an SMTP server in your local environment for testing purpose.",
            "date_modified": "2009-10-20T18:30:00.000Z"
        },
        {
            "id": "https://prasanna.dev/posts/first-post",
            "content_html": "<p>So after writing lots of shits and craps in various blogs now I have decided to have my own blog (another one :P) to continue writing those same shits and craps.</p>\n<p><strong>Disclaimer:</strong> All the writings made in this blog are my own thoughts, opinions, views, suggestions, impressions, feelings, beliefs, faiths, sentiments, notions, thinking and ideas. In short this blog is written by me considering myself as its only reader. If you accidently landed on any of the posts here I’m not responsible for its consequences.</p>",
            "url": "https://prasanna.dev/posts/first-post",
            "title": "First Post",
            "summary": "An introductory post to start off my blog.",
            "date_modified": "2009-08-28T18:30:00.000Z"
        }
    ]
}